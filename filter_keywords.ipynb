{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_paper = pd.read_csv('./backdoor_papers_240509.csv')\n",
    "len(df_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, paper \u001b[38;5;129;01min\u001b[39;00m df_paper\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     37\u001b[0m     default_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOthers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2016\u001b[39m:\n\u001b[1;32m     39\u001b[0m         default_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIrrelevant\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "keywords = {\n",
    "    'Survey': ['survey', 'review'],\n",
    "    'Federated Learning (FL)': ['federated'],\n",
    "    'Natural Language Processing (NLP)': ['language', 'nlp', 'text classification', 'textual backdoor', 'gpt'],\n",
    "    'Automatic Speech Recognition (ASR)': ['speech', 'audio', 'voice', 'acoustic'],\n",
    "    'Face Recognition': ['face recognition'],\n",
    "    'Time Series': ['time series'],\n",
    "    'Transfer Learning': ['transfer learning'],\n",
    "    'Watermarking': ['watermarking', 'watermarks', 'watermark', 'copyright'],\n",
    "    'Certified': ['certifi'],\n",
    "    'Robustness': ['robustness'],\n",
    "    'Few-Shot Learning': ['few-shot', 'zero-shot'],\n",
    "    '3D': ['3d', 'point cloud '],\n",
    "    'Unlearning': ['unlearning'],\n",
    "    'Physical Backdoor': ['physical'],\n",
    "    'Diffusion': ['diffusion'],\n",
    "    'Transformer': ['transformer'],\n",
    "    'Clean-label Backdoor': ['clean-label'],\n",
    "    'Video': ['video', 'object detection '],\n",
    "    'Segmentation': ['segmentation'],\n",
    "    'Perturbation': ['perturbation'],\n",
    "    'Imperceptible': ['imperceptible', 'invisible'],\n",
    "    'Graph Learning': ['graph'],\n",
    "    'Foundation Models': ['foundation model'],\n",
    "    'Reinforcement Learning': ['reinforcement learning'],\n",
    "    'Poisoning Attack': ['poisoning attack'],\n",
    "    'Machine Learning (ML)': ['machine learning'],\n",
    "    'Deep Neural Networks': ['deep learning', 'deep neural networks', 'dnn', 'dnns', 'deep neural network'],\n",
    "    # 'RSA': ['rsa'],\n",
    "    'Others': [],\n",
    "    'Irrelevant': [],\n",
    "}\n",
    "\n",
    "data_cat = {k: [] for k in keywords.keys()}\n",
    "\n",
    "for _, paper in df_paper.iterrows():\n",
    "    default_cat = 'Others'\n",
    "    if paper['pub_time'] <= 2016:\n",
    "        default_cat = 'Irrelevant'\n",
    "    else:\n",
    "        for cat in data_cat.keys():\n",
    "            kw =  keywords[cat]\n",
    "            for w in kw:\n",
    "                if w in paper['title'].lower():\n",
    "                    default_cat = cat\n",
    "            if default_cat != 'Others':\n",
    "                break\n",
    "    data_cat[default_cat].append(paper)\n",
    "for k in data_cat:\n",
    "    print(k, len(data_cat[k]))\n",
    "\n",
    "# for id, pp in enumerate(data_cat['Others']):\n",
    "#     print(f\"Other {id}: {pp['title']} -- {pp['info']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backdoor_md = \"\"\n",
    "\n",
    "for kk, vv in data_cat.items():\n",
    "\n",
    "    data_to_md = f'# {kk}\\n'\n",
    "    data_to_md += '|No. | Title | Venue | Year | Author |\\n'\n",
    "    data_to_md += '|----|-------|-------|------|--------|\\n'\n",
    "\n",
    "    for id, pp in enumerate(vv):\n",
    "        # print(f\"Other {id}: {pp['title']} -- {pp['info']}\")\n",
    "        # data_to_md += f\"| {id + 1} | {pp['title']} | {pp['info']} | {pp['pub_time']} | {pp['author']} |\\n\"\n",
    "        data_to_md += f\"| {id + 1} | {pp['title']} | {pp['info']} | {pp['pub_time']} | {pp['author']} |\\n\"\n",
    "\n",
    "    # print(data_to_md)\n",
    "    data_backdoor_md += data_to_md\n",
    "    \n",
    "with open('./bd_paper.md', \"w\") as fw:\n",
    "    fw.write(data_backdoor_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other 0: Defending Against Backdoor Attacks by Quarantine Training -- IEEE Access 12: 10681-10689 (2024)\n",
      "Other 1: Imperceptible and multi-channel backdoor attack -- Appl. Intell. 54(1): 1099-1116 (2024)\n",
      "Other 2: SGBA: A stealthy scapegoat backdoor attack against deep neural networks -- Comput. Secur. 136: 103523 (2024)\n",
      "Other 3: The reality of backdoored S-Boxes - An eye opener -- J. Inf. Secur. Appl. 80: 103674 (2024)\n",
      "Other 4: The Reality of Backdoored S-Boxes - An Eye Opener -- IACR Cryptol. ePrint Arch. 2023: 1073 (2023)\n",
      "Other 5: TridentShell: An enhanced covert and scalable backdoor injection attack on web applications -- J. Netw. Comput. Appl. 223: 103823 (2024)\n",
      "Other 6: A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving -- IEEE J. Sel. Areas Commun. 42(4): 948-962 (2024)\n",
      "Other 7: Invisible backdoor learning in regional transform domain -- Neural Comput. Appl. 36(14): 8097-8108 (2024)\n",
      "Other 8: Backdoor advertising scandals, Yingyeo culture, and cancel culture among YouTube Influencers in South Korea -- New Media Soc. 26(1): 405-425 (2024)\n",
      "Other 9: Enhanced Coalescence Backdoor Attack Against DNN Based on Pixel Gradient -- Neural Process. Lett. 56(2): 114 (2024)\n",
      "Other 10: SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers -- Pattern Recognit. Lett. 177: 103-109 (2024)\n",
      "Other 11: Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection -- IEEE Trans. Comput. Soc. Syst. 11(2): 1816-1831 (2024)\n",
      "Other 12: Incremental Learning, Incremental Backdoor Threats -- IEEE Trans. Dependable Secur. Comput. 21(2): 559-572 (2024)\n",
      "Other 13: Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network -- IEEE Trans. Inf. Forensics Secur. 19: 280-292 (2024)\n",
      "Other 14: NTD: Non-Transferability Enabled Deep Learning Backdoor Detection -- IEEE Trans. Inf. Forensics Secur. 19: 104-119 (2024)\n",
      "Other 15: On Model Outsourcing Adaptive Attacks to Deep Learning Backdoor Defenses -- IEEE Trans. Inf. Forensics Secur. 19: 2356-2369 (2024)\n",
      "Other 16: Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures -- IEEE Trans. Inf. Forensics Secur. 19: 455-468 (2024)\n",
      "Other 17: Invisible Backdoor Attack With Dynamic Triggers Against Person Re-Identification -- IEEE Trans. Inf. Forensics Secur. 19: 307-319 (2024)\n",
      "Other 18: Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger -- IEEE Trans. Ind. Informatics 20(3): 5004-5013 (2024)\n",
      "Other 19: BadCM: Invisible Backdoor Attack Against Cross-Modal Learning -- IEEE Trans. Image Process. 33: 2558-2571 (2024)\n",
      "Other 20: Critical Path-Based Backdoor Detection for Deep Neural Networks -- IEEE Trans. Neural Networks Learn. Syst. 35(3): 4032-4046 (2024)\n",
      "Other 21: Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction -- IEEE Trans. Netw. Sci. Eng. 11(1): 525-542 (2024)\n",
      "Other 22: Stealthy Backdoor Attack for Code Models -- IEEE Trans. Software Eng. 50(4): 721-741 (2024)\n",
      "Other 23: BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract) -- AAAI 2024: 23506-23507\n",
      "Other 24: Progressive Poisoned Data Isolation for Training-Time Backdoor Defense -- AAAI 2024: 11425-11433\n",
      "Other 25: Conditional Backdoor Attack via JPEG Compression -- AAAI 2024: 19823-19831\n",
      "Other 26: Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations -- AAAI 2024: 11944-11952\n",
      "Other 27: A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives -- AAAI 2024: 1851-1859\n",
      "Other 28: Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks -- AAAI 2024: 274-282\n",
      "Other 29: Chronic Poisoning: Backdoor Attack against Split Learning -- AAAI 2024: 16531-16538\n",
      "Other 30: Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems -- ANZCC 2024: 115-120\n",
      "Other 31: Backdoor Attacks and Generative Model Fairness: Current Trends and Future Research Directions -- COMSNETS 2024: 31-36\n",
      "Other 32: Detecting Backdoors Embedded in Ensembles -- ICEIC 2024: 1-3\n",
      "Other 33: Defense Method Challenges Against Backdoor Attacks in Neural Networks -- ICAIIC 2024: 396-400\n",
      "Other 34: Backdoor Attack Against One-Class Sequential Anomaly Detection Models -- PAKDD (3) 2024: 262-274\n",
      "Other 35: On the Possibility of a Backdoor in the Micali-Schnorr Generator -- Public Key Cryptography (1) 2024: 352-386\n",
      "Other 36: On the Possibility of a Backdoor in the Micali-Schnorr Generator -- IACR Cryptol. ePrint Arch. 2023: 440 (2023)\n",
      "Other 37: SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection -- CoRR abs/2401.00137 (2024)\n",
      "Other 38: Is It Possible to Backdoor Face Forgery Detection with Natural Triggers -- CoRR abs/2401.00414 (2024)\n",
      "Other 39: The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers -- CoRR abs/2401.01537 (2024)\n",
      "Other 40: Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP -- CoRR abs/2401.01911 (2024)\n",
      "Other 41: Object-Oriented Backdoor Attack Against Image Captioning -- ICASSP 2022: 2864-2868\n",
      "Other 42: MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack -- CoRR abs/2401.02659 (2024)\n",
      "Other 43: End-to-End Anti-Backdoor Learning on Images and Time Series -- CoRR abs/2401.03215 (2024)\n",
      "Other 44: TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks -- CoRR abs/2401.05432 (2024)\n",
      "Other 45: Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation -- CoRR abs/2401.06030 (2024)\n",
      "Other 46: Learning Backdoors for Mixed Integer Programs with Contrastive Learning -- CoRR abs/2401.10467 (2024)\n",
      "Other 47: WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition -- CoRR abs/2401.13578 (2024)\n",
      "Other 48: BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning -- CoRR abs/2401.15002 (2024)\n",
      "Other 49: Multi-Trigger Backdoor Attacks: More Triggers, More Threats -- CoRR abs/2401.15295 (2024)\n",
      "Other 50: TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability -- CoRR abs/2401.15883 (2024)\n",
      "Other 51: Architectural Neural Backdoors from First Principles -- CoRR abs/2402.06957 (2024)\n",
      "Other 52: OrderBkd: Textual backdoor attack through repositioning -- CoRR abs/2402.07689 (2024)\n",
      "Other 53: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents -- CoRR abs/2402.11208 (2024)\n",
      "Other 54: Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection -- CoRR abs/2402.11473 (2024)\n",
      "Other 55: Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning -- CoRR abs/2402.12168 (2024)\n",
      "Other 56: Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation -- CoRR abs/2402.13532 (2024)\n",
      "Other 57: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment -- CoRR abs/2402.14968 (2024)\n",
      "Other 58: Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models -- CoRR abs/2402.14977 (2024)\n",
      "Other 59: Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm -- CoRR abs/2402.15653 (2024)\n",
      "Other 60: On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem -- CoRR abs/2402.16926 (2024)\n",
      "Other 61: Model X-ray: Detect Backdoored Models via Decision Boundary -- CoRR abs/2402.17465 (2024)\n",
      "Other 62: Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks -- CoRR abs/2402.18718 (2024)\n",
      "Other 63: Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge -- CoRR abs/2402.19334 (2024)\n",
      "Other 64: A general approach to enhance the survivability of backdoor attacks by decision path coupling -- CoRR abs/2403.02950 (2024)\n",
      "Other 65: On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder -- CoRR abs/2403.03846 (2024)\n",
      "Other 66: AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration -- CoRR abs/2403.06430 (2024)\n",
      "Other 67: Real is not True: Backdoor Attacks Against Deepfake Detection -- CoRR abs/2403.06610 (2024)\n",
      "Other 68: Backdoor Attack with Mode Mixture Latent Modification -- CoRR abs/2403.07463 (2024)\n",
      "Other 69: Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks -- CoRR abs/2403.08208 (2024)\n",
      "Other 70: Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency -- CoRR abs/2403.10717 (2024)\n",
      "Other 71: Impart: An Imperceptible and Effective Label-Specific Backdoor Attack -- CoRR abs/2403.13017 (2024)\n",
      "Other 72: Invisible Backdoor Attack Through Singular Value Decomposition -- CoRR abs/2403.13018 (2024)\n",
      "Other 73: Clean-image Backdoor Attacks -- CoRR abs/2403.15010 (2024)\n",
      "Other 74: An Embarrassingly Simple Defense Against Backdoor Attacks On SSL -- CoRR abs/2403.15918 (2024)\n",
      "Other 75: Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors -- CoRR abs/2403.16569 (2024)\n",
      "Other 76: Task-Agnostic Detector for Insertion-Based Backdoor Attacks -- CoRR abs/2403.17155 (2024)\n",
      "Other 77: LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning -- CoRR abs/2403.17188 (2024)\n",
      "Other 78: A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks -- CoRR abs/2404.00076 (2024)\n",
      "Other 79: Privacy Backdoors: Stealing Data with Corrupted Pretrained Models -- CoRR abs/2404.00473 (2024)\n",
      "Other 80: Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models -- CoRR abs/2404.01231 (2024)\n",
      "Other 81: Highly-Effective Backdoors for Hash Functions and Beyond -- IACR Cryptol. ePrint Arch. 2024: 536 (2024)\n",
      "Other 82: Towards a Robust Defense: A Multifaceted Approach to the Detection and Mitigation of Neural Backdoor Attacks through Feature Space Exploration and Analysis -- Old Dominion University, Norfolk, Virginia, USA, 2023\n",
      "Other 83: Instance-Agnostic and Practical Clean Label Backdoor Attack Method for Deep Learning Based Face Recognition Models -- IEEE Access 11: 144040-144050 (2023)\n",
      "Other 84: Compression-resistant backdoor attack against deep neural networks -- Appl. Intell. 53(17): 20402-20417 (2023)\n",
      "Other 85: Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces -- Sci. China Inf. Sci. 66(8) (2023)\n",
      "Other 86: DIHBA: Dynamic, invisible and high attack success rate boundary backdoor attack with low poison ratio -- Comput. Secur. 129: 103212 (2023)\n",
      "Other 87: DLP: towards active defense against backdoor attacks with decoupled learning process -- Cybersecur. 6(1): 9 (2023)\n",
      "Other 88: NBA: defensive distillation for backdoor removal via neural behavior alignment -- Cybersecur. 6(1): 20 (2023)\n",
      "Other 89: A Textual Backdoor Defense Method Based on Deep Feature Classification -- Entropy 25(2): 220 (2023)\n",
      "Other 90: Backdoor Attack against Face Sketch Synthesis -- Entropy 25(7): 974 (2023)\n",
      "Other 91: A defense method against backdoor attacks on neural networks -- Expert Syst. Appl. 213(Part): 118990 (2023)\n",
      "Other 92: A lightweight backdoor defense framework based on image inpainting -- Neurocomputing 537: 22-36 (2023)\n",
      "Other 93: A Triggerless Backdoor Attack and Defense Mechanism for Intelligent Task Offloading in Multi-UAV Systems -- IEEE Internet Things J. 10(7): 5719-5732 (2023)\n",
      "Other 94: Backdoor-Resistant Public Data Integrity Verification Scheme Based on Smart Contracts -- IEEE Internet Things J. 10(16): 14269-14284 (2023)\n",
      "Other 95: Turning backdoors for efficient privacy protection against image retrieval violations -- Inf. Process. Manag. 60(5): 103471 (2023)\n",
      "Other 96: Unlabeled backdoor poisoning on trained-from-scratch semi-supervised learning -- Inf. Sci. 647: 119453 (2023)\n",
      "Other 97: Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias -- Inf. Sci. 643: 119171 (2023)\n",
      "Other 98: Multidomain active defense: Detecting multidomain backdoor poisoned samples via ALL-to-ALL decoupling training without clean datasets -- Neural Networks 168: 350-362 (2023)\n",
      "Other 99: How to backdoor split learning -- Neural Networks 168: 326-336 (2023)\n",
      "Other 100: TAT: Targeted backdoor attacks against visual object tracking -- Pattern Recognit. 142: 109629 (2023)\n",
      "Other 101: Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface -- Sensors 23(10): 4742 (2023)\n",
      "Other 102: Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface -- ASHES@CCS 2021: 63-72\n",
      "Other 103: Backdoor Pony: Evaluating backdoor attacks and defenses in different domains -- SoftwareX 22: 101387 (2023)\n",
      "Other 104: Stealthy Frequency-Domain Backdoor Attacks: Fourier Decomposition and Fundamental Frequency Injection -- IEEE Signal Process. Lett. 30: 1677-1681 (2023)\n",
      "Other 105: Stealthy Backdoor Attack Against Speaker Recognition Using Phase-Injection Hidden Trigger -- IEEE Signal Process. Lett. 30: 1057-1061 (2023)\n",
      "Other 106: FooBaR: Fault Fooling Backdoor Attack on Neural Network Training -- IEEE Trans. Dependable Secur. Comput. 20(3): 1895-1908 (2023)\n",
      "Other 107: Enhancing Backdoor Attacks With Multi-Level MMD Regularization -- IEEE Trans. Dependable Secur. Comput. 20(2): 1675-1686 (2023)\n",
      "Other 108: Backdoor Attacks for Remote Sensing Data With Wavelet Transform -- IEEE Trans. Geosci. Remote. Sens. 61: 1-15 (2023)\n",
      "Other 109: Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection -- IEEE Trans. Inf. Forensics Secur. 18: 4668-4680 (2023)\n",
      "Other 110: Bayesian Causal Bandits with Backdoor Adjustment Prior -- Trans. Mach. Learn. Res. 2023 (2023)\n",
      "Other 111: Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts -- ACM Trans. Softw. Eng. Methodol. 32(3): 61:1-61:24 (2023)\n",
      "Other 112: Hidden Backdoor Attack Against Deep Learning-Based Wireless Signal Modulation Classifiers -- IEEE Trans. Veh. Technol. 72(9): 12396-12400 (2023)\n",
      "Other 113: A stealthy and robust backdoor attack via frequency domain transform -- World Wide Web (WWW) 26(5): 2767-2783 (2023)\n",
      "Other 114: Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks -- SafeAI@AAAI 2023\n",
      "Other 115: Poisoning-Based Backdoor Attacks in Computer Vision -- AAAI 2023: 16121-16122\n",
      "Other 116: Towards Understanding How Self-training Tolerates Data Backdoor Poisoning -- SafeAI@AAAI 2023\n",
      "Other 117: Probabilistic Generalization of Backdoor Trees with Application to SAT -- AAAI 2023: 4095-4103\n",
      "Other 118: BITE: Textual Backdoor Attacks with Iterative Trigger Injection -- ACL (1) 2023: 12951-12968\n",
      "Other 119: A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning -- ACL (1) 2023: 3508-3520\n",
      "Other 120: Defending against Insertion-based Textual Backdoor Attacks via Attribution -- ACL (Findings) 2023: 8818-8833\n",
      "Other 121: Multi-target Backdoor Attacks for Code Pre-trained Models -- ACL (1) 2023: 7236-7254\n",
      "Other 122: Backdooring Neural Code Search -- ACL (1) 2023: 9692-9708\n",
      "Other 123: Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks -- ACM TUR-C 2023: 145-147\n",
      "Other 124: DFaP: Data Filtering and Purification Against Backdoor Attacks -- AIS&P (1) 2023: 81-97\n",
      "Other 125: CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses -- AsiaCCS 2023: 938-950\n",
      "Other 126: SolScope: Effectively Hunting Potential Permission Backdoor Threats in Smart Contracts -- BIGCOM 2023: 88-95\n",
      "Other 127: Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors -- AISec@CCS 2023: 209-220\n",
      "Other 128: Poster: Fooling XAI with Explanation-Aware Backdoors -- CCS 2023: 3612-3614\n",
      "Other 129: Poster: Backdoor Attack on Extreme Learning Machines -- CCS 2023: 3588-3590\n",
      "Other 130: Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks -- CIKM 2023: 608-618\n",
      "Other 131: TRGE: A Backdoor Detection After Quantization -- Inscrypt (2) 2023: 394-398\n",
      "Other 132: Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks -- CISS 2023: 1-6\n",
      "Other 133: Blind Concealment from Reconstruction-based Attack Detectors for Industrial Control Systems via Backdoor Attacks -- CPSS@AsiaCCS 2023: 36-47\n",
      "Other 134: Detecting Backdoors in Pre-trained Encoders -- CVPR 2023: 16352-16362\n",
      "Other 135: Backdoor Cleansing with Unlabeled Data -- CVPR 2023: 12218-12227\n",
      "Other 136: Architectural Backdoors in Neural Networks -- CVPR 2023: 24595-24604\n",
      "Other 137: The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection -- CVPR 2023: 24585-24594\n",
      "Other 138: Backdoor Defense via Adaptively Splitting Poisoned Dataset -- CVPR 2023: 4005-4014\n",
      "Other 139: Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs -- CVPR Workshops 2023: 2338-2345\n",
      "Other 140: Color Backdoor: A Robust Poisoning Attack in Color Space -- CVPR 2023: 8133-8142\n",
      "Other 141: Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency -- CVPR 2023: 16363-16372\n",
      "Other 142: Single Image Backdoor Inversion via Robust Smoothed Classifiers -- CVPR 2023: 8113-8122\n",
      "Other 143: Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning -- CVPR 2023: 12239-12249\n",
      "Other 144: MEDIC: Remove Model Backdoors via Importance Driven Cloning -- CVPR 2023: 20485-20494\n",
      "Other 145: Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger -- CVPR 2023: 12250-12259\n",
      "Other 146: Backdoor Defense via Deconfounded Representation Learning -- CVPR 2023: 12228-12238\n",
      "Other 147: Don't Knock! Rowhammer at the Backdoor of DNN Models -- DSN 2023: 109-122\n",
      "Other 148: Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain -- ECAI 2023: 2954-2961\n",
      "Other 149: Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation -- EMNLP 2023: 953-967\n",
      "Other 150: Attention-Enhancing Backdoor Attacks Against BERT-based Models -- EMNLP (Findings) 2023: 10672-10690\n",
      "Other 151: Backdoor Attacks Leveraging Latent Representation in Competitive Learning -- ESORICS Workshops (2) 2023: 700-718\n",
      "Other 152: Backdoor Mitigation in Deep Neural Networks via Strategic Retraining -- FM 2023: 635-647\n",
      "Other 153: Backdoor Attacks Against Deep Learning-Based Massive MIMO Localization -- GLOBECOM 2023: 2796-2801\n",
      "Other 154: Fully Hidden Dynamic Trigger Backdoor Attacks -- ICAART (3) 2023: 81-91\n",
      "Other 155: QTROJAN: A Circuit Backdoor Against Quantum Neural Networks -- ICASSP 2023: 1-5\n",
      "Other 156: BadRes: Reveal the Backdoors Through Residual Connection -- ICASSP 2023: 1-5\n",
      "Other 157: Untargeted Backdoor Attack Against Object Detection -- ICASSP 2023: 1-5\n",
      "Other 158: Training Set Cleansing of Backdoor Poisoning by Self-Supervised Representation Learning -- ICASSP 2023: 1-5\n",
      "Other 159: BATT: Backdoor Attack with Transformation-Based Triggers -- ICASSP 2023: 1-5\n",
      "Other 160: Backdoor Defense via Suppressing Model Shortcuts -- ICASSP 2023: 1-5\n",
      "Other 161: NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning -- ICASSP 2023: 1-5\n",
      "Other 162: An Empirical Study of Backdoor Attacks on Masked Auto Encoders -- ICASSP 2023: 1-5\n",
      "Other 163: Random Location Poisoning Backdoor Attack Against Automatic Modulation Classification in Wireless Networks -- ICCC 2023: 1-6\n",
      "Other 164: Stealthy Backdoor Attack on RF Signal Classification -- ICCCN 2023: 1-10\n",
      "Other 165: Countermeasure against Backdoor Attack for Deep Learning-Based Phishing Detection -- ICCE-Taiwan 2023: 651-652\n",
      "Other 166: Invisible Encoded Backdoor attack on DNNs using Conditional GAN -- ICCE 2023: 1-5\n",
      "Other 167: An Embarrassingly Simple Backdoor Attack on Self-supervised Learning -- ICCV 2023: 4344-4355\n",
      "Other 168: Beating Backdoor Attack at Its Own Game -- ICCV 2023: 4597-4606\n",
      "Other 169: The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning -- ICCV 2023: 4707-4717\n",
      "Other 170: Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis -- ICCV 2023: 4561-4573\n",
      "Other 171: TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models -- ICCV 2023: 165-175\n",
      "Other 172: Computation and Data Efficient Backdoor Attacks -- ICCV 2023: 4782-4791\n",
      "Other 173: Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization -- ICCV 2023: 4443-4454\n",
      "Other 174: BHAC-MRI: Backdoor and Hybrid Attacks on MRI Brain Tumor Classification Using CNN -- ICIAP (2) 2023: 332-344\n",
      "Other 175: Neural Network Backdoor Attacks Fully Controlled by Composite Natural Utterance Fragments -- ICICS 2023: 451-466\n",
      "Other 176: Efficient any-Target Backdoor Attack with Pseudo Poisoned Samples -- ICIP 2023: 3319-3323\n",
      "Other 177: CSSBA: A Clean Label Sample-Specific Backdoor Attack -- ICIP 2023: 965-969\n",
      "Other 178: Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only -- ICLR 2023\n",
      "Other 179: SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency -- ICLR 2023\n",
      "Other 180: Distilling Cognitive Backdoor Patterns within an Image -- ICLR 2023\n",
      "Other 181: Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks -- ICLR 2023\n",
      "Other 182: The Dark Side of AutoML: Towards Architectural Backdoor Search -- ICLR 2023\n",
      "Other 183: Revisiting the Assumption of Latent Separability for Backdoor Defenses -- ICLR 2023\n",
      "Other 184: UNICORN: A Unified Backdoor Trigger Inversion Framework -- ICLR 2023\n",
      "Other 185: Rethinking Backdoor Attacks -- ICML 2023: 16216-16236\n",
      "Other 186: Reconstructive Neuron Pruning for Backdoor Defense -- ICML 2023: 19837-19854\n",
      "Other 187: Understanding Backdoor Attacks through the Adaptability Hypothesis -- ICML 2023: 37952-37976\n",
      "Other 188: UMD: Unsupervised Model Detection for X2X Backdoor Attacks -- ICML 2023: 38013-38038\n",
      "Other 189: SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach -- ICONIP (12) 2023: 285-297\n",
      "Other 190: MIC: An Effective Defense Against Word-Level Textual Backdoor Attacks -- ICONIP (6) 2023: 3-18\n",
      "Other 191: X-HDNN: Explainable Hybrid DNN for Industrial Internet of Things Backdoor Attack Detection -- ICTC 2023: 1053-1057\n",
      "Other 192: Orion: Online Backdoor Sample Detection via Evolution Deviance -- IJCAI 2023: 864-874\n",
      "Other 193: Backdoor Attack on Deep Neural Networks in Perception Domain -- IJCNN 2023: 1-8\n",
      "Other 194: Computational Color Constancy-Based Backdoor Attacks -- ISPA 2023: 1-6\n",
      "Other 195: SDN Application Backdoor: Disrupting the Service via Poisoning the Topology -- INFOCOM 2023: 1-10\n",
      "Other 196: Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing -- INFOCOM 2023: 1-10\n",
      "Other 197: SEBD: Sensor Emulation Based Backdoor for Autopilot -- IoT 2023: 265-269\n",
      "Other 198: DUBIOUS: Detecting Unknown Backdoored Input by Observing Unusual Signatures -- MILCOM 2023: 696-702\n",
      "Other 199: Robust Sentiment Classification Based on the Backdoor Adjustment -- MLNLP 2023: 41-47\n",
      "Other 200: PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification -- ACM Multimedia 2023: 9134-9142\n",
      "Other 201: Model-Contrastive Learning for Backdoor Elimination -- ACM Multimedia 2023: 8869-8880\n",
      "Other 202: MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems -- MobiCom 2023: 48:1-48:15\n",
      "Other 203: BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense -- NDSS 2023\n",
      "Other 204: Backdoor Attacks Against Dataset Distillation -- NDSS 2023\n",
      "Other 205: The \"Beatrix\" Resurrections: Robust Backdoor Detection via Gram Matrices -- NDSS 2023\n",
      "Other 206: BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking -- NeurIPS 2023\n",
      "Other 207: Towards Stable Backdoor Purification through Feature Shift Tuning -- NeurIPS 2023\n",
      "Other 208: A Unified Detection Framework for Inference-Stage Backdoor Defenses -- NeurIPS 2023\n",
      "Other 209: CBD: A Certified Backdoor Detector Based on Local Dominant Probability -- NeurIPS 2023\n",
      "Other 210: Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features -- NeurIPS 2023\n",
      "Other 211: IMTM: Invisible Multi-trigger Multimodal Backdoor Attack -- NLPCC (2) 2023: 533-545\n",
      "Other 212: Defending Against Backdoor Attacks by Layer-wise Feature Analysis -- PAKDD (2) 2023: 428-440\n",
      "Other 213: QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks -- QCE 2023: 1098-1106\n",
      "Other 214: Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation -- SP 2023: 755-772\n",
      "Other 215: MagBackdoor: Beware of Your Loudspeaker as A Backdoor For Magnetic Injection Attacks -- SP 2023: 3416-3431\n",
      "Other 216: Disguising Attacks with Explanation-Aware Backdoors -- SP 2023: 664-681\n",
      "Other 217: On Feasibility of Server-side Backdoor Attacks on Split Learning -- SP (Workshops) 2023: 84-93\n",
      "Other 218: RAB: Provable Robustness Against Backdoor Attacks -- SP 2023: 1311-1328\n",
      "Other 219: Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers -- SP 2023: 719-736\n",
      "Other 220: TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World -- SRDS 2023: 82-92\n",
      "Other 221: Training Data Leakage via Imperceptible Backdoor Attack -- SSCI 2023: 1553-1559\n",
      "Other 222: Immunizing Backdoored PRGs -- TCC (3) 2023: 153-182\n",
      "Other 223: Immunizing Backdoored PRGs -- IACR Cryptol. ePrint Arch. 2023: 1778 (2023)\n",
      "Other 224: PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis -- USENIX Security Symposium 2023: 2365-2382\n",
      "Other 225: VILLAIN: Backdoor Attacks Against Vertical Split Learning -- USENIX Security Symposium 2023: 2743-2760\n",
      "Other 226: A Data-free Backdoor Injection Approach in Neural Networks -- USENIX Security Symposium 2023: 2671-2688\n",
      "Other 227: ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms -- USENIX Security Symposium 2023: 2725-2742\n",
      "Other 228: Towards A Proactive ML Approach for Detecting Backdoor Poison Samples -- USENIX Security Symposium 2023: 1685-1702\n",
      "Other 229: Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks -- USENIX Security Symposium 2023: 2689-2706\n",
      "Other 230: Aliasing Backdoor Attacks on Pre-trained Models -- USENIX Security Symposium 2023: 2707-2724\n",
      "Other 231: Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack -- CoRR abs/2301.02615 (2023)\n",
      "Other 232: PECAN: A Deterministic Certified Defense Against Backdoor Attacks -- CoRR abs/2301.11824 (2023)\n",
      "Other 233: Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering -- CoRR abs/2301.12318 (2023)\n",
      "Other 234: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification -- CoRR abs/2302.01740 (2023)\n",
      "Other 235: BackdoorBox: A Python Toolbox for Backdoor Learning -- CoRR abs/2302.01762 (2023)\n",
      "Other 236: Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder -- CoRR abs/2302.04457 (2023)\n",
      "Other 237: Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness -- CoRR abs/2302.04977 (2023)\n",
      "Other 238: Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data -- CoRR abs/2302.06279 (2023)\n",
      "Other 239: Backdoor Attacks to Pre-trained Unified Foundation Models -- CoRR abs/2302.09360 (2023)\n",
      "Other 240: SATBA: An Invisible Backdoor Attack Based On Spatial Attention -- CoRR abs/2302.13056 (2023)\n",
      "Other 241: Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias -- CoRR abs/2303.01504 (2023)\n",
      "Other 242: Do Backdoors Assist Membership Inference Attacks -- CoRR abs/2303.12589 (2023)\n",
      "Other 243: Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder -- CoRR abs/2303.15564 (2023)\n",
      "Other 244: Launching a Robust Backdoor Attack under Capability Constrained Scenarios -- CoRR abs/2304.10985 (2023)\n",
      "Other 245: BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT -- CoRR abs/2304.12298 (2023)\n",
      "Other 246: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger -- CoRR abs/2304.14475 (2023)\n",
      "Other 247: Backdoor Learning on Sequence to Sequence Models -- CoRR abs/2305.02424 (2023)\n",
      "Other 248: BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks -- CoRR abs/2305.03289 (2023)\n",
      "Other 249: BadCS: A Backdoor Attack Framework for Code search -- CoRR abs/2305.05503 (2023)\n",
      "Other 250: Backdoor to the Hidden Ground State: Planted Vertex Cover Example -- CoRR abs/2305.06610 (2023)\n",
      "Other 251: Stealthy Low-frequency Backdoor Attack against Deep Neural Networks -- CoRR abs/2305.09677 (2023)\n",
      "Other 252: Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks -- CoRR abs/2305.10596 (2023)\n",
      "Other 253: From Shortcuts to Triggers: Backdoor Defense with Denoised PoE -- CoRR abs/2305.14910 (2023)\n",
      "Other 254: IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks -- CoRR abs/2305.16503 (2023)\n",
      "Other 255: Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study -- CoRR abs/2305.18384 (2023)\n",
      "Other 256: Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers -- CoRR abs/2306.00816 (2023)\n",
      "Other 257: Mitigating Backdoor Attack Via Prerequisite Transformation -- CoRR abs/2306.01983 (2023)\n",
      "Other 258: Backdoor Attack with Sparse and Invisible Trigger -- CoRR abs/2306.06209 (2023)\n",
      "Other 259: A Proxy-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks -- CoRR abs/2306.08313 (2023)\n",
      "Other 260: Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios -- CoRR abs/2306.08386 (2023)\n",
      "Other 261: IMPOSITION: Implicit Backdoor Attack through Scenario Injection -- CoRR abs/2306.15755 (2023)\n",
      "Other 262: Efficient Backdoor Removal Through Natural Gradient Fine-tuning -- CoRR abs/2306.17441 (2023)\n",
      "Other 263: Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy -- CoRR abs/2307.07328 (2023)\n",
      "Other 264: Backdoor Attack against Object Detection with Clean Annotation -- CoRR abs/2307.10487 (2023)\n",
      "Other 265: FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks -- CoRR abs/2307.11565 (2023)\n",
      "Other 266: BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models -- CoRR abs/2307.16489 (2023)\n",
      "Other 267: Test-Time Adaptation for Backdoor Defense -- CoRR abs/2308.06107 (2023)\n",
      "Other 268: Backdoor Mitigation by Correcting the Distribution of Neural Activations -- CoRR abs/2308.09850 (2023)\n",
      "Other 269: Hiding Backdoors within Event Sequence Data via Poisoning Attacks -- CoRR abs/2308.10201 (2023)\n",
      "Other 270: Backdooring Textual Inversion for Concept Censorship -- CoRR abs/2308.10718 (2023)\n",
      "Other 271: BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection -- CoRR abs/2308.12439 (2023)\n",
      "Other 272: LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors -- CoRR abs/2308.13904 (2023)\n",
      "Other 273: Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack -- CoRR abs/2308.16684 (2023)\n",
      "Other 274: Robust Backdoor Attacks on Object Detection in Real World -- CoRR abs/2309.08953 (2023)\n",
      "Other 275: Horizontal Class Backdoor to Deep Learning -- CoRR abs/2310.00542 (2023)\n",
      "Other 276: GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning -- CoRR abs/2310.00626 (2023)\n",
      "Other 277: Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes -- CoRR abs/2310.02451 (2023)\n",
      "Other 278: Confidence-driven Sampling for Backdoor Attacks -- CoRR abs/2310.05263 (2023)\n",
      "Other 279: Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks -- CoRR abs/2310.05862 (2023)\n",
      "Other 280: High Dimensional Causal Inference with Variational Backdoor Adjustment -- CoRR abs/2310.06100 (2023)\n",
      "Other 281: Prompt Backdoors in Visual Prompt Learning -- CoRR abs/2310.07632 (2023)\n",
      "Other 282: Invisible Threats: Backdoor Attack in OCR Systems -- CoRR abs/2310.08259 (2023)\n",
      "Other 283: Defending Our Privacy With Backdoors -- CoRR abs/2310.08320 (2023)\n",
      "Other 284: Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks -- CoRR abs/2310.09744 (2023)\n",
      "Other 285: Demystifying Poisoning Backdoor Attacks from a Statistical Perspective -- CoRR abs/2310.10780 (2023)\n",
      "Other 286: WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks -- CoRR abs/2310.11595 (2023)\n",
      "Other 287: Does Differential Privacy Prevent Backdoor Attacks in Practice -- CoRR abs/2311.06227 (2023)\n",
      "Other 288: Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration -- CoRR abs/2311.07417 (2023)\n",
      "Other 289: TextGuard: Provable Defense against Backdoor Attacks on Text Classification -- CoRR abs/2311.11225 (2023)\n",
      "Other 290: Effective Backdoor Mitigation Depends on the Pre-training Objective -- CoRR abs/2311.14948 (2023)\n",
      "Other 291: BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP -- CoRR abs/2311.16194 (2023)\n",
      "Other 292: Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective -- CoRR abs/2311.16646 (2023)\n",
      "Other 293: Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics -- CoRR abs/2312.02673 (2023)\n",
      "Other 294: Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger -- CoRR abs/2312.04584 (2023)\n",
      "Other 295: BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting -- CoRR abs/2312.04902 (2023)\n",
      "Other 296: Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks -- CoRR abs/2312.06230 (2023)\n",
      "Other 297: Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking -- CoRR abs/2312.07955 (2023)\n",
      "Other 298: On the Difficulty of Defending Contrastive Learning against Backdoor Attacks -- CoRR abs/2312.09057 (2023)\n",
      "Other 299: UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks -- CoRR abs/2312.10657 (2023)\n",
      "Other 300: Manipulating Trajectory Prediction with Backdoors -- CoRR abs/2312.13863 (2023)\n",
      "Other 301: (Withdrawn) BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning -- CoRR abs/2311.12075 (2023)\n",
      "Other 302: A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks -- Ad Hoc Networks 124: 102727 (2022)\n",
      "Other 303: Boosting the Performance of CDCL-Based SAT Solvers by Exploiting Backbones and Backdoors -- Algorithms 15(9): 302 (2022)\n",
      "Other 304: Active intellectual property protection for deep neural networks through stealthy backdoor and users' identities authentication -- Appl. Intell. 52(14): 16497-16511 (2022)\n",
      "Other 305: Backdoor smoothing: Demystifying backdoor attacks on deep neural networks -- Comput. Secur. 120: 102814 (2022)\n",
      "Other 306: Experimental Study of Fault Injection Attack on Image Sensor Interface for Triggering Backdoored DNN Models -- IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 105-A(3): 336-343 (2022)\n",
      "Other 307: Multi-Model Selective Backdoor Attack with Different Trigger Positions -- IEICE Trans. Inf. Syst. 105-D(1): 170-174 (2022)\n",
      "Other 308: A multitarget backdooring attack on deep neural networks with random location trigger -- Int. J. Intell. Syst. 37(3): 2567-2583 (2022)\n",
      "Other 309: Susceptibility & defense of satellite image-trained convolutional networks to backdoor attacks -- Inf. Sci. 603: 244-261 (2022)\n",
      "Other 310: Robust backdoor injection with the capability of resisting network transfer -- Inf. Sci. 612: 594-611 (2022)\n",
      "Other 311: Backdoor-resistant identity-based proxy re-encryption for cloud-assisted wireless body area networks -- Inf. Sci. 604: 80-96 (2022)\n",
      "Other 312: BlindNet backdoor: Attack on deep neural network using blind watermark -- Multim. Tools Appl. 81(5): 6217-6234 (2022)\n",
      "Other 313: MP-BADNet+: Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs -- Peer-to-Peer Netw. Appl. 15(6): 2457-2473 (2022)\n",
      "Other 314: IBD: An Interpretable Backdoor-Detection Method via Multivariate Interactions -- Sensors 22(22): 8697 (2022)\n",
      "Other 315: Are Backdoor Mandates Ethical? - A Position Paper -- IEEE Technol. Soc. Mag. 41(4): 63-70 (2022)\n",
      "Other 316: Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks -- IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 41(8): 2611-2624 (2022)\n",
      "Other 317: One-to-N & N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models -- IEEE Trans. Dependable Secur. Comput. 19(3): 1562-1578 (2022)\n",
      "Other 318: LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check -- IEEE Trans. Inf. Forensics Secur. 17: 2000-2014 (2022)\n",
      "Other 319: Stealthy Backdoors as Compression Artifacts -- IEEE Trans. Inf. Forensics Secur. 17: 1372-1387 (2022)\n",
      "Other 320: Dispersed Pixel Perturbation-Based Imperceptible Backdoor Trigger for Image Classifier Models -- IEEE Trans. Inf. Forensics Secur. 17: 3091-3106 (2022)\n",
      "Other 321: Poison Ink: Robust and Invisible Backdoor Attack -- IEEE Trans. Image Process. 31: 5691-5705 (2022)\n",
      "Other 322: Detection of Backdoors in Trained Classifiers Without Access to the Training Set -- IEEE Trans. Neural Networks Learn. Syst. 33(3): 1177-1191 (2022)\n",
      "Other 323: Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models -- IEEE Trans. Serv. Comput. 15(3): 1526-1539 (2022)\n",
      "Other 324: Tractable Abstract Argumentation via Backdoor-Treewidth -- AAAI 2022: 5608-5615\n",
      "Other 325: Backdoor Attacks on the DNN Interpretation System -- AAAI 2022: 561-570\n",
      "Other 326: Faster Algorithms for Weak Backdoors -- AAAI 2022: 3741-3748\n",
      "Other 327: Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks -- AAAI 2022: 9575-9583\n",
      "Other 328: Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework -- AAAI 2022: 3786-3795\n",
      "Other 329: Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks -- AAAI 2022: 10309-10318\n",
      "Other 330: On Probabilistic Generalization of Backdoors in Boolean Satisfiability -- AAAI 2022: 10353-10361\n",
      "Other 331: COLLIDER: A Robust Training Framework for Backdoor Data -- ACCV (6) 2022: 681-698\n",
      "Other 332: Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks -- ACSAC 2022: 267-278\n",
      "Other 333: An Approach to Generation Triggers for Parrying Backdoor in Neural Networks -- AGI 2022: 304-314\n",
      "Other 334: Dynamic Backdoors with Global Average Pooling -- AICAS 2022: 320-323\n",
      "Other 335: Sample-Specific Backdoor based Active Intellectual Property Protection for Deep Neural Networks -- AICAS 2022: 316-319\n",
      "Other 336: Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection -- BDCAT 2022: 40-47\n",
      "Other 337: SentMod: Hidden Backdoor Attack on Unstructured Textual Data -- BigDataSecurity/HPSC/IDS 2022: 224-231\n",
      "Other 338: An anomaly detection approach for backdoored neural networks: face recognition as a case study -- BIOSIG 2022: 80-88\n",
      "Other 339: Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain -- BMVC 2022: 259\n",
      "Other 340: Verifying Neural Networks Against Backdoor Attacks -- CAV (1) 2022: 171-192\n",
      "Other 341: Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets -- CCS 2022: 3315-3317\n",
      "Other 342: Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation -- AISec@CCS 2022: 91-102\n",
      "Other 343: Asynchronous Evolutionary Algorithm for Finding Backdoors in Boolean Satisfiability -- CEC 2022: 1-8\n",
      "Other 344: Efficient DNN Backdoor Detection Guided by Static Weight Analysis -- Inscrypt 2022: 408-428\n",
      "Other 345: Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications -- COLING 2022: 984-993\n",
      "Other 346: Deletion-Backdoors for Argumentation Frameworks with Collective Attacks -- SAFA@COMMA 2022: 98-110\n",
      "Other 347: Learning Pseudo-Backdoors for Mixed Integer Programs -- CPAIOR 2022: 91-102\n",
      "Other 348: Learning Pseudo-Backdoors for Mixed Integer Programs -- SOCS 2021: 170-172\n",
      "Other 349: FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis -- CVPR 2022: 20844-20853\n",
      "Other 350: Complex Backdoor Detection by Symmetric Feature Differencing -- CVPR 2022: 14983-14993\n",
      "Other 351: Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks -- CVPR 2022: 13337-13347\n",
      "Other 352: Backdoor Attacks on Self-Supervised Learning -- CVPR 2022: 13327-13336\n",
      "Other 353: Better Trigger Inversion Optimization in Backdoor Scanning -- CVPR 2022: 13358-13368\n",
      "Other 354: Dual-Key Multimodal Backdoors for Visual Question Answering -- CVPR 2022: 15354-15364\n",
      "Other 355: DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints -- CVPR 2022: 15192-15201\n",
      "Other 356: Combining Defences Against Data-Poisoning Based Backdoor Attacks on Neural Networks -- DBSec 2022: 28-47\n",
      "Other 357: BadDet: Backdoor Attacks on Object Detection -- ECCV Workshops (1) 2022: 396-412\n",
      "Other 358: RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN -- ECCV (4) 2022: 708-724\n",
      "Other 359: An Invisible Black-Box Backdoor Attack Through Frequency Domain -- ECCV (13) 2022: 396-413\n",
      "Other 360: Data-Free Backdoor Removal Based on Channel Lipschitzness -- ECCV (5) 2022: 175-191\n",
      "Other 361: Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks -- EMNLP 2022: 11215-11221\n",
      "Other 362: Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks -- EMNLP (Findings) 2022: 668-683\n",
      "Other 363: WeDef: Weakly Supervised Backdoor Defense for Text Classification -- EMNLP 2022: 11614-11626\n",
      "Other 364: SAT Backdoors: Depth Beats Size -- ESA 2022: 46:1-46:18\n",
      "Other 365: The Devil Is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models -- ESORICS (3) 2022: 776-783\n",
      "Other 366: TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors -- EuroS&P 2022: 684-702\n",
      "Other 367: A Pragmatic Label-Specific Backdoor Attack -- FCS 2022: 149-162\n",
      "Other 368: Backdoor Sets on Nowhere Dense SAT -- ICALP 2022: 91:1-91:20\n",
      "Other 369: Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks -- ICASSP 2022: 3333-3337\n",
      "Other 370: Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks -- ICASSP 2022: 96-100\n",
      "Other 371: When Does Backdoor Attack Succeed in Image Reconstruction? A Study of Heuristics vs -- Bi-Level Solution. ICASSP 2022: 4398-4402\n",
      "Other 372: Detecting Backdoor Attacks against Point Cloud Classifiers -- ICASSP 2022: 3159-3163\n",
      "Other 373: Towards Backdoor Attack on Deep Learning based Time Series Classification -- ICDE 2022: 1274-1287\n",
      "Other 374: Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models -- ICDIS 2022: 61-66\n",
      "Other 375: Backdoor Poisoning of Encrypted Traffic Classifiers -- ICDM (Workshops) 2022: 577-585\n",
      "Other 376: Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack -- ICIAP (2) 2022: 176-185\n",
      "Other 377: CRAB: Certified Patch Robustness Against Poisoning-Based Backdoor Attacks -- ICIP 2022: 2486-2490\n",
      "Other 378: Poisoning and Backdooring Contrastive Learning -- ICLR 2022\n",
      "Other 379: Backdoor Defense via Decoupling the Training Process -- ICLR 2022\n",
      "Other 380: Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios -- ICLR 2022\n",
      "Other 381: How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data -- ICLR 2022\n",
      "Other 382: Unlabeled Backdoor Poisoning in Semi-Supervised Learning -- ICME 2022: 1-6\n",
      "Other 383: A Generic Enhancer for Backdoor Attacks on Deep Neural Networks -- ICONIP (7) 2022: 296-307\n",
      "Other 384: Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers -- ICONIP (3) 2022: 216-227\n",
      "Other 385: Backdoors in Neural Models of Source Code -- ICPR 2022: 2892-2899\n",
      "Other 386: Detecting Backdoor Attacks on Deep Neural Networks Based on Model Parameters Analysis -- ICTAI 2022: 630-637\n",
      "Other 387: PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning -- IJCAI 2022: 680-686\n",
      "Other 388: Membership Inference via Backdooring -- IJCAI 2022: 3832-3838\n",
      "Other 389: Data-Efficient Backdoor Attacks -- IJCAI 2022: 3992-3998\n",
      "Other 390: Imperceptible Backdoor Attack: From Input Space to Feature Representation -- IJCAI 2022: 1736-1742\n",
      "Other 391: Latent Space-Based Backdoor Attacks Against Deep Neural Networks -- IJCNN 2022: 1-10\n",
      "Other 392: ACTSS: Input Detection Defense against Backdoor Attacks via Activation Subset Scanning -- IJCNN 2022: 1-8\n",
      "Other 393: TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers -- INFOCOM 2022: 1429-1438\n",
      "Other 394: An Improved Method for Making CNN Immune to Backdoor Attack by Activating Clustering -- ISCSIC 2022: 1-6\n",
      "Other 395: Practical Backdoor Attack Against Speaker Recognition System -- ISPEC 2022: 468-484\n",
      "Other 396: Energy-Based Learning for Preventing Backdoor Attack -- KSEM (3) 2022: 706-721\n",
      "Other 397: I Know Your Triggers: Defending Against Textual Backdoor Attacks with Benign Backdoor Augmentation -- MILCOM 2022: 442-449\n",
      "Other 398: BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label -- ACM Multimedia 2022: 678-686\n",
      "Other 399: Backdoor Attacks on Crowd Counting -- ACM Multimedia 2022: 5351-5360\n",
      "Other 400: Purifier: Plug-and-play Backdoor Mitigation for Pre-trained Models Via Anomaly Activation Suppression -- ACM Multimedia 2022: 4291-4299\n",
      "Other 401: ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks -- NDSS 2022\n",
      "Other 402: Handcrafted Backdoors in Deep Neural Networks -- NeurIPS 2022\n",
      "Other 403: BadPrompt: Backdoor Attacks on Continuous Prompts -- NeurIPS 2022\n",
      "Other 404: Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets -- NeurIPS 2022\n",
      "Other 405: Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples -- NeurIPS 2022\n",
      "Other 406: A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks -- NeurIPS 2022\n",
      "Other 407: Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class -- NeurIPS 2022\n",
      "Other 408: Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection -- NeurIPS 2022\n",
      "Other 409: Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch -- NeurIPS 2022\n",
      "Other 410: Training with More Confidence: Mitigating Injected and Natural Backdoors During Training -- NeurIPS 2022\n",
      "Other 411: Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork -- NeurIPS 2022\n",
      "Other 412: BackdoorBench: A Comprehensive Benchmark of Backdoor Learning -- NeurIPS 2022\n",
      "Other 413: Pre-activation Distributions Expose Backdoor Neurons -- NeurIPS 2022\n",
      "Other 414: How to Backdoor (Classic) McEliece and How to Guard Against Backdoors -- PQCrypto 2022: 24-44\n",
      "Other 415: Un-Fair Trojan: Targeted Backdoor Attacks Against Model Fairness -- SDS 2022: 1-9\n",
      "Other 416: Towards Backdoor Attacks against LiDAR Object Detection in Autonomous Driving -- SenSys 2022: 533-547\n",
      "Other 417: Energy-Based Learning for Polluted Outlier Detection in Backdoor -- SmartCloud 2022: 47-52\n",
      "Other 418: Inconspicuous Data Augmentation Based Backdoor Attack on Deep Neural Networks -- SOCC 2022: 1-6\n",
      "Other 419: Patch-Based Backdoors Detection and Mitigation with Feature Masking -- SocialSec 2022: 229-246\n",
      "Other 420: BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning -- SP 2022: 2043-2059\n",
      "Other 421: Big Brother Is Watching You: A Closer Look at Backdoor Construction -- SPACE 2022: 81-96\n",
      "Other 422: Big Brother Is Watching You: A Closer Look At Backdoor Construction -- IACR Cryptol. ePrint Arch. 2022: 953 (2022)\n",
      "Other 423: A Novel Backdoor Attack Adapted to Transfer Learning -- SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta 2022: 1730-1735\n",
      "Other 424: Low-Poisoning Rate Invisible Backdoor Attack Based on Important Neurons -- WASA (2) 2022: 375-383\n",
      "Other 425: Extracting a Minimal Trigger for an Efficient Backdoor Poisoning Attack Using the Activation Values of a Deep Neural Network -- WDC@AsiaCCS 2022: 3-6\n",
      "Other 426: Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers -- WiseML@WiSec 2022: 57-62\n",
      "Other 427: Deep Learning Backdoors -- Security and Artificial Intelligence 2022: 313-334\n",
      "Other 428: Hiding Behind Backdoors: Self-Obfuscation Against Generative Models -- CoRR abs/2201.09774 (2022)\n",
      "Other 429: Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire -- CoRR abs/2201.12211 (2022)\n",
      "Other 430: Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks -- CoRR abs/2201.13164 (2022)\n",
      "Other 431: False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger -- CoRR abs/2202.04479 (2022)\n",
      "Other 432: Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches -- CoRR abs/2202.10320 (2022)\n",
      "Other 433: Label-Smoothed Backdoor Attack -- CoRR abs/2202.11203 (2022)\n",
      "Other 434: Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild -- CoRR abs/2203.00858 (2022)\n",
      "Other 435: Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks -- CoRR abs/2203.03692 (2022)\n",
      "Other 436: PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks -- CoRR abs/2203.09289 (2022)\n",
      "Other 437: Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning -- CoRR abs/2203.15506 (2022)\n",
      "Other 438: An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks -- CoRR abs/2204.04329 (2022)\n",
      "Other 439: Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures -- CoRR abs/2204.06273 (2022)\n",
      "Other 440: Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering -- CoRR abs/2204.12848 (2022)\n",
      "Other 441: Model-Contrastive Learning for Backdoor Defense -- CoRR abs/2205.04411 (2022)\n",
      "Other 442: Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution -- CoRR abs/2205.09167 (2022)\n",
      "Other 443: Textual Backdoor Attacks with Iterative Trigger Injection -- CoRR abs/2205.12700 (2022)\n",
      "Other 444: Circumventing Backdoor Defenses That Are Based on Latent Separability -- CoRR abs/2205.13613 (2022)\n",
      "Other 445: Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations -- CoRR abs/2205.13616 (2022)\n",
      "Other 446: Defending Against Stealthy Backdoor Attacks -- CoRR abs/2205.14246 (2022)\n",
      "Other 447: CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences -- CoRR abs/2206.00145 (2022)\n",
      "Other 448: Can Backdoor Attacks Survive Time-Varying Models -- CoRR abs/2206.04677 (2022)\n",
      "Other 449: Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers -- CoRR abs/2206.04881 (2022)\n",
      "Other 450: DECK: Model Hardening for Defending Pervasive Backdoors -- CoRR abs/2206.09272 (2022)\n",
      "Other 451: Natural Backdoor Datasets -- CoRR abs/2206.10673 (2022)\n",
      "Other 452: Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain -- CoRR abs/2207.04209 (2022)\n",
      "Other 453: FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair -- CoRR abs/2207.12863 (2022)\n",
      "Other 454: Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons -- CoRR abs/2208.06537 (2022)\n",
      "Other 455: Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer -- CoRR abs/2208.06592 (2022)\n",
      "Other 456: MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World -- CoRR abs/2209.02339 (2022)\n",
      "Other 457: Adaptive Perturbation Generation for Multiple Backdoors Detection -- CoRR abs/2209.05244 (2022)\n",
      "Other 458: Augmentation Backdoors -- CoRR abs/2209.15139 (2022)\n",
      "Other 459: ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks -- CoRR abs/2210.00108 (2022)\n",
      "Other 460: Backdoor Attacks in the Supply Chain of Masked Image Modeling -- CoRR abs/2210.01632 (2022)\n",
      "Other 461: Understanding Impacts of Task Similarity on Backdoor Attack and Detection -- CoRR abs/2210.06509 (2022)\n",
      "Other 462: Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning -- CoRR abs/2210.11082 (2022)\n",
      "Other 463: Detecting Backdoors in Deep Text Classifiers -- CoRR abs/2210.11264 (2022)\n",
      "Other 464: M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models -- CoRR abs/2211.01875 (2022)\n",
      "Other 465: Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models -- CoRR abs/2211.02408 (2022)\n",
      "Other 466: Physics-Constrained Backdoor Attacks on Power System Fault Localization -- CoRR abs/2211.04445 (2022)\n",
      "Other 467: Backdoor Attacks on Time Series: A Generative Approach -- CoRR abs/2211.07915 (2022)\n",
      "Other 468: CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning -- CoRR abs/2211.08229 (2022)\n",
      "Other 469: PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking -- CoRR abs/2211.08697 (2022)\n",
      "Other 470: Backdoor Attacks on Multiagent Collaborative Systems -- CoRR abs/2211.11455 (2022)\n",
      "Other 471: Backdoor Vulnerabilities in Normally Trained Deep Learning Models -- CoRR abs/2211.15929 (2022)\n",
      "Other 472: Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning -- CoRR abs/2212.02582 (2022)\n",
      "Other 473: Fine-Tuning Is All You Need to Mitigate Backdoor Attacks -- CoRR abs/2212.09067 (2022)\n",
      "Other 474: Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation -- CoRR abs/2212.09979 (2022)\n",
      "Other 475: How to backdoor LWE-like cryptosystems -- IACR Cryptol. ePrint Arch. 2022: 1381 (2022)\n",
      "Other 476: How to Backdoor (Classical) McEliece and How to Guard Against Backdoors -- IACR Cryptol. ePrint Arch. 2022: 362 (2022)\n",
      "Other 477: Use Procedural Noise to Achieve Backdoor Attack -- IEEE Access 9: 127204-127216 (2021)\n",
      "Other 478: BDDR: An Effective Defense Against Textual Backdoor Attacks -- Comput. Secur. 110: 102433 (2021)\n",
      "Other 479: Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing -- Comput. Secur. 106: 102280 (2021)\n",
      "Other 480: Internet of Things backdoors: Resource management issues, security challenges, and detection methods -- Trans. Emerg. Telecommun. Technol. 32(2) (2021)\n",
      "Other 481: Study of scale-free structures in feed-forward neural networks against backdoor attacks -- ICT Express 7(2): 265-268 (2021)\n",
      "Other 482: Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification -- Neurocomputing 452: 253-262 (2021)\n",
      "Other 483: Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment -- IEEE J. Sel. Areas Commun. 39(8): 2617-2631 (2021)\n",
      "Other 484: Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set -- Neural Comput. 33(5): 1329-1371 (2021)\n",
      "Other 485: Stability-Based Analysis and Defense against Backdoor Attacks on Edge Computing Services -- IEEE Netw. 35(1): 163-169 (2021)\n",
      "Other 486: Existence versus exploitation: the opacity of backdoors and backbones -- Prog. Artif. Intell. 10(3): 297-308 (2021)\n",
      "Other 487: Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems -- Peer-to-Peer Netw. Appl. 14(3): 1458-1474 (2021)\n",
      "Other 488: PBDT: Python Backdoor Detection Model Based on Combined Features -- Secur. Commun. Networks 2021: 9923234:1-9923234:13 (2021)\n",
      "Other 489: Text Backdoor Detection Using an Interpretable RNN Abstract Model -- IEEE Trans. Inf. Forensics Secur. 16: 4117-4132 (2021)\n",
      "Other 490: Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures -- IEEE Trans. Inf. Forensics Secur. 16: 4827-4842 (2021)\n",
      "Other 491: Backdoor Decomposable Monotone Circuits and Propagation Complete Encodings -- AAAI 2021: 3832-3840\n",
      "Other 492: Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger -- ACL/IJCNLP (1) 2021: 443-453\n",
      "Other 493: Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution -- ACL/IJCNLP (1) 2021: 4873-4883\n",
      "Other 494: MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks -- ACM TUR-C 2021: 104-109\n",
      "Other 495: DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation -- AsiaCCS 2021: 363-377\n",
      "Other 496: Covid-19 digital Contact-tracing: a doorway to well-being or a backdoor to security vulnerabilities -- IEEE BigData 2021: 4297-4302\n",
      "Other 497: Countermeasures Against Backdoor Attacks Towards Malware Detectors -- CANS 2021: 295-314\n",
      "Other 498: Backdoor Pre-trained Models Can Transfer to All -- CCS 2021: 3141-3158\n",
      "Other 499: A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems -- CDC 2021: 4394-4399\n",
      "Other 500: Reasoning Short Cuts in Infinite Domain Constraint Satisfaction: Algorithms and Lower Bounds for Backdoors -- CP 2021: 32:1-32:20\n",
      "Other 501: Protecting Deep Cerebrospinal Fluid Cell Image Processing Models with Backdoor and Semi-Distillation -- DICTA 2021: 1-7\n",
      "Other 502: A Random Multi-target Backdooring Attack on Deep Neural Networks -- DMBD (2) 2021: 45-52\n",
      "Other 503: Backdoor Filter: Mitigating Visible Backdoor Triggers in Dataset -- DTPI 2021: 102-105\n",
      "Other 504: BFClass: A Backdoor-free Text Classification Framework -- EMNLP (Findings) 2021: 444-453\n",
      "Other 505: Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning -- EMNLP (1) 2021: 3023-3032\n",
      "Other 506: ONION: A Simple and Effective Defense Against Textual Backdoor Attacks -- EMNLP (1) 2021: 9558-9566\n",
      "Other 507: Pixdoor: A Pixel-space Backdoor Attack on Deep Learning Models -- EUSIPCO 2021: 681-685\n",
      "Other 508: Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack -- GLOBECOM 2021: 1-6\n",
      "Other 509: Why is Your Trojan NOT Responding? A Quantitative Analysis of Failures in Backdoor Attacks of Neural Networks -- ICA3PP (3) 2021: 754-771\n",
      "Other 510: Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff -- ICASSP 2021: 3855-3859\n",
      "Other 511: L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access to the Training Set -- ICASSP 2021: 3745-3749\n",
      "Other 512: Backdoor Attack Against Speaker Verification -- ICASSP 2021: 2560-2564\n",
      "Other 513: LIRA: Learnable, Imperceptible and Robust Backdoor Attacks -- ICCV 2021: 11946-11956\n",
      "Other 514: Black-box Detection of Backdoor Attacks with Limited Information and Data -- ICCV 2021: 16462-16471\n",
      "Other 515: Invisible Backdoor Attack with Sample-Specific Triggers -- ICCV 2021: 16443-16452\n",
      "Other 516: Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective -- ICCV 2021: 16453-16461\n",
      "Other 517: CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks -- ICCV 2021: 16433-16442\n",
      "Other 518: Backdoor Investigation and Incident Response: From Zero to Profit -- ICDF2C 2021: 229-247\n",
      "Other 519: Simtrojan: Stealthy Backdoor Attack -- ICIP 2021: 819-823\n",
      "Other 520: Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks -- ICLR 2021\n",
      "Other 521: WaNet - Imperceptible Warping-based Backdoor Attack -- ICLR 2021\n",
      "Other 522: Defense against backdoor attacks via robust covariance estimation -- ICML 2021: 4129-4139\n",
      "Other 523: Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks -- ICML 2021: 9389-9398\n",
      "Other 524: Backdoor Scanning for Deep Neural Networks through K-Arm Optimization -- ICML 2021: 9525-9536\n",
      "Other 525: DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection -- ICSE 2021: 263-274\n",
      "Other 526: Backdoor DNFs -- IJCAI 2021: 1403-1409\n",
      "Other 527: Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks -- INFOCOM 2021: 1-10\n",
      "Other 528: TridentShell: a Covert and Scalable Backdoor Injection Attack on Web Applications -- ISC 2021: 177-194\n",
      "Other 529: On-line Functional Testing of Memristor-mapped Deep Neural Networks using Backdoored Checksums -- ITC 2021: 83-92\n",
      "Other 530: The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game -- IUI 2021: 423-433\n",
      "Other 531: What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors -- KDD 2021: 1027-1035\n",
      "Other 532: Recursive Backdoors for SAT -- MFCS 2021: 73:1-73:18\n",
      "Other 533: Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation -- ACM Multimedia 2021: 826-834\n",
      "Other 534: Backdoor Attack with Imperceptible Input and Latent Modification -- NeurIPS 2021: 18944-18957\n",
      "Other 535: Anti-Backdoor Learning: Training Clean Models on Poisoned Data -- NeurIPS 2021: 14900-14912\n",
      "Other 536: Excess Capacity and Backdoor Poisoning -- NeurIPS 2021: 20373-20384\n",
      "Other 537: Identifying and blocking the backdoors in Linux -- RTA-CSIT 2021: 193-197\n",
      "Other 538: A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks -- UbiSec 2021: 1-12\n",
      "Other 539: Blind Backdoors in Deep Learning Models -- USENIX Security Symposium 2021: 1505-1521\n",
      "Other 540: Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers -- USENIX Security Symposium 2021: 1487-1504\n",
      "Other 541: Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection -- USENIX Security Symposium 2021: 1541-1558\n",
      "Other 542: Explainability Matters: Backdoor Attacks on Medical Imaging -- CoRR abs/2101.00008 (2021)\n",
      "Other 543: On Provable Backdoor Defense in Collaborative Learning -- CoRR abs/2101.08177 (2021)\n",
      "Other 544: DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations -- CoRR abs/2103.02079 (2021)\n",
      "Other 545: EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry -- CoRR abs/2103.08820 (2021)\n",
      "Other 546: TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation -- CoRR abs/2103.10274 (2021)\n",
      "Other 547: RABA: A Robust Avatar Backdoor Attack on Deep Neural Network -- CoRR abs/2104.01026 (2021)\n",
      "Other 548: SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics -- CoRR abs/2104.11315 (2021)\n",
      "Other 549: Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions -- CoRR abs/2106.07214 (2021)\n",
      "Other 550: Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting -- CoRR abs/2107.07240 (2021)\n",
      "Other 551: Can You Hear It? Backdoor Attacks via Ultrasonic Triggers -- CoRR abs/2107.14569 (2021)\n",
      "Other 552: The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks -- CoRR abs/2108.01644 (2021)\n",
      "Other 553: Quantization Backdoors to Deep Learning Models -- CoRR abs/2108.09187 (2021)\n",
      "Other 554: Backdoor Attacks on Network Certification via Data Poisoning -- CoRR abs/2108.11299 (2021)\n",
      "Other 555: Backdoor Attack and Defense for Deep Regression -- CoRR abs/2109.02381 (2021)\n",
      "Other 556: Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain -- CoRR abs/2109.05507 (2021)\n",
      "Other 557: Widen The Backdoor To Let More Attackers In -- CoRR abs/2110.04571 (2021)\n",
      "Other 558: An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware -- CoRR abs/2110.07683 (2021)\n",
      "Other 559: A Statistical Difference Reduction Method for Escaping Backdoor Detection -- CoRR abs/2111.05077 (2021)\n",
      "Other 560: An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences -- CoRR abs/2111.08429 (2021)\n",
      "Other 561: Backdoor Attack through Frequency Domain -- CoRR abs/2111.10991 (2021)\n",
      "Other 562: NTD: Non-Transferability Enabled Backdoor Detection -- CoRR abs/2111.11157 (2021)\n",
      "Other 563: How to Backdoor a Cipher -- IACR Cryptol. ePrint Arch. 2021: 442 (2021)\n",
      "Other 564: Factoring Primes to Factor Moduli: Backdooring and Distributed Generation of Semiprimes -- IACR Cryptol. ePrint Arch. 2021: 1610 (2021)\n",
      "Other 565: (Withdrawn) Spinning Sequence-to-Sequence Models with Meta-Backdoors -- CoRR abs/2107.10443 (2021)\n",
      "Other 566: (Withdrawn) CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing -- CoRR abs/2112.13064 (2021)\n",
      "Other 567: Detecting Backdoor Attacks via Class Difference in Deep Neural Networks -- IEEE Access 8: 191049-191056 (2020)\n",
      "Other 568: Backdoor Suppression in Neural Networks using Input Fuzzing and Majority Voting -- IEEE Des. Test 37(2): 103-110 (2020)\n",
      "Other 569: Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition -- EURASIP J. Inf. Secur. 2020: 12 (2020)\n",
      "Other 570: Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks -- IEICE Trans. Inf. Syst. 103-D(4): 883-887 (2020)\n",
      "Other 571: Backdoors into Two Occurrences -- J. Satisf. Boolean Model. Comput. 12(1): 1-15 (2020)\n",
      "Other 572: Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments -- IEEE Netw. 34(5): 141-147 (2020)\n",
      "Other 573: Hidden Trigger Backdoor Attacks -- AAAI 2020: 11957-11965\n",
      "Other 574: Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems -- ACM TUR-C 2020: 231-235\n",
      "Other 575: Differentiable Causal Backdoor Discovery -- AISTATS 2020: 3970-3979\n",
      "Other 576: Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features -- CCS 2020: 113-131\n",
      "Other 577: Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks -- AISec@CCS 2020: 117-127\n",
      "Other 578: Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation -- CODASPY 2020: 97-108\n",
      "Other 579: The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers -- CRYPTO (3) 2020: 249-278\n",
      "Other 580: The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers -- IACR Cryptol. ePrint Arch. 2020: 986 (2020)\n",
      "Other 581: Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers -- CVPR Workshops 2020: 3422-3431\n",
      "Other 582: One-Pixel Signature: Characterizing CNN Models for Backdoor Detection -- ECCV (27) 2020: 326-341\n",
      "Other 583: Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks -- ECCV (10) 2020: 182-199\n",
      "Other 584: Interpretability Derived Backdoor Attacks Detection in Deep Neural Networks: Work-in-Progress -- EMSOFT 2020: 13-14\n",
      "Other 585: Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating -- EuroS&P 2020: 184-197\n",
      "Other 586: Bypassing Backdoor Detection Algorithms in Deep Learning -- EuroS&P 2020: 175-183\n",
      "Other 587: Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification -- ICASSP 2020: 3827-3831\n",
      "Other 588: Backdooring Convolutional Neural Networks via Targeted Weight Perturbations -- IJCB 2020: 1-9\n",
      "Other 589: Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks -- ICDM 2020: 162-171\n",
      "Other 590: TargetNet Backdoor: Attack on Deep Neural Network with Use of Different Triggers -- ICIIT 2020: 140-145\n",
      "Other 591: A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks -- ICISS 2020: 69-80\n",
      "Other 592: Backdooring Deep Learning Architectures: Threats and (some) Opportunities -- ICISSP 2020: 15-16\n",
      "Other 593: Robust anomaly detection and backdoor attack detection via differential privacy -- ICLR 2020\n",
      "Other 594: FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network -- ICSIM 2020: 53-57\n",
      "Other 595: Application of complex systems in neural networks against Backdoor attacks -- ICTC 2020: 57-59\n",
      "Other 596: A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model -- MLHC 2020: 376-396\n",
      "Other 597: Revealing Perceptible Backdoors in DNNs, Without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic -- MLSP 2020: 1-6\n",
      "Other 598: GangSweep: Sweep out Neural Backdoors by GAN -- ACM Multimedia 2020: 3173-3181\n",
      "Other 599: Input-Aware Dynamic Backdoor Attack -- NeurIPS 2020\n",
      "Other 600: Scalable Backdoor Detection in Neural Networks -- ECML/PKDD (2) 2020: 289-304\n",
      "Other 601: Escaping Backdoor Attack Detection of Deep Learning -- SEC 2020: 431-445\n",
      "Other 602: Backdooring and Poisoning Neural Networks with Image-Scaling Attacks -- SP (Workshops) 2020: 41-47\n",
      "Other 603: Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity -- TCC (3) 2020: 241-273\n",
      "Other 604: Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity -- IACR Cryptol. ePrint Arch. 2020: 1199 (2020)\n",
      "Other 605: Detecting acoustic backdoor transmission of inaudible messages using deep learning -- WiseML@WiSec 2020: 80-85\n",
      "Other 606: NNoculation: Broad Spectrum and Targeted Treatment of Backdoored DNNs -- CoRR abs/2002.08313 (2020)\n",
      "Other 607: On Certifying Robustness against Backdoor Attacks via Randomized Smoothing -- CoRR abs/2002.11750 (2020)\n",
      "Other 608: Defending against Backdoor Attack on Deep Neural Networks -- CoRR abs/2002.12162 (2020)\n",
      "Other 609: Exploring Backdoor Poisoning Attacks Against Malware Classifiers -- CoRR abs/2003.01031 (2020)\n",
      "Other 610: Rethinking the Trigger of Backdoor Attack -- CoRR abs/2004.04692 (2020)\n",
      "Other 611: A new measure for overfitting and its implications for backdooring of deep learning -- CoRR abs/2006.06721 (2020)\n",
      "Other 612: FaceHack: Triggering backdoored facial recognition systems using facial characteristics -- CoRR abs/2006.11623 (2020)\n",
      "Other 613: Natural Backdoor Attack on Text Data -- CoRR abs/2006.16176 (2020)\n",
      "Other 614: Backdoor attacks and defenses in feature-partitioned collaborative learning -- CoRR abs/2007.03608 (2020)\n",
      "Other 615: Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review -- CoRR abs/2007.10760 (2020)\n",
      "Other 616: Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection -- CoRR abs/2007.14748 (2020)\n",
      "Other 617: Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks -- CoRR abs/2008.00123 (2020)\n",
      "Other 618: Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems -- CoRR abs/2009.06996 (2020)\n",
      "Other 619: What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors -- CoRR abs/2009.10639 (2020)\n",
      "Other 620: Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks -- CoRR abs/2010.03282 (2020)\n",
      "Other 621: Poisoned classifiers are not only backdoored, they are fundamentally broken -- CoRR abs/2010.09080 (2020)\n",
      "Other 622: On Evaluating Neural Network Backdoor Defenses -- CoRR abs/2010.12186 (2020)\n",
      "Other 623: EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks -- CoRR abs/2011.00101 (2020)\n",
      "Other 624: Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection -- CoRR abs/2011.02526 (2020)\n",
      "Other 625: Effect of backdoor attacks over the complexity of the latent space distribution -- CoRR abs/2012.01931 (2020)\n",
      "Other 626: Backdoor Attack with Sample-Specific Triggers -- CoRR abs/2012.03816 (2020)\n",
      "Other 627: HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios -- CoRR abs/2012.07474 (2020)\n",
      "Other 628: TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask) -- CoRR abs/2012.09302 (2020)\n",
      "Other 629: A Backdoor Attack Against LSTM-Based Text Classification Systems -- IEEE Access 7: 138872-138878 (2019)\n",
      "Other 630: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks -- IEEE Access 7: 47230-47244 (2019)\n",
      "Other 631: Backdoors to planning -- Artif. Intell. 269: 49-75 (2019)\n",
      "Other 632: Backdoors to Planning -- AAAI 2014: 2300-2307\n",
      "Other 633: Backdoors for Linear Temporal Logic -- Algorithmica 81(2): 476-496 (2019)\n",
      "Other 634: Backdoors for Linear Temporal Logic -- IPEC 2016: 23:1-23:17\n",
      "Other 635: Testing the Human Backdoor: Organizational Response to a Phishing Campaign -- J. Univers. Comput. Sci. 25(11): 1458-1477 (2019)\n",
      "Other 636: Interbank Networks and Backdoor Bailouts: Benefiting from Other Banks' Government Guarantees -- Manag. Sci. 65(8): 3673-3693 (2019)\n",
      "Other 637: Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering -- SafeAI@AAAI 2019\n",
      "Other 638: Latent Backdoor Attacks on Deep Neural Networks -- CCS 2019: 2041-2055\n",
      "Other 639: Backdoor Attacks in Neural Networks - A Systematic Evaluation on Multiple Traffic Sign Datasets -- CD-MAKE 2019: 285-300\n",
      "Other 640: Walling up Backdoors in Intrusion Detection Systems -- Big-DAMA@CoNEXT 2019: 8-13\n",
      "Other 641: A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning -- ICIP 2019: 101-105\n",
      "Other 642: Is Backside the New Backdoor in Modern SoCs?: Invited Paper -- ITC 2019: 1-10\n",
      "Other 643: Towards Leveraging Backdoors in Qualitative Constraint Networks -- KI 2019: 308-315\n",
      "Other 644: A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense -- MLSP 2019: 1-6\n",
      "Other 645: Defending Neural Backdoors via Generative Distribution Modeling -- NeurIPS 2019: 14004-14013\n",
      "Other 646: Existence Versus Exploitation: The Opacity of Backdoors and Backbones Under a Weak Assumption -- SOFSEM 2019: 247-259\n",
      "Other 647: True2F: Backdoor-Resistant Authentication Tokens -- IEEE Symposium on Security and Privacy 2019: 398-416\n",
      "Other 648: Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks -- IEEE Symposium on Security and Privacy 2019: 707-723\n",
      "Other 649: Design of intentional backdoors in sequential models -- CoRR abs/1902.09972 (2019)\n",
      "Other 650: BSEA-1 - A Stream Cipher Backdooring Technique -- CoRR abs/1903.11063 (2019)\n",
      "Other 651: Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks -- CoRR abs/1905.10447 (2019)\n",
      "Other 652: TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems -- CoRR abs/1908.01763 (2019)\n",
      "Other 653: Invisible Backdoor Attacks Against Deep Neural Networks -- CoRR abs/1909.02742 (2019)\n",
      "Other 654: NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations -- CoRR abs/1911.07399 (2019)\n",
      "Other 655: Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic -- CoRR abs/1911.07970 (2019)\n",
      "Other 656: Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks -- CoRR abs/1911.08040 (2019)\n",
      "Other 657: Label-Consistent Backdoor Attacks -- CoRR abs/1912.02771 (2019)\n",
      "Other 658: Backdoor detection systems for embedded devices -- University of Birmingham, UK, 2018\n",
      "Other 659: Technical perspective: Backdoor engineering -- Commun. ACM 61(11): 147 (2018)\n",
      "Other 660: Real-time Detection of Passive Backdoor Behaviors on Android System -- CNS 2018: 1-9\n",
      "Other 661: Learning-Sensitive Backdoors with Restarts -- CP 2018: 453-469\n",
      "Other 662: From Backdoor Key to Backdoor Completability: Improving a Known Measure of Hardness for the Satisfiable CSP -- CPAIOR 2018: 198-214\n",
      "Other 663: Combiners for Backdoored Random Oracles -- CRYPTO (2) 2018: 272-302\n",
      "Other 664: Combiners for Backdoored Random Oracles -- IACR Cryptol. ePrint Arch. 2018: 770 (2018)\n",
      "Other 665: Backdoored Hash Functions: Immunizing HMAC and HKDF -- CSF 2018: 105-118\n",
      "Other 666: Backdoored Hash Functions: Immunizing HMAC and HKDF -- IACR Cryptol. ePrint Arch. 2018: 362 (2018)\n",
      "Other 667: Remote Desktop Backdoor Implementation with Reverse TCP Payload Using Open Source Tools for Instructional Use -- EIT 2018: 249-254\n",
      "Other 668: Backdoor Attacks on Neural Network Operations -- GlobalSIP 2018: 1154-1158\n",
      "Other 669: UFO - Hidden Backdoor Discovery and Security Verification in IoT Device Firmware -- ISSRE Workshops 2018: 18-23\n",
      "Other 670: Spectral Signatures in Backdoor Attacks -- NeurIPS 2018: 8011-8021\n",
      "Other 671: Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks -- RAID 2018: 273-294\n",
      "Other 672: Backdoors: Definition, Deniability and Detection -- RAID 2018: 92-113\n",
      "Other 673: ALIAS: A Modular Tool for Finding Backdoors for SAT -- SAT 2018: 419-427\n",
      "Other 674: Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings -- CoRR abs/1811.09435 (2018)\n",
      "Other 675: How a simple bug in ML compiler could be exploited for backdoors -- CoRR abs/1811.10851 (2018)\n",
      "Other 676: On the Existence of Non-Linear Invariants and Algebraic Polynomial Constructive Approach to Backdoors in Block Ciphers -- IACR Cryptol. ePrint Arch. 2018: 807 (2018)\n",
      "Other 677: Backdoors into heterogeneous classes of SAT and CSP -- J. Comput. Syst. Sci. 85: 38-56 (2017)\n",
      "Other 678: Backdoors into Heterogeneous Classes of SAT and CSP -- AAAI 2014: 2652-2658\n",
      "Other 679: Open Sesame! Design and Implementation of Backdoor to Secretly Unlock Android Devices -- J. Internet Serv. Inf. Secur. 7(4): 35-44 (2017)\n",
      "Other 680: BackDoor: Sounds that a microphone can record, but that humans can't hear -- GetMobile Mob. Comput. Commun. 21(4): 25-29 (2017)\n",
      "Other 681: Backdoor attacks against learning systems -- CNS 2017: 1-9\n",
      "Other 682: Stringer: Measuring the Importance of Static Data Comparisons to Detect Backdoors and Undocumented Functionality -- ESORICS (2) 2017: 513-531\n",
      "Other 683: Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher -- ICISSP 2017: 622-631\n",
      "Other 684: Low-cost detection of backdoor malware -- ICITST 2017: 197-198\n",
      "Other 685: Backdoor Trees for Answer Set Programming -- ASPOCP@LPNMR 2017\n",
      "Other 686: BackDoor: Making Microphones Hear Inaudible Sounds -- MobiSys 2017: 2-14\n",
      "Other 687: Indiscreet Logs: Diffie-Hellman Backdoors in TLS -- NDSS 2017\n",
      "Other 688: Backdoor Treewidth for SAT -- SAT 2017: 20-37\n",
      "Other 689: Combining Treewidth and Backdoors for CSP -- STACS 2017: 36:1-36:17\n",
      "Other 690: Backdoor Sets for CSP -- The Constraint Satisfaction Problem 2017: 137-157\n",
      "Other 691: The Opacity of Backbones and Backdoors Under a Weak Assumption -- CoRR abs/1706.04582 (2017)\n",
      "Other 692: Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning -- CoRR abs/1712.05526 (2017)\n",
      "Other 693: Backdoors to q-Horn -- Algorithmica 74(1): 540-557 (2016)\n",
      "Other 694: Backdoors to q-Horn -- STACS 2013: 67-79\n",
      "Other 695: No backdoor required or expected -- Commun. ACM 59(6): 8-9 (2016)\n",
      "Other 696: Detecting Stealthy Backdoors and Port Knocking Sequences through Flow Analysis -- Prax. Inf.verarb. Kommun. 38(3-4): 97-104 (2016)\n",
      "Other 697: Backdoors to Tractable Valued CSP -- CP 2016: 233-250\n",
      "Other 698: Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results -- CRYPTO (1) 2016: 403-432\n",
      "Other 699: Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results -- IACR Cryptol. ePrint Arch. 2016: 577 (2016)\n",
      "Other 700: Strong Backdoors for Default Logic -- SAT 2016: 45-59\n",
      "Other 701: Backdoors to SAT -- Encyclopedia of Algorithms 2016: 167-170\n",
      "Other 702: Strong Backdoors for Linear Temporal Logic -- CoRR abs/1602.04934 (2016)\n",
      "Other 703: A Formal Treatment of Backdoored Pseudorandom Generators -- IACR Cryptol. ePrint Arch. 2016: 306 (2016)\n",
      "Other 704: A Formal Treatment of Backdoored Pseudorandom Generators -- EUROCRYPT (1) 2015: 101-126\n",
      "Other 705: Indiscreet Logs: Persistent Diffie-Hellman Backdoors in TLS -- IACR Cryptol. ePrint Arch. 2016: 999 (2016)\n",
      "Other 706: DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors -- IACR Cryptol. ePrint Arch. 2016: 245 (2016)\n",
      "Other 707: How to Backdoor Diffie-Hellman -- IACR Cryptol. ePrint Arch. 2016: 644 (2016)\n",
      "Other 708: Backdoors to tractable answer set programming -- Artif. Intell. 220: 64-103 (2015)\n",
      "Other 709: Backdoors to Normality for Disjunctive Logic Programs -- ACM Trans. Comput. Log. 17(1): 7 (2015)\n",
      "Other 710: Backdoors to Normality for Disjunctive Logic Programs -- AAAI 2013: 320-327\n",
      "Other 711: Variable-Deletion Backdoors to Planning -- AAAI 2015: 3305-3312\n",
      "Other 712: Internet-facing PLCs as a network backdoor -- CNS 2015: 524-532\n",
      "Other 713: Netzbasierte Erkennung von mittels Port Knocking verstecksten Dienstern und Backdoors -- DFN-Forum Kommunikationstechnologien 2015: 57-67\n",
      "Other 714: Integrated Sensor: A Backdoor for Hardware Trojan Insertions -- DSD 2015: 415-422\n",
      "Other 715: Covert remote syscall communication at kernel level: A SPOOKY backdoor -- MALWARE 2015: 74-81\n",
      "Other 716: Devil in a box: Installing backdoors in electronic door locks -- PST 2015: 139-144\n",
      "Other 717: Solving d-SAT via Backdoors to Small Treewidth -- SODA 2015: 630-641\n",
      "Other 718: Tradeoffs in the complexity of backdoors to satisfiability: dynamic sub-solvers and learning during search -- Ann. Math. Artif. Intell. 70(4): 399-431 (2014)\n",
      "Other 719: Backdoor -- Datenschutz und Datensicherheit 38(2): 119 (2014)\n",
      "Other 720: Answer Set Solver Backdoors -- JELIA 2014: 674-683\n",
      "Other 721: Backdoor Branching -- INFORMS J. Comput. 25(4): 693-700 (2013)\n",
      "Other 722: Backdoor Branching -- IPCO 2011: 183-191\n",
      "Other 723: Backdoors to the Tractability of Answer Set Programming -- Theory Pract. Log. Program. 13(4-5-Online-Supplement) (2013)\n",
      "Other 724: Backdoors to Tractability of Answer-Set Programming -- AAAI 2013: 1662-1663\n",
      "Other 725: Implementation and implications of a stealth hard-drive backdoor -- ACSAC 2013: 279-288\n",
      "Other 726: Towards reducing the attack surface of software backdoors -- CCS 2013: 851-862\n",
      "Other 727: Preventing Backdoors in Server Applications with a Separated Software Architecture - (Short Paper) -- DIMVA 2013: 197-206\n",
      "Other 728: Strong Backdoors to Bounded Treewidth SAT -- FOCS 2013: 489-498\n",
      "Other 729: Crowdsourcing Backdoor Identification for Combinatorial Optimization -- IJCAI 2013: 2840-2847\n",
      "Other 730: Backdoors to Abduction -- IJCAI 2013: 1046-1052\n",
      "Other 731: Upper and Lower Bounds for Weak Backdoor Set Detection -- SAT 2013: 394-402\n",
      "Other 732: Vulnerability-Based Backdoors: Threats from Two-step Trojans -- SERE 2013: 169-177\n",
      "Other 733: A generalized backdoor criterion -- CoRR abs/1307.5636 (2013)\n",
      "Other 734: Backdoors to Satisfaction -- The Multivariate Algorithmic Revolution and Beyond 2012: 287-317\n",
      "Other 735: Breakthrough Silicon Scanning Discovers Backdoor in Military Chip -- CHES 2012: 23-40\n",
      "Other 736: Backdoors to Acyclic SAT -- ICALP (1) 2012: 363-374\n",
      "Other 737: Detecting Stealthy Backdoors with Association Rule Mining -- Networking (2) 2012: 161-171\n",
      "Other 738: Strong Backdoors to Nested Satisfiability -- SAT 2012: 72-85\n",
      "Other 739: A Framework to Eliminate Backdoors from Response-Computable Authentication -- IEEE Symposium on Security and Privacy 2012: 3-17\n",
      "Other 740: Finding Small Backdoors in SAT Instances -- Canadian AI 2011: 269-280\n",
      "Other 741: Trusting the open latent IC backdoors -- STC@CCS 2011: 1-2\n",
      "Other 742: TorusDesktop: pointing via the backdoor is sometimes shorter -- CHI 2011: 829-838\n",
      "Other 743: Backdoors to Tractable Answer-Set Programming -- IJCAI 2011: 863-868\n",
      "Other 744: Silencing Hardware Backdoors -- IEEE Symposium on Security and Privacy 2011: 49-63\n",
      "Other 745: Static detection of application backdoors - Detecting both malicious software behavior and malicious indicators from the static analysis of executable code -- Datenschutz und Datensicherheit 34(3): 149-155 (2010)\n",
      "Other 746: Backdoor Sets of Quantified Boolean Formulas -- J. Autom. Reason. 42(1): 77-97 (2009)\n",
      "Other 747: Backdoor Sets of Quantified Boolean Formulas -- SAT 2007: 230-243\n",
      "Other 748: Matched Formulas and Backdoor Sets -- J. Satisf. Boolean Model. Comput. 6(1-3): 1-12 (2009)\n",
      "Other 749: Matched Formulas and Backdoor Sets -- SAT 2007: 94-99\n",
      "Other 750: CPU bugs, CPU backdoors and consequences on security -- J. Comput. Virol. 5(2): 91-104 (2009)\n",
      "Other 751: CPU Bugs, CPU Backdoors and Consequences on Security -- ESORICS 2008: 580-599\n",
      "Other 752: A chipset level network backdoor: bypassing host-based firewall & IDS -- AsiaCCS 2009: 125-134\n",
      "Other 753: Backdoors to Combinatorial Optimization: Feasibility and Optimality -- CPAIOR 2009: 56-70\n",
      "Other 754: A study on intrusion protection techniques against Linux kernel backdoor -- ICHIT 2009: 86-90\n",
      "Other 755: Backdoors in the Context of Learning -- SAT 2009: 73-79\n",
      "Other 756: Backdoor Trees -- AAAI 2008: 363-368\n",
      "Other 757: A New Empirical Study of Weak Backdoors -- CP 2008: 618-623\n",
      "Other 758: Tradeoffs in Backdoors: Inconsistency Detection, Dynamic Simplification, and Preprocessing -- ISAIM 2008\n",
      "Other 759: Computation of Renameable Horn Backdoors -- SAT 2008: 154-160\n",
      "Other 760: A New Bound for an NP-Hard Subclass of 3-SAT Using Backdoors -- SAT 2008: 161-167\n",
      "Other 761: Detecting and Guarding against Kernel Backdoors through Packet Flow Differentials -- IEICE Trans. Commun. 90-B(10): 2638-2645 (2007)\n",
      "Other 762: Tradeoffs in the Complexity of Backdoor Detection -- CP 2007: 256-270\n",
      "Other 763: From Horn Strong Backdoor Sets to Ordered Strong Backdoor Sets -- MICAI 2007: 105-117\n",
      "Other 764: COTS and other electronic voting backdoors -- Commun. ACM 49(11): 112 (2006)\n",
      "Other 765: Computing Horn Strong Backdoor Sets Thanks to Local Search -- ICTAI 2006: 139-143\n",
      "Other 766: Backdoor Sets for DLL Subsolvers -- J. Autom. Reason. 35(1-3): 73-88 (2005)\n",
      "Other 767: Backbones and Backdoors in Satisfiability -- AAAI 2005: 1368-1373\n",
      "Other 768: A self-checking signature scheme for checking backdoor security attacks in Internet -- J. High Speed Networks 13(4): 309-317 (2004)\n",
      "Other 769: The Backdoor Key: A Path to Understanding Problem Hardness -- AAAI 2004: 124-130\n",
      "Other 770: Backdoor Creativity: Collaborative Creativity in Technology Supported Teams -- COOP 2004: 99-114\n",
      "Other 771: Remote Repair of Operating System State Using Backdoors -- ICAC 2004: 256-263\n",
      "Other 772: Detecting Backdoor Sets with Respect to Horn and Binary Clauses -- SAT 2004\n",
      "Other 773: Backdoor Attacks on Black-Box Ciphers Exploiting Low-Entropy Plaintexts -- ACISP 2003: 297-311\n",
      "Other 774: Automatic Backdoor Analysis with Network Intrusion Detection System and Integrated Service Checker -- IAW 2003: 122-126\n",
      "Other 775: Backdoors To Typical Case Complexity -- IJCAI 2003: 1173-1178\n",
      "Other 776: Detecting Backdoors -- USENIX Security Symposium 2000\n"
     ]
    }
   ],
   "source": [
    "for id, pp in enumerate(data_cat['Others']):\n",
    "    print(f\"Other {id}: {pp['title']} -- {pp['info']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Orson Mengara, Anderson R. Avila, Tiago H. Falk:\n",
      "Backdoor Attacks to Deep Neural Networks: A Survey of the Literature, Challenges, and Future Research Directions. IEEE Access 12: 29004-29023 (2024)\n",
      "1 Quentin Le Roux, Eric Bourbao, Yannick Teglia, Kassem Kallas:\n",
      "A Comprehensive Survey on Backdoor Attacks and Their Defenses in Face Recognition Systems. IEEE Access 12: 47433-47468 (2024)\n",
      "2 Chengxu Yu, Yulai Zhang:\n",
      "Defending Against Backdoor Attacks by Quarantine Training. IEEE Access 12: 10681-10689 (2024)\n",
      "3 Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Weiqiang Liu:\n",
      "Imperceptible and multi-channel backdoor attack. Appl. Intell. 54(1): 1099-1116 (2024)\n",
      "4 Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai:\n",
      "Universal adversarial backdoor attacks to fool vertical federated learning. Comput. Secur. 137: 103601 (2024)\n",
      "5 Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong:\n",
      "SGBA: A stealthy scapegoat backdoor attack against deep neural networks. Comput. Secur. 136: 103523 (2024)\n",
      "6 Zihan Ma, Tianchong Gao:\n",
      "Federated learning backdoor attack detection with persistence diagram. Comput. Secur. 136: 103557 (2024)\n",
      "7 Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa D. Doan, Kok-Seng Wong:\n",
      "Backdoor attacks and defenses in federated learning: Survey, challenges and future research directions. Eng. Appl. Artif. Intell. 127(Part A): 107166 (2024)\n",
      "8 Shingo Yashiki, Chako Takahashi, Koutarou Suzuki:\n",
      "Backdoor Attacks on Graph Neural Networks Trained with Data Augmentation. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 107(3): 355-358 (2024)\n",
      "9 Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu:\n",
      "Enrollment-Stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound. IEEE Internet Things J. 11(8): 13108-13124 (2024)\n",
      "10 Yongkang Wang, Di-Hua Zhai, Dongyu Han, Yuyin Guan, Yuanqing Xia:\n",
      "MITDBA: Mitigating Dynamic Backdoor Attacks in Federated Learning for IoT Applications. IEEE Internet Things J. 11(6): 10115-10132 (2024)\n",
      "11 Xiangyu Zhao, Hanzhou Wu, Xinpeng Zhang:\n",
      "Effective Backdoor Attack on Graph Neural Networks in Spectral Domain. IEEE Internet Things J. 11(7): 12102-12114 (2024)\n",
      "12 Tao Xiang, Fei Ouyang, Di Zhang, Chunlong Xie, Hao Wang:\n",
      "NLPSweep: A comprehensive defense scheme for mitigating NLP backdoor attacks. Inf. Sci. 661: 120176 (2024)\n",
      "13 Shah Fahd, Mehreen Afzal, Waseem Iqbal, Dawood Shah, Ijaz Khalid:\n",
      "The reality of backdoored S-Boxes - An eye opener. J. Inf. Secur. Appl. 80: 103674 (2024)\n",
      "14 Xiaobo Yu, Weizhi Meng, Yi-Ning Liu, Fei Zhou:\n",
      "TridentShell: An enhanced covert and scalable backdoor injection attack on web applications. J. Netw. Comput. Appl. 223: 103823 (2024)\n",
      "15 Yinbo Yu, Jiajia Liu, Hongzhi Guo, Bomin Mao, Nei Kato:\n",
      "A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving. IEEE J. Sel. Areas Commun. 42(4): 948-962 (2024)\n",
      "16 Yubin Qu, Song Huang, Xiang Chen, Xingya Wang, Yongming Yao:\n",
      "Detection of backdoor attacks using targeted universal adversarial perturbations for deep neural networks. J. Syst. Softw. 207: 111859 (2024)\n",
      "17 Yuyuan Sun, Yuliang Lu, Xuehu Yan, Xuan Wang:\n",
      "Invisible backdoor learning in regional transform domain. Neural Comput. Appl. 36(14): 8097-8108 (2024)\n",
      "18 Jin Lee, Crystal Abidin:\n",
      "Backdoor advertising scandals, Yingyeo culture, and cancel culture among YouTube Influencers in South Korea. New Media Soc. 26(1): 405-425 (2024)\n",
      "19 Jianyao Yin, Honglong Chen, Junjian Li, Yudong Gao:\n",
      "Enhanced Coalescence Backdoor Attack Against DNN Based on Pixel Gradient. Neural Process. Lett. 56(2): 114 (2024)\n",
      "20 Yu Tang, Lijuan Sun, Xiaolong Xu:\n",
      "SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers. Pattern Recognit. Lett. 177: 103-109 (2024)\n",
      "21 Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, Guohan Huang:\n",
      "Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. IEEE Trans. Comput. Soc. Syst. 11(2): 2479-2493 (2024)\n",
      "22 Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, Jinyin Chen:\n",
      "Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection. IEEE Trans. Comput. Soc. Syst. 11(2): 1816-1831 (2024)\n",
      "23 Wenbo Jiang, Tianwei Zhang, Han Qiu, Hongwei Li, Guowen Xu:\n",
      "Incremental Learning, Incremental Backdoor Threats. IEEE Trans. Dependable Secur. Comput. 21(2): 559-572 (2024)\n",
      "24 Zekai Chen, Shengxing Yu, Mingyuan Fan, Ximeng Liu, Robert H. Deng:\n",
      "Privacy-Enhancing and Robust Backdoor Defense for Federated Learning on Heterogeneous Data. IEEE Trans. Inf. Forensics Secur. 19: 693-707 (2024)\n",
      "25 Yi Ding, Zi Wang, Zhen Qin, Erqiang Zhou, Guobin Zhu, Zhiguang Qin, Kim-Kwang Raymond Choo:\n",
      "Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network. IEEE Trans. Inf. Forensics Secur. 19: 280-292 (2024)\n",
      "26 Linkun Fan, Fazhi He, Tongzhen Si, Rubin Fan, Chuanlong Ye, Bing Li:\n",
      "MBA: Backdoor Attacks Against 3D Mesh Classifier. IEEE Trans. Inf. Forensics Secur. 19: 2127-2142 (2024)\n",
      "27 Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia:\n",
      "Imperceptible and Robust Backdoor Attack in 3D Point Cloud. IEEE Trans. Inf. Forensics Secur. 19: 1267-1282 (2024)\n",
      "28 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "Universal Detection of Backdoor Attacks via Density-Based Clustering and Centroids Analysis. IEEE Trans. Inf. Forensics Secur. 19: 970-984 (2024)\n",
      "29 Yu Guo, Yu Zhao, Saihui Hou, Cong Wang, Xiaohua Jia:\n",
      "Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers. IEEE Trans. Inf. Forensics Secur. 19: 708-721 (2024)\n",
      "30 Ying He, Zhili Shen, Jingyu Hua, Qixuan Dong, Jiacheng Niu, Wei Tong, Xu Huang, Chen Li, Sheng Zhong:\n",
      "Backdoor Attack Against Split Neural Network-Based Vertical Federated Learning. IEEE Trans. Inf. Forensics Secur. 19: 748-763 (2024)\n",
      "31 Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott:\n",
      "NTD: Non-Transferability Enabled Deep Learning Backdoor Detection. IEEE Trans. Inf. Forensics Secur. 19: 104-119 (2024)\n",
      "32 Huaibing Peng, Huming Qiu, Hua Ma, Shuo Wang, Anmin Fu, Said F. Al-Sarawi, Derek Abbott, Yansong Gao:\n",
      "On Model Outsourcing Adaptive Attacks to Deep Learning Backdoor Defenses. IEEE Trans. Inf. Forensics Secur. 19: 2356-2369 (2024)\n",
      "33 Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao:\n",
      "Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures. IEEE Trans. Inf. Forensics Secur. 19: 455-468 (2024)\n",
      "34 Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao:\n",
      "Invisible Backdoor Attack With Dynamic Triggers Against Person Re-Identification. IEEE Trans. Inf. Forensics Secur. 19: 307-319 (2024)\n",
      "35 Yongkang Wang, Di-Hua Zhai, Yuanqing Xia, Danyang Liu:\n",
      "PerVK: A Robust Personalized Federated Framework to Defend Against Backdoor Attacks for IoT Applications. IEEE Trans. Ind. Informatics 20(3): 4930-4939 (2024)\n",
      "36 Mingfu Xue, Yinghao Wu, Shifeng Ni, Leo Yu Zhang, Yushu Zhang, Weiqiang Liu:\n",
      "Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger. IEEE Trans. Ind. Informatics 20(3): 5004-5013 (2024)\n",
      "37 Zheng Zhang, Xu Yuan, Lei Zhu, Jingkuan Song, Liqiang Nie:\n",
      "BadCM: Invisible Backdoor Attack Against Cross-Modal Learning. IEEE Trans. Image Process. 33: 2558-2571 (2024)\n",
      "38 Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, Hui Fang:\n",
      "Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring. ACM Trans. Intell. Syst. Technol. 15(1): 5:1-5:25 (2024)\n",
      "39 Wei Jiang, Xiangyu Wen, Jinyu Zhan, Xupeng Wang, Ziwei Song, Chen Bian:\n",
      "Critical Path-Based Backdoor Detection for Deep Neural Networks. IEEE Trans. Neural Networks Learn. Syst. 35(3): 4032-4046 (2024)\n",
      "40 Yiming Li, Yong Jiang, Zhifeng Li, Shu-Tao Xia:\n",
      "Backdoor Learning: A Survey. IEEE Trans. Neural Networks Learn. Syst. 35(1): 5-22 (2024)\n",
      "41 Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Yi Liu:\n",
      "Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction. IEEE Trans. Netw. Sci. Eng. 11(1): 525-542 (2024)\n",
      "42 Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo:\n",
      "Stealthy Backdoor Attack for Code Models. IEEE Trans. Software Eng. 50(4): 721-741 (2024)\n",
      "43 Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu:\n",
      "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract). AAAI 2024: 23506-23507\n",
      "44 Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Ming Hu, Tianlin Li, Geguang Pu, Yang Liu:\n",
      "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models. AAAI 2024: 21169-21178\n",
      "45 Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao, Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu Zhang:\n",
      "Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift. AAAI 2024: 10847-10855\n",
      "46 Yiming Chen, Haiwei Wu, Jiantao Zhou:\n",
      "Progressive Poisoned Data Isolation for Training-Time Backdoor Defense. AAAI 2024: 11425-11433\n",
      "47 Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang:\n",
      "BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning. AAAI 2024: 11687-11694\n",
      "48 Qiuyu Duan, Zhongyun Hua, Qing Liao, Yushu Zhang, Leo Yu Zhang:\n",
      "Conditional Backdoor Attack via JPEG Compression. AAAI 2024: 19823-19831\n",
      "49 Linkun Fan, Fazhi He, Tongzhen Si, Wei Tang, Bing Li:\n",
      "Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain. AAAI 2024: 21072-21080\n",
      "50 Junpeng Fang, Gongduo Zhang, Qing Cui, Caizhi Tang, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou:\n",
      "Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations. AAAI 2024: 11944-11952\n",
      "51 Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang, Weifeng Liu:\n",
      "A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. AAAI 2024: 1851-1859\n",
      "52 Tran Huynh, Dang Nguyen, Tung Pham, Anh Tran:\n",
      "COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks. AAAI 2024: 2436-2444\n",
      "53 Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis:\n",
      "Temporal-Distributed Backdoor Attack against Video Based Action Recognition. AAAI 2024: 3199-3207\n",
      "54 Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, Xiaochun Cao:\n",
      "Does Few-Shot Learning Suffer from Backdoor Attacks? AAAI 2024: 19893-19901\n",
      "55 Zihao Liu, Tianhao Wang, Mengdi Huai, Chenglin Miao:\n",
      "Backdoor Attacks via Machine Unlearning. AAAI 2024: 14115-14123\n",
      "56 Tao Liu, Yuhang Zhang, Zhu Feng, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang:\n",
      "Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning. AAAI 2024: 21359-21367\n",
      "57 Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng:\n",
      "Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. AAAI 2024: 14677-14685\n",
      "58 Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang:\n",
      "Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks. AAAI 2024: 274-282\n",
      "59 Fangchao Yu, Bo Zeng, Kai Zhao, Zhi Pang, Lina Wang:\n",
      "Chronic Poisoning: Backdoor Attack against Split Learning. AAAI 2024: 16531-16538\n",
      "60 Yue Zhao, Congyi Li, Kai Chen:\n",
      "UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation. AAAI 2024: 21823-21831\n",
      "61 Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma:\n",
      "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models. AAAI 2024: 21850-21858\n",
      "62 Liuwan Zhu, Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu:\n",
      "SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly. AAAI 2024: 7766-7774\n",
      "63 Yue Wang, Wenqing Li, Michail Maniatakos, Saif Eddin Jabari:\n",
      "Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. ANZCC 2024: 115-120\n",
      "64 Ryan Holland, Shantanu Pal, Lei Pan, Leo Yu Zhang:\n",
      "Backdoor Attacks and Generative Model Fairness: Current Trends and Future Research Directions. COMSNETS 2024: 31-36\n",
      "65 SeokHee Kim, Changhee Hahn:\n",
      "Detecting Backdoors Embedded in Ensembles. ICEIC 2024: 1-3\n",
      "66 Samaneh Shamshiri, Insoo Sohn:\n",
      "Defense Method Challenges Against Backdoor Attacks in Neural Networks. ICAIIC 2024: 396-400\n",
      "67 Ryo Meguro, Hiroya Kato, Shintaro Narisada, Seira Hidano, Kazuhide Fukushima, Takuo Suganuma, Masahiro Hiji:\n",
      "Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks. ICISSP 2024: 510-521\n",
      "68 He Cheng, Shuhan Yuan:\n",
      "Backdoor Attack Against One-Class Sequential Anomaly Detection Models. PAKDD (3) 2024: 262-274\n",
      "69 Xi Li, Chen Wu, Jiaqi Wang:\n",
      "Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning. PAKDD (3) 2024: 168-181\n",
      "70 Hannah Davis, Matthew D. Green, Nadia Heninger, Keegan Ryan, Adam Suhl:\n",
      "On the Possibility of a Backdoor in the Micali-Schnorr Generator. Public Key Cryptography (1) 2024: 352-386\n",
      "71 Akshayvarun Subramanya, Soroush Abbasi Koohpayegani, Aniruddha Saha, Ajinkya Tejankar, Hamed Pirsiavash:\n",
      "A Closer Look at Robustness of Vision Transformers to Backdoor Attacks. WACV 2024: 3862-3871\n",
      "72 Qiannan Wang, Changchun Yin, Liming Fang, Lu Zhou, Zhe Liu, Run Wang, Chenhao Lin:\n",
      "SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection. CoRR abs/2401.00137 (2024)\n",
      "73 Xiaogang Xing, Ming Xu, Yujing Bai, Dongdong Yang:\n",
      "A clean-label graph backdoor attack method in node classification task. CoRR abs/2401.00163 (2024)\n",
      "74 Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong:\n",
      "Is It Possible to Backdoor Face Forgery Detection with Natural Triggers? CoRR abs/2401.00414 (2024)\n",
      "75 Ka Ho Chow, Wenqi Wei, Lei Yu:\n",
      "Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control. CoRR abs/2401.01085 (2024)\n",
      "76 Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, Xiaochun Cao:\n",
      "Does Few-shot Learning Suffer from Backdoor Attacks? CoRR abs/2401.01377 (2024)\n",
      "77 Orson Mengara:\n",
      "The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. CoRR abs/2401.01537 (2024)\n",
      "78 Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li:\n",
      "Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. CoRR abs/2401.01911 (2024)\n",
      "79 Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang:\n",
      "Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack. CoRR abs/2401.02031 (2024)\n",
      "80 Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li:\n",
      "Object-oriented backdoor attack against image captioning. CoRR abs/2401.02600 (2024)\n",
      "81 Jiayi Hua, Kailong Wang, Meizhen Wang, Guangdong Bai, Xiapu Luo, Haoyu Wang:\n",
      "MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack. CoRR abs/2401.02659 (2024)\n",
      "82 Jiazhu Dai, Haoyu Sun:\n",
      "A backdoor attack against link prediction tasks with graph neural networks. CoRR abs/2401.02663 (2024)\n",
      "83 Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey:\n",
      "End-to-End Anti-Backdoor Learning on Images and Time Series. CoRR abs/2401.03215 (2024)\n",
      "84 Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi:\n",
      "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. CoRR abs/2401.04136 (2024)\n",
      "85 Khondoker Murad Hossain, Tim Oates:\n",
      "TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. CoRR abs/2401.05432 (2024)\n",
      "86 Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen:\n",
      "Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. CoRR abs/2401.05949 (2024)\n",
      "87 Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan:\n",
      "Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation. CoRR abs/2401.06030 (2024)\n",
      "88 Junyang Cai, Taoan Huang, Bistra Dilkina:\n",
      "Learning Backdoors for Mixed Integer Programs with Contrastive Learning. CoRR abs/2401.10467 (2024)\n",
      "89 Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li:\n",
      "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. CoRR abs/2401.12242 (2024)\n",
      "90 Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, Baoyuan Wu:\n",
      "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition. CoRR abs/2401.13578 (2024)\n",
      "91 Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen:\n",
      "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning. CoRR abs/2401.15002 (2024)\n",
      "92 Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang:\n",
      "Multi-Trigger Backdoor Attacks: More Triggers, More Threats. CoRR abs/2401.15295 (2024)\n",
      "93 Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang:\n",
      "TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability. CoRR abs/2401.15883 (2024)\n",
      "94 Xi Li, Hang Wang, David J. Miller, George Kesidis:\n",
      "Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks. CoRR abs/2402.02034 (2024)\n",
      "95 Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan:\n",
      "DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models. CoRR abs/2402.02739 (2024)\n",
      "96 Gorka Abad, Stjepan Picek, Aitor Urbieta:\n",
      "Time-Distributed Backdoor Attacks on Federated Spiking Learning. CoRR abs/2402.02886 (2024)\n",
      "97 Orson Mengara:\n",
      "The last Dance : Robust backdoor attack via diffusion models and bayesian approach. CoRR abs/2402.05967 (2024)\n",
      "98 Harry Langford, Ilia Shumailov, Yiren Zhao, Robert D. Mullins, Nicolas Papernot:\n",
      "Architectural Neural Backdoors from First Principles. CoRR abs/2402.06957 (2024)\n",
      "99 Irina Alekseevskaia, Konstantin Arkhipenko:\n",
      "OrderBkd: Textual backdoor attack through repositioning. CoRR abs/2402.07689 (2024)\n",
      "100 Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin:\n",
      "Test-Time Backdoor Attacks on Multimodal Large Language Models. CoRR abs/2402.08577 (2024)\n",
      "101 He Cheng, Shuhan Yuan:\n",
      "Backdoor Attack against One-Class Sequential Anomaly Detection Models. CoRR abs/2402.10283 (2024)\n",
      "102 Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun:\n",
      "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents. CoRR abs/2402.11208 (2024)\n",
      "103 Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao:\n",
      "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection. CoRR abs/2402.11473 (2024)\n",
      "104 Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu:\n",
      "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space. CoRR abs/2402.12026 (2024)\n",
      "105 Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen:\n",
      "Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning. CoRR abs/2402.12168 (2024)\n",
      "106 Quanyu Long, Yue Deng, Leilei Gan, Wenya Wang, Sinno Jialin Pan:\n",
      "Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation. CoRR abs/2402.13532 (2024)\n",
      "107 Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao:\n",
      "VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models. CoRR abs/2402.13851 (2024)\n",
      "108 Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao:\n",
      "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment. CoRR abs/2402.14968 (2024)\n",
      "109 Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong:\n",
      "Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models. CoRR abs/2402.14977 (2024)\n",
      "110 Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang:\n",
      "Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm. CoRR abs/2402.15653 (2024)\n",
      "111 Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg:\n",
      "On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem. CoRR abs/2402.16926 (2024)\n",
      "112 Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu:\n",
      "Model X-ray: Detect Backdoored Models via Decision Boundary. CoRR abs/2402.17465 (2024)\n",
      "113 Alexander Unnervik, Hatef Otroshi-Shahreza, Anjith George, Sébastien Marcel:\n",
      "Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks. CoRR abs/2402.18718 (2024)\n",
      "114 Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu:\n",
      "Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models. CoRR abs/2402.18945 (2024)\n",
      "115 Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu:\n",
      "Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge. CoRR abs/2402.19334 (2024)\n",
      "116 Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu:\n",
      "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection. CoRR abs/2403.01472 (2024)\n",
      "117 Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng:\n",
      "A general approach to enhance the survivability of backdoor attacks by decision path coupling. CoRR abs/2403.02950 (2024)\n",
      "118 Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen:\n",
      "On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder. CoRR abs/2403.03846 (2024)\n",
      "119 Yuhao Bian, Shengjing Tian, Xiuping Liu:\n",
      "MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror. CoRR abs/2403.05847 (2024)\n",
      "120 Zhenbo Song, Wenhao Gao, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu:\n",
      "AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration. CoRR abs/2403.06430 (2024)\n",
      "121 Hong Sun, Ziqiang Li, Lei Liu, Bin Li:\n",
      "Real is not True: Backdoor Attacks Against Deepfake Detection. CoRR abs/2403.06610 (2024)\n",
      "122 Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang:\n",
      "Backdoor Attack with Mode Mixture Latent Modification. CoRR abs/2403.07463 (2024)\n",
      "123 Khondoker Murad Hossain, Tim Oates:\n",
      "Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks. CoRR abs/2403.08208 (2024)\n",
      "124 Samuel Pagliarini, Aikata, Malik Imran, Sujoy Sinha Roy:\n",
      "REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography. CoRR abs/2403.09352 (2024)\n",
      "125 Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu:\n",
      "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency. CoRR abs/2403.10717 (2024)\n",
      "126 Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang:\n",
      "Impart: An Imperceptible and Effective Label-Specific Backdoor Attack. CoRR abs/2403.13017 (2024)\n",
      "127 Wenmin Chen, Xiaowei Xu:\n",
      "Invisible Backdoor Attack Through Singular Value Decomposition. CoRR abs/2403.13018 (2024)\n",
      "128 Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu:\n",
      "BadEdit: Backdooring large language models by model editing. CoRR abs/2403.13355 (2024)\n",
      "129 Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang:\n",
      "Clean-image Backdoor Attacks. CoRR abs/2403.15010 (2024)\n",
      "130 Aryan Satpathy, Nilaksh Nilaksh, Dhruva Rajwade:\n",
      "An Embarrassingly Simple Defense Against Backdoor Attacks On SSL. CoRR abs/2403.15918 (2024)\n",
      "131 Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao:\n",
      "Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning. CoRR abs/2403.16257 (2024)\n",
      "132 Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum:\n",
      "Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion. CoRR abs/2403.16365 (2024)\n",
      "133 Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag:\n",
      "Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors. CoRR abs/2403.16569 (2024)\n",
      "134 Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen:\n",
      "Task-Agnostic Detector for Insertion-Based Backdoor Attacks. CoRR abs/2403.17155 (2024)\n",
      "135 Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang:\n",
      "LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning. CoRR abs/2403.17188 (2024)\n",
      "136 Jane Downer, Ren Wang, Binghui Wang:\n",
      "Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs. CoRR abs/2403.18136 (2024)\n",
      "137 Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu:\n",
      "Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices. CoRR abs/2403.18607 (2024)\n",
      "138 Orson Mengara:\n",
      "A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks. CoRR abs/2404.00076 (2024)\n",
      "139 Shanglun Feng, Florian Tramèr:\n",
      "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models. CoRR abs/2404.00473 (2024)\n",
      "140 Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti:\n",
      "UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models. CoRR abs/2404.01101 (2024)\n",
      "141 Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini:\n",
      "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models. CoRR abs/2404.01231 (2024)\n",
      "142 Mihir Bellare, Doreen Riepel, Laura Shea:\n",
      "Highly-Effective Backdoors for Hash Functions and Beyond. IACR Cryptol. ePrint Arch. 2024: 536 (2024)\n",
      "143 Liuwan Zhu:\n",
      "Towards a Robust Defense: A Multifaceted Approach to the Detection and Mitigation of Neural Backdoor Attacks through Feature Space Exploration and Analysis. Old Dominion University, Norfolk, Virginia, USA, 2023\n",
      "144 Tae-Hoon Kim, SeokHwan Choi, Yoon-Ho Choi:\n",
      "Instance-Agnostic and Practical Clean Label Backdoor Attack Method for Deep Learning Based Face Recognition Models. IEEE Access 11: 144040-144050 (2023)\n",
      "145 Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "Compression-resistant backdoor attack against deep neural networks. Appl. Intell. 53(17): 20402-20417 (2023)\n",
      "146 Yunchun Zhang, Fan Feng, Zikun Liao, Zixuan Li, Shaowen Yao:\n",
      "Universal backdoor attack on deep neural networks for malware detection. Appl. Soft Comput. 143: 110389 (2023)\n",
      "147 Xue Jiang, Lubin Meng, Siyang Li, Dongrui Wu:\n",
      "Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces. Sci. China Inf. Sci. 66(8) (2023)\n",
      "148 Yunjie Ge, Qian Wang, Jiayuan Yu, Chao Shen, Qi Li:\n",
      "Data Poisoning and Backdoor Attacks on Audio Intelligence Systems. IEEE Commun. Mag. 61(12): 176-182 (2023)\n",
      "149 Yuhao Gu, Yuebin Bai:\n",
      "LR-BA: Backdoor attack against vertical federated learning using local latent representations. Comput. Secur. 129: 103193 (2023)\n",
      "150 Binhao Ma, Can Zhao, Dejun Wang, Bo Meng:\n",
      "DIHBA: Dynamic, invisible and high attack success rate boundary backdoor attack with low poison ratio. Comput. Secur. 129: 103212 (2023)\n",
      "151 Jiaoze Mao, Yaguan Qian, Jianchang Huang, Zejie Lian, Renhui Tao, Bin Wang, Wei Wang, Tengteng Yao:\n",
      "Object-free backdoor attack and defense on semantic segmentation. Comput. Secur. 132: 103365 (2023)\n",
      "152 Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay:\n",
      "Towards Backdoor Attacks and Defense in Robust Machine Learning Models. Comput. Secur. 127: 103101 (2023)\n",
      "153 Yongkang Wang, Di-Hua Zhai, Yuanqing Xia:\n",
      "SCFL: Mitigating backdoor attacks in federated learning based on SVD and clustering. Comput. Secur. 133: 103414 (2023)\n",
      "154 Chengcheng Zhu, Jiale Zhang, Xiaobing Sun, Bing Chen, Weizhi Meng:\n",
      "ADFL: Defending backdoor attacks in federated learning via adversarial distillation. Comput. Secur. 132: 103366 (2023)\n",
      "155 Marco Cesati:\n",
      "A New Idea for RSA Backdoors. Cryptogr. 7(3): 45 (2023)\n",
      "156 Zonghao Ying, Bin Wu:\n",
      "DLP: towards active defense against backdoor attacks with decoupled learning process. Cybersecur. 6(1): 9 (2023)\n",
      "157 Zonghao Ying, Bin Wu:\n",
      "NBA: defensive distillation for backdoor removal via neural behavior alignment. Cybersecur. 6(1): 20 (2023)\n",
      "158 Kun Shao, Jun-an Yang, Pengjiang Hu, Xiaoshuai Li:\n",
      "A Textual Backdoor Defense Method Based on Deep Feature Classification. Entropy 25(2): 220 (2023)\n",
      "159 Shengchuan Zhang, Suhang Ye:\n",
      "Backdoor Attack against Face Sketch Synthesis. Entropy 25(7): 974 (2023)\n",
      "160 Sara Kaviani, Samaneh Shamshiri, Insoo Sohn:\n",
      "A defense method against backdoor attacks on neural networks. Expert Syst. Appl. 213(Part): 118990 (2023)\n",
      "161 Yongkang Wang, Di-Hua Zhai, Yongping He, Yuanqing Xia:\n",
      "An adaptive robust defending algorithm against backdoor attacks in federated learning. Future Gener. Comput. Syst. 143: 118-131 (2023)\n",
      "162 Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun:\n",
      "Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-level Backdoor Attacks. Mach. Intell. Res. 20(2): 180-193 (2023)\n",
      "163 Yang Chen, Zhonglin Ye, Haixing Zhao, Ying Wang:\n",
      "Feature-Based Graph Backdoor Attack in the Node Classification Task. Int. J. Intell. Syst. 2023: 1-13 (2023)\n",
      "164 Wenjuan Lian, Yichi Zhang, Xin Chen, Bin Jia, Xiaosong Zhang:\n",
      "IPCADP-Equalizer: An Improved Multibalance Privacy Preservation Scheme against Backdoor Attacks in Federated Learning. Int. J. Intell. Syst. 2023: 1-20 (2023)\n",
      "165 Yier Wei, Haichang Gao, Yufei Wang, Yipeng Gao, Huan Liu:\n",
      "A lightweight backdoor defense framework based on image inpainting. Neurocomputing 537: 22-36 (2023)\n",
      "166 Shafkat Islam, Shahriar Badsha, Ibrahim Khalil, Mohammed Atiquzzaman, Charalambos Konstantinou:\n",
      "A Triggerless Backdoor Attack and Defense Mechanism for Intelligent Task Offloading in Multi-UAV Systems. IEEE Internet Things J. 10(7): 5719-5732 (2023)\n",
      "167 Shanshan Li, Chunxiang Xu, Yuan Zhang, Yicong Du, Anjia Yang, Xinsheng Wen, Kefei Chen:\n",
      "Backdoor-Resistant Public Data Integrity Verification Scheme Based on Smart Contracts. IEEE Internet Things J. 10(16): 14269-14284 (2023)\n",
      "168 Tian Liu, Xueyang Hu, Tao Shu:\n",
      "Facilitating Early-Stage Backdoor Attacks in Federated Learning With Whole Population Distribution Inference. IEEE Internet Things J. 10(12): 10385-10399 (2023)\n",
      "169 Qiang Liu, Tongqing Zhou, Zhiping Cai, Yuan Yuan, Ming Xu, Jiaohua Qin, Wentao Ma:\n",
      "Turning backdoors for efficient privacy protection against image retrieval violations. Inf. Process. Manag. 60(5): 103471 (2023)\n",
      "170 Le Feng, Zhenxing Qian, Xinpeng Zhang, Sheng Li:\n",
      "Unlabeled backdoor poisoning on trained-from-scratch semi-supervised learning. Inf. Sci. 647: 119453 (2023)\n",
      "171 Shangxi Wu, Qiuyang He, Yi Zhang, Dongyuan Lu, Jitao Sang:\n",
      "Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias. Inf. Sci. 643: 119171 (2023)\n",
      "172 Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "Detecting backdoor in deep neural networks via intentional adversarial perturbations. Inf. Sci. 634: 564-577 (2023)\n",
      "173 Deshan Yang, Senlin Luo, Jinjie Zhou, Limin Pan, Xiaonan Yang, Jiyuan Xing:\n",
      "Efficient and persistent backdoor attack by boundary trigger set constructing against federated learning. Inf. Sci. 651: 119743 (2023)\n",
      "174 Ruinan Jin, Xiaoxiao Li:\n",
      "Backdoor attack and defense in federated generative adversarial network-based medical image synthesis. Medical Image Anal. 90: 102965 (2023)\n",
      "175 Tao Liu, Mingjun Li, Haibin Zheng, Zhaoyan Ming, Jinyin Chen:\n",
      "Evil vs evil: using adversarial examples to against backdoor attack in federated learning. Multim. Syst. 29(2): 553-568 (2023)\n",
      "176 Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng:\n",
      "Multidomain active defense: Detecting multidomain backdoor poisoned samples via ALL-to-ALL decoupling training without clean datasets. Neural Networks 168: 350-362 (2023)\n",
      "177 Fangchao Yu, Lina Wang, Bo Zeng, Kai Zhao, Zhi Pang, Tian Wu:\n",
      "How to backdoor split learning. Neural Networks 168: 326-336 (2023)\n",
      "178 Yudong Li, Shigeng Zhang, Weiping Wang, Hong Song:\n",
      "Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey. IEEE Open J. Comput. Soc. 4: 134-146 (2023)\n",
      "179 Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein:\n",
      "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. IEEE Trans. Pattern Anal. Mach. Intell. 45(2): 1563-1580 (2023)\n",
      "180 Heng Zhang, Jun Gu, Zhikun Zhang, Linkang Du, Yongmin Zhang, Yan Ren, Jian Zhang, Hongran Li:\n",
      "Backdoor attacks against deep reinforcement learning based traffic signal control systems. Peer Peer Netw. Appl. 16(1): 466-474 (2023)\n",
      "181 Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, Jianjun Zhao:\n",
      "TAT: Targeted backdoor attacks against visual object tracking. Pattern Recognit. 142: 109629 (2023)\n",
      "182 Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, Shu-Tao Xia:\n",
      "Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks. Pattern Recognit. 139: 109512 (2023)\n",
      "183 Guang Hua, Andrew Beng Jin Teoh:\n",
      "Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models. Pattern Recognit. 144: 109844 (2023)\n",
      "184 Zhen Wang, Buhong Wang, Chuanlei Zhang, Yaohui Liu, Jianxin Guo:\n",
      "Robust Feature-Guided Generative Adversarial Network for Aerial Image Semantic Segmentation against Backdoor Attacks. Remote. Sens. 15(10): 2580 (2023)\n",
      "185 Tatsuya Oyama, Shunsuke Okura, Kota Yoshida, Takeshi Fujino:\n",
      "Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface. Sensors 23(10): 4742 (2023)\n",
      "186 Jie Yang, Jun Zheng, Haochen Wang, Jiaxing Li, Haipeng Sun, Weifeng Han, Nan Jiang, Yu-An Tan:\n",
      "Edge-Cloud Collaborative Defense against Backdoor Attacks in Federated Learning. Sensors 23(3): 1052 (2023)\n",
      "187 Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu:\n",
      "Did You Train on My Dataset? Towards Public Dataset Protection with CleanLabel Backdoor Watermarking. SIGKDD Explor. 25(1): 43-53 (2023)\n",
      "188 Arthur Mercier, Nikita Smolin, Oliver Sihlovec, Stefanos Koffas, Stjepan Picek:\n",
      "Backdoor Pony: Evaluating backdoor attacks and defenses in different domains. SoftwareX 22: 101387 (2023)\n",
      "189 Qianli Ma, Junping Qin, Kai Yan, Lei Wang, Hao Sun:\n",
      "Stealthy Frequency-Domain Backdoor Attacks: Fourier Decomposition and Fundamental Frequency Injection. IEEE Signal Process. Lett. 30: 1677-1681 (2023)\n",
      "190 Zhe Ye, Diqun Yan, Li Dong, Jiacheng Deng, Shui Yu:\n",
      "Stealthy Backdoor Attack Against Speaker Recognition Using Phase-Injection Hidden Trigger. IEEE Signal Process. Lett. 30: 1057-1061 (2023)\n",
      "191 Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu:\n",
      "$\\tt{PoisonedGNN}$: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems. IEEE Trans. Computers 72(10): 2822-2834 (2023)\n",
      "192 Chaohui Xu, Wenye Liu, Yue Zheng, Si Wang, Chip-Hong Chang:\n",
      "An Imperceptible Data Augmentation Based Blackbox Clean-Label Backdoor Attack on Deep Neural Networks. IEEE Trans. Circuits Syst. I Regul. Pap. 70(12): 5011-5024 (2023)\n",
      "193 Jakub Breier, Xiaolu Hou, Martín Ochoa, Jesus Solano:\n",
      "FooBaR: Fault Fooling Backdoor Attack on Neural Network Training. IEEE Trans. Dependable Secur. Comput. 20(3): 1895-1908 (2023)\n",
      "194 Yanjiao Chen, Zhicong Zheng, Xueluan Gong:\n",
      "MARNet: Backdoor Attacks Against Cooperative Multi-Agent Reinforcement Learning. IEEE Trans. Dependable Secur. Comput. 20(5): 4188-4198 (2023)\n",
      "195 Xueluan Gong, Ziyao Wang, Yanjiao Chen, Meng Xue, Qian Wang, Chao Shen:\n",
      "Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks With RGB Filters. IEEE Trans. Dependable Secur. Comput. 20(6): 4993-5004 (2023)\n",
      "196 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection. IEEE Trans. Dependable Secur. Comput. 20(6): 4752-4762 (2023)\n",
      "197 Kaidi Jin, Tianwei Zhang, Chao Shen, Yufei Chen, Ming Fan, Chenhao Lin, Ting Liu:\n",
      "Can We Mitigate Backdoor Attack Using Adversarial Detection Methods? IEEE Trans. Dependable Secur. Comput. 20(4): 2867-2881 (2023)\n",
      "198 Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li:\n",
      "Enhancing Backdoor Attacks With Multi-Level MMD Regularization. IEEE Trans. Dependable Secur. Comput. 20(2): 1675-1686 (2023)\n",
      "199 Nikolaus Dräger, Yonghao Xu, Pedram Ghamisi:\n",
      "Backdoor Attacks for Remote Sensing Data With Wavelet Transform. IEEE Trans. Geosci. Remote. Sens. 61: 1-15 (2023)\n",
      "200 Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami:\n",
      "Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. IEEE Trans. Inf. Forensics Secur. 18: 4668-4680 (2023)\n",
      "201 Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia:\n",
      "Black-Box Dataset Ownership Verification via Backdoor Watermarking. IEEE Trans. Inf. Forensics Secur. 18: 2318-2332 (2023)\n",
      "202 Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya:\n",
      "SAFELearning: Secure Aggregation in Federated Learning With Backdoor Detectability. IEEE Trans. Inf. Forensics Secur. 18: 3289-3304 (2023)\n",
      "203 Xueluan Gong, Yanjiao Chen, Wenbin Yang, Huayang Huang, Qian Wang:\n",
      "B3: Backdoor Attacks against Black-box Machine Learning Models. ACM Trans. Priv. Secur. 26(4): 43:1-43:24 (2023)\n",
      "204 Jireh Huang, Qing Zhou:\n",
      "Bayesian Causal Bandits with Backdoor Adjustment Prior. Trans. Mach. Learn. Res. 2023 (2023)\n",
      "205 Fuchen Ma, Meng Ren, Lerong Ouyang, Yuanliang Chen, Juan Zhu, Ting Chen, Yingli Zheng, Xiao Dai, Yu Jiang, Jiaguang Sun:\n",
      "Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts. ACM Trans. Softw. Eng. Methodol. 32(3): 61:1-61:24 (2023)\n",
      "206 Yunsong Huang, Weicheng Liu, Hui-Ming Wang:\n",
      "Hidden Backdoor Attack Against Deep Learning-Based Wireless Signal Modulation Classifiers. IEEE Trans. Veh. Technol. 72(9): 12396-12400 (2023)\n",
      "207 Xueluan Gong, Yanjiao Chen, Qian Wang, Weihan Kong:\n",
      "Backdoor Attacks and Defenses in Federated Learning: State-of-the-Art, Taxonomy, and Future Directions. IEEE Wirel. Commun. 30(2): 114-121 (2023)\n",
      "208 Ruitao Hou, Teng Huang, Hongyang Yan, Lishan Ke, Weixuan Tang:\n",
      "A stealthy and robust backdoor attack via frequency domain transform. World Wide Web (WWW) 26(5): 2767-2783 (2023)\n",
      "209 Khoa D. Doan, Yingjie Lao, Peng Yang, Ping Li:\n",
      "Defending Backdoor Attacks on Vision Transformer via Patch Processing. AAAI 2023: 506-515\n",
      "210 Pei Fang, Jinghui Chen:\n",
      "On the Vulnerability of Backdoor Defenses for Federated Learning. AAAI 2023: 11800-11808\n",
      "211 Khondoker Murad Hossain, Tim Oates:\n",
      "Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks. SafeAI@AAAI 2023\n",
      "212 Yiming Li:\n",
      "Poisoning-Based Backdoor Attacks in Computer Vision. AAAI 2023: 16121-16122\n",
      "213 Xiaoting Lyu, Yufei Han, Wei Wang, Jingkai Liu, Bin Wang, Jiqiang Liu, Xiangliang Zhang:\n",
      "Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning. AAAI 2023: 9020-9028\n",
      "214 Soumyadeep Pal, Ren Wang, Yuguang Yao, Sijia Liu:\n",
      "Towards Understanding How Self-training Tolerates Data Backdoor Poisoning. SafeAI@AAAI 2023\n",
      "215 Alexander A. Semenov, Daniil Chivilikhin, Stepan Kochemazov, Ibragim Dzhiblavi:\n",
      "Probabilistic Generalization of Backdoor Trees with Application to SAT. AAAI 2023: 4095-4103\n",
      "216 Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, Tianwei Zhang:\n",
      "Defending against Backdoor Attacks in Natural Language Generation. AAAI 2023: 5257-5265\n",
      "217 Jun Yan, Vansh Gupta, Xiang Ren:\n",
      "BITE: Textual Backdoor Attacks with Iterative Trigger Injection. ACL (1) 2023: 12951-12968\n",
      "218 Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, Weiping Wang:\n",
      "A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning. ACL (1) 2023: 3508-3520\n",
      "219 Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V. G. Vinod Vydiswaran:\n",
      "Defending against Insertion-based Textual Backdoor Attacks via Attribution. ACL (Findings) 2023: 8818-8833\n",
      "220 Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu:\n",
      "Multi-target Backdoor Attacks for Code Pre-trained Models. ACL (1) 2023: 7236-7254\n",
      "221 Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, Weiping Wang:\n",
      "Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models. ACL (Findings) 2023: 3850-3868\n",
      "222 Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma:\n",
      "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models. ACL (1) 2023: 15551-15565\n",
      "223 Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie:\n",
      "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. ACL (1) 2023: 7653-7668\n",
      "224 Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo:\n",
      "Backdooring Neural Code Search. ACL (1) 2023: 9692-9708\n",
      "225 Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun:\n",
      "Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias. ACL (Findings) 2023: 2495-2517\n",
      "226 Tiantian Liu, Feng Lin:\n",
      "Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks. ACM TUR-C 2023: 145-147\n",
      "227 Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad-Reza Sadeghi:\n",
      "FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks. ACSAC 2023: 647-661\n",
      "228 Xiao Yang, Gaolei Li, Chaofeng Zhang, Meng Han, Wu Yang:\n",
      "PerCBA: Persistent Clean-label Backdoor Attacks on Semi-Supervised Graph Node Classification. AISafety/SafeRL@IJCAI 2023\n",
      "229 Haochen Wang, Tianshi Mu, Guocong Feng, ShangBo Wu, Yuanzhang Li:\n",
      "DFaP: Data Filtering and Purification Against Backdoor Attacks. AIS&P (1) 2023: 81-97\n",
      "230 Hui Yang, Ruilin Yang, Heqiu Cai, Xiao Zhang, Qingqi Pei, Shaowei Wang, Hongyang Yan:\n",
      "SSL-ABD : An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning. AIS&P (1) 2023: 456-467\n",
      "231 Yuxi Mi, Yiheng Sun, Jihong Guan, Shuigeng Zhou:\n",
      "Identifying Backdoor Attacks in Federated Learning via Anomaly Detection. APWeb/WAIM (3) 2023: 111-126\n",
      "232 Shang Wang, Yansong Gao, Anmin Fu, Zhi Zhang, Yuqing Zhang, Willy Susilo, Dongxi Liu:\n",
      "CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses. AsiaCCS 2023: 938-950\n",
      "233 Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao:\n",
      "DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. AsiaCCS 2023: 731-745\n",
      "234 Martin Sustek, Sonal Joshi, Henry Li, Thomas Thebaud, Jesús Villalba, Sanjeev Khudanpur, Najim Dehak:\n",
      "Joint Energy-Based Model for Robust Speech Classification System Against Dirty-Label Backdoor Poisoning Attacks. ASRU 2023: 1-8\n",
      "235 Jinke Cheng, Gaolei Li, Xi Lin, Hao Peng, Jianhua Li:\n",
      "Content Style-triggered Backdoor Attack in Non-IID Federated Learning via Generative AI. ISPA/BDCloud/SocialCom/SustainCom 2023: 640-647\n",
      "236 Renjie Ji, Wansen Wang, Yan Xiong, Wenchao Huang:\n",
      "SolScope: Effectively Hunting Potential Permission Backdoor Threats in Smart Contracts. BIGCOM 2023: 88-95\n",
      "237 Omar Abdel Wahab, Anderson Avila:\n",
      "A Max-Min Security Game for Coordinated Backdoor Attacks on Federated Learning. IEEE Big Data 2023: 3566-3573\n",
      "238 Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia:\n",
      "Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning. BMVC 2023: 172-173\n",
      "239 Mario D'Onghia, Federico Di Cesare, Luigi Gallo, Michele Carminati, Mario Polino, Stefano Zanero:\n",
      "Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors. AISec@CCS 2023: 209-220\n",
      "240 Maximilian Noppel, Christian Wressnegger:\n",
      "Poster: Fooling XAI with Explanation-Aware Backdoors. CCS 2023: 3612-3614\n",
      "241 Behrad Tajalli, Gorka Abad, Stjepan Picek:\n",
      "Poster: Backdoor Attack on Extreme Learning Machines. CCS 2023: 3588-3590\n",
      "242 Jing Xu, Stjepan Picek:\n",
      "Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks. CCS 2023: 3570-3572\n",
      "243 Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia:\n",
      "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information. CCS 2023: 771-785\n",
      "244 Wenqing Li, Yue Wang, Muhammad Shafique, Saif Eddin Jabari:\n",
      "Physical Backdoor Trigger Activation of Autonomous Vehicle Using Reachability Analysis. CDC 2023: 821-826\n",
      "245 Zihan Guan, Lichao Sun, Mengnan Du, Ninghao Liu:\n",
      "Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks. CIKM 2023: 608-618\n",
      "246 Renhua Xie, Xuxin Fang, Bo Ma, Chuanhuang Li, Xiaoyong Yuan:\n",
      "TRGE: A Backdoor Detection After Quantization. Inscrypt (2) 2023: 394-398\n",
      "247 Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener:\n",
      "Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks. CISS 2023: 1-6\n",
      "248 Tom Ganz, Inaam Ashraf, Martin Härterich, Konrad Rieck:\n",
      "Detecting Backdoors in Collaboration Graphs of Software Repositories. CODASPY 2023: 189-200\n",
      "249 Manaar Alam, Esha Sarkar, Michail Maniatakos:\n",
      "PerDoor: Persistent Backdoors in Federated Learning using Adversarial Perturbations. COINS 2023: 1-6\n",
      "250 Tim Walita, Alessandro Erba, John Henry Castellanos, Nils Ole Tippenhauer:\n",
      "Blind Concealment from Reconstruction-based Attack Detectors for Industrial Control Systems via Backdoor Attacks. CPSS@AsiaCCS 2023: 36-47\n",
      "251 Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang:\n",
      "Detecting Backdoors in Pre-trained Encoders. CVPR 2023: 16352-16362\n",
      "252 Lu Pang, Tao Sun, Haibin Ling, Chao Chen:\n",
      "Backdoor Cleansing with Unlabeled Data. CVPR 2023: 12218-12227\n",
      "253 Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert D. Mullins, Nicolas Papernot:\n",
      "Architectural Backdoors in Neural Networks. CVPR 2023: 24595-24604\n",
      "254 Simin Chen, Hanlin Chen, Mirazul Haque, Cong Liu, Wei Yang:\n",
      "The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection. CVPR 2023: 24585-24594\n",
      "255 Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho:\n",
      "How to Backdoor Diffusion Models? CVPR 2023: 4015-4024\n",
      "256 Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia:\n",
      "Backdoor Defense via Adaptively Splitting Poisoned Dataset. CVPR 2023: 4005-4014\n",
      "257 Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem:\n",
      "Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs. CVPR Workshops 2023: 2338-2345\n",
      "258 Wenbo Jiang, Hongwei Li, Guowen Xu, Tianwei Zhang:\n",
      "Color Backdoor: A Robust Poisoning Attack in Color Space. CVPR 2023: 8133-8142\n",
      "259 Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao:\n",
      "Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. CVPR 2023: 16363-16372\n",
      "260 Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Qiguang Mia, Rong Jin, Gang Hua:\n",
      "Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks. CVPR 2023: 20495-20503\n",
      "261 Mingjie Sun, Zico Kolter:\n",
      "Single Image Backdoor Inversion via Robust Smoothed Classifiers. CVPR 2023: 8113-8122\n",
      "262 Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan:\n",
      "Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning. CVPR 2023: 12239-12249\n",
      "263 Qiuling Xu, Guanhong Tao, Jean Honorio, Yingqi Liu, Shengwei An, Guangyu Shen, Siyuan Cheng, Xiangyu Zhang:\n",
      "MEDIC: Remove Model Backdoors via Importance Driven Cloning. CVPR 2023: 20485-20494\n",
      "264 Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot:\n",
      "Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger. CVPR 2023: 12250-12259\n",
      "265 Zenghui Yuan, Pan Zhou, Kai Zou, Yu Cheng:\n",
      "You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks? CVPR 2023: 24605-24615\n",
      "266 Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu:\n",
      "Backdoor Defense via Deconfounded Representation Learning. CVPR 2023: 12228-12238\n",
      "267 M. Caner Tol, Saad Islam, Andrew J. Adiletta, Berk Sunar, Ziming Zhang:\n",
      "Don't Knock! Rowhammer at the Backdoor of DNN Models. DSN 2023: 109-122\n",
      "268 Zihan Guan, Mengnan Du, Ninghao Liu:\n",
      "XGBD: Explanation-Guided Graph Backdoor Detection. ECAI 2023: 932-939\n",
      "269 Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen:\n",
      "Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain. ECAI 2023: 2954-2961\n",
      "270 Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn:\n",
      "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. EMNLP 2023: 953-967\n",
      "271 Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen:\n",
      "Attention-Enhancing Backdoor Attacks Against BERT-based Models. EMNLP (Findings) 2023: 10672-10690\n",
      "272 Wencong You, Zayd Hammoudeh, Daniel Lowd:\n",
      "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers. EMNLP (Findings) 2023: 12499-12527\n",
      "273 Shuai Zhao, Jinming Wen, Anh Tuan Luu, Junbo Zhao, Jie Fu:\n",
      "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. EMNLP 2023: 12303-12317\n",
      "274 Kazuki Iwahana, Naoto Yanai, Toru Fujiwara:\n",
      "Backdoor Attacks Leveraging Latent Representation in Competitive Learning. ESORICS Workshops (2) 2023: 700-718\n",
      "275 Jing Xu, Stefanos Koffas, Oguzhan Ersoy, Stjepan Picek:\n",
      "Watermarking Graph Neural Networks based on Backdoor Attacks. EuroS&P 2023: 1179-1197\n",
      "276 Akshay Dhonthi, Ernst Moritz Hahn, Vahid Hashemi:\n",
      "Backdoor Mitigation in Deep Neural Networks via Strategic Retraining. FM 2023: 635-647\n",
      "277 Qiuxian Chen, Yizheng Tao:\n",
      "An Investigation of Recent Backdoor Attacks and Defenses in Federated Learning. FMEC 2023: 262-269\n",
      "278 Mahmoud Nazzal, Nura Aljaafari, Ahmad H. Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Abdallah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed:\n",
      "Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. FMEC 2023: 204-209\n",
      "279 Yu-Wen Chen, Bo-Hsu Ke, Bozhong Chen, Si-Rong Chiu, Chun-Wei Tu, Jian-Jhih Kuo:\n",
      "Knowledge Distillation Based Defense for Audio Trigger Backdoor in Federated Learning. GLOBECOM 2023: 4271-4276\n",
      "280 Hongyi Zhang, Mingqian Liu, Yunfei Chen:\n",
      "Backdoor Attacks on Multi-Agent Reinforcement Learning-based Spectrum Management. GLOBECOM 2023: 3361-3365\n",
      "281 Tianya Zhao, Xuyu Wang, Shiwen Mao:\n",
      "Backdoor Attacks Against Deep Learning-Based Massive MIMO Localization. GLOBECOM 2023: 2796-2801\n",
      "282 Bei Chen, Gaolei Li, Mingzhe Chen, Yuchen Liu, Xiaoyu Yi, Jianhua Li:\n",
      "PBE-Plan: Periodic Backdoor Erasing Plan for Trustworthy Federated Learning. HPCC/DSS/SmartCity/DependSys 2023: 41-48\n",
      "283 Xiao Yang, Gaolei Li, Xiaoyi Tao, Chaofeng Zhang, Jianhua Li:\n",
      "Black-Box Graph Backdoor Defense. ICA3PP (5) 2023: 163-180\n",
      "284 Shintaro Narisada, Seira Hidano, Kazuhide Fukushima:\n",
      "Fully Hidden Dynamic Trigger Backdoor Attacks. ICAART (3) 2023: 81-91\n",
      "285 Cheng Chu, Lei Jiang, Martin Swany, Fan Chen:\n",
      "QTROJAN: A Circuit Backdoor Against Quantum Neural Networks. ICASSP 2023: 1-5\n",
      "286 Mingrui He, Tianyu Chen, Haoyi Zhou, Shanghang Zhang, Jianxin Li:\n",
      "BadRes: Reveal the Backdoors Through Residual Connection. ICASSP 2023: 1-5\n",
      "287 Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti:\n",
      "Going in Style: Audio Backdoors Through Stylistic Transformations. ICASSP 2023: 1-5\n",
      "288 Fang-Qi Li, Shi-Lin Wang, Yun Zhu:\n",
      "Measure and Countermeasure of the Capsulation Attack Against Backdoor-Based Deep Neural Network Watermarks. ICASSP 2023: 1-5\n",
      "289 Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "Untargeted Backdoor Attack Against Object Detection. ICASSP 2023: 1-5\n",
      "290 Dan Meng, Xue Wang, Jun Wang:\n",
      "Backdoor Attack Against Automatic Speaker Verification Models in Federated Learning. ICASSP 2023: 1-5\n",
      "291 Hang Wang, Sahar Karami, Ousmane Dia, Hippolyt Ritter, Ehsan Emamjomeh-Zadeh, Jiahui Chen, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Training Set Cleansing of Backdoor Poisoning by Self-Supervised Representation Learning. ICASSP 2023: 1-5\n",
      "292 Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "BATT: Backdoor Attack with Transformation-Based Triggers. ICASSP 2023: 1-5\n",
      "293 Sheng Yang, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "Backdoor Defense via Suppressing Model Shortcuts. ICASSP 2023: 1-5\n",
      "294 Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, Zhonghai Wu:\n",
      "NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning. ICASSP 2023: 1-5\n",
      "295 Shuli Zhuang, Pengfei Xia, Bin Li:\n",
      "An Empirical Study of Backdoor Attacks on Masked Auto Encoders. ICASSP 2023: 1-5\n",
      "296 Yu-Wen Chen, Bo-Hsu Ke, Bozhong Chen, Si-Rong Chiu, Chun-Wei Tu, Jian-Jhih Kuo:\n",
      "Successive Interference Cancellation Based Defense for Trigger Backdoor in Federated Learning. ICC 2023: 26-32\n",
      "297 Weiqi Wang, Chenhan Zhang, Shushu Liu, Mingjian Tang, An Liu, Shui Yu:\n",
      "FedMC: Federated Learning with Mode Connectivity Against Distributed Backdoor Attacks. ICC 2023: 4873-4878\n",
      "298 Han Yang, Dongbing Gu, Jianhua He:\n",
      "Towards Defending Adaptive Backdoor Attacks in Federated Learning. ICC 2023: 5078-5084\n",
      "299 Zixin Li, Hang Jiang, Sicheng Zhang, Wei Xiang, Yun Lin:\n",
      "Random Location Poisoning Backdoor Attack Against Automatic Modulation Classification in Wireless Networks. ICCC 2023: 1-6\n",
      "300 Tianming Zhao, Zijie Tang, Tianfang Zhang, Huy Phan, Yan Wang, Cong Shi, Bo Yuan, Yingying Chen:\n",
      "Stealthy Backdoor Attack on RF Signal Classification. ICCCN 2023: 1-10\n",
      "301 Koko Nishiura, Tomotaka Kimura, Jun Cheng:\n",
      "Countermeasure against Backdoor Attack for Deep Learning-Based Phishing Detection. ICCE-Taiwan 2023: 651-652\n",
      "302 Iram Arshad, Yuansong Qiao, Brian Lee, Yuhang Ye:\n",
      "Invisible Encoded Backdoor attack on DNNs using Conditional GAN. ICCE 2023: 1-5\n",
      "303 Junfeng Guo, Ang Li, Lixu Wang, Cong Liu:\n",
      "PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning. ICCV 2023: 4676-4685\n",
      "304 Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, Ying Gao:\n",
      "Multi-metrics adaptively identifies backdoors in Federated learning. ICCV 2023: 4629-4639\n",
      "305 Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji, Yuan Yao, Ting Wang:\n",
      "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. ICCV 2023: 4344-4355\n",
      "306 Min Liu, Alberto L. Sangiovanni-Vincentelli, Xiangyu Yue:\n",
      "Beating Backdoor Attack at Its Own Game. ICCV 2023: 4597-4606\n",
      "307 Virat Shejwalkar, Lingjuan Lyu, Amir Houmansadr:\n",
      "The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning. ICCV 2023: 4707-4717\n",
      "308 Lukas Struppek, Dominik Hintersdorf, Kristian Kersting:\n",
      "Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. ICCV 2023: 4561-4573\n",
      "309 Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha:\n",
      "TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. ICCV 2023: 165-175\n",
      "310 Yutong Wu, Xingshuo Han, Han Qiu, Tianwei Zhang:\n",
      "Computation and Data Efficient Backdoor Attacks. ICCV 2023: 4782-4791\n",
      "311 Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu:\n",
      "Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization. ICCV 2023: 4443-4454\n",
      "312 Rui Ning, Jiang Li, Chunsheng Xin, Chonggang Wang, Xu Li, Robert Gazda, Jin-Hee Cho, Hongyi Wu:\n",
      "ScanFed: Scalable Behavior-Based Backdoor Detection in Federated Learning. ICDCS 2023: 782-793\n",
      "313 Zeshan Pang, Yuyuan Sun, Shasha Guo, Yuliang Lu:\n",
      "Backdoor Learning on Siamese Networks Using Physical Triggers: FaceNet as a Case Study. ICDF2C (1) 2023: 279-292\n",
      "314 Xiao Yang, Gaolei Li, Meng Han:\n",
      "Persistent Clean-Label Backdoor on Graph-Based Semi-supervised Cybercrime Detection. ICDF2C (1) 2023: 264-278\n",
      "315 Xubo Yang, Linsen Li, Cunqing Hua, Changhao Yao:\n",
      "CCBA: Code Poisoning-Based Clean-Label Covert Backdoor Attack Against DNNs. ICDF2C (1) 2023: 179-192\n",
      "316 Peng Chen, Jirui Yang, Junxiong Lin, Zhihui Lu, Qiang Duan, Hongfeng Chai:\n",
      "A Practical Clean-Label Backdoor Attack with Limited Information in Vertical Federated Learning. ICDM 2023: 41-50\n",
      "317 Honghui Xu, Zhipeng Cai, Zuobin Xiong, Wei Li:\n",
      "Backdoor Attack on 3D Grey Image Segmentation. ICDM 2023: 708-717\n",
      "318 Muhammad Imran, Hassaan Khaliq Qureshi, Irene Amerini:\n",
      "BHAC-MRI: Backdoor and Hybrid Attacks on MRI Brain Tumor Classification Using CNN. ICIAP (2) 2023: 332-344\n",
      "319 Xubo Yang, Linsen Li, Yenan Chen:\n",
      "Neural Network Backdoor Attacks Fully Controlled by Composite Natural Utterance Fragments. ICICS 2023: 451-466\n",
      "320 Bin Huang, Zhi Wang:\n",
      "Efficient any-Target Backdoor Attack with Pseudo Poisoned Samples. ICIP 2023: 3319-3323\n",
      "321 Zihan Shen, Wei Hou, Yun Li:\n",
      "CSSBA: A Clean Label Sample-Specific Backdoor Attack. ICIP 2023: 965-969\n",
      "322 Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang:\n",
      "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning. ICLR 2023\n",
      "323 Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, Tianwei Zhang:\n",
      "Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only. ICLR 2023\n",
      "324 Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu:\n",
      "SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency. ICLR 2023\n",
      "325 Jonathan Hayase, Sewoong Oh:\n",
      "Few-shot Backdoor Attacks via Neural Tangent Kernels. ICLR 2023\n",
      "326 Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey:\n",
      "Distilling Cognitive Backdoor Patterns within an Image. ICLR 2023\n",
      "327 Charles Jin, Melinda Sun, Martin C. Rinard:\n",
      "Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks. ICLR 2023\n",
      "328 Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang:\n",
      "The Dark Side of AutoML: Towards Architectural Backdoor Search. ICLR 2023\n",
      "329 Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal:\n",
      "Revisiting the Assumption of Latent Separability for Backdoor Defenses. ICLR 2023\n",
      "330 Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma:\n",
      "UNICORN: A Unified Backdoor Trigger Inversion Framework. ICLR 2023\n",
      "331 Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin:\n",
      "Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data. ICME 2023: 348-353\n",
      "332 Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, Yunfei Yang:\n",
      "DBIA: Data-Free Backdoor Attack Against Transformer Networks. ICME 2023: 2819-2824\n",
      "333 Yuwei Zeng, Jingxuan Tan, Zhengxin You, Zhenxing Qian, Xinpeng Zhang:\n",
      "Watermarks for Generative Adversarial Network Based on Steganographic Invisible Backdoor. ICME 2023: 1211-1216\n",
      "334 Yanbo Dai, Songze Li:\n",
      "Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. ICML 2023: 6712-6725\n",
      "335 Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry:\n",
      "Rethinking Backdoor Attacks. ICML 2023: 16216-16236\n",
      "336 Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang:\n",
      "Reconstructive Neuron Pruning for Backdoor Defense. ICML 2023: 19837-19854\n",
      "337 Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding:\n",
      "Understanding Backdoor Attacks through the Adaptability Hypothesis. ICML 2023: 37952-37976\n",
      "338 Zhen Xiang, Zidi Xiong, Bo Li:\n",
      "UMD: Unsupervised Model Detection for X2X Backdoor Attacks. ICML 2023: 38013-38038\n",
      "339 Hangfan Zhang, Jinghui Chen, Lu Lin, Jinyuan Jia, Dinghao Wu:\n",
      "Graph Contrastive Backdoor Attacks. ICML 2023: 40888-40910\n",
      "340 Sheng Ran, Baolin Zheng, Mingwei Sun:\n",
      "SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach. ICONIP (12) 2023: 285-297\n",
      "341 Shufan Yang, Qianmu Li, Zhichao Lian, Pengchuan Wang, Jun Hou:\n",
      "MIC: An Effective Defense Against Word-Level Textual Backdoor Attacks. ICONIP (6) 2023: 3-18\n",
      "342 Yingrui Tong, Jun Feng, Gaolei Li, Xi Lin, Chengcheng Zhao, Xiaoyu Yi, Jianhua Li:\n",
      "SemSBA: Semantic-perturbed Stealthy Backdoor Attack on Federated Semi-supervised Learning. ICPADS 2023: 1569-1576\n",
      "343 Di Xiao, Zhuyang Yu, Lvjun Chen:\n",
      "RPFL: Robust and Privacy Federated Learning against Backdoor and Sample Inference Attacks. ICPADS 2023: 1508-1515\n",
      "344 Love Allen Chijioke Ahakonye, Cosmas Ifeanyi Nwakanma, Jae Min Lee, Dong-Seong Kim:\n",
      "X-HDNN: Explainable Hybrid DNN for Industrial Internet of Things Backdoor Attack Detection. ICTC 2023: 1053-1057\n",
      "345 Huayang Huang, Qian Wang, Xueluan Gong, Tao Wang:\n",
      "Orion: Online Backdoor Sample Detection via Evolution Deviance. IJCAI 2023: 864-874\n",
      "346 Haochen Mei, Gaolei Li, Jun Wu, Longfei Zheng:\n",
      "Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. IJCNN 2023: 1-10\n",
      "347 Xiaoxing Mo, Leo Yu Zhang, Nan Sun, Wei Luo, Shang Gao:\n",
      "Backdoor Attack on Deep Neural Networks in Perception Domain. IJCNN 2023: 1-8\n",
      "348 Thuy Dung Nguyen, Anh Duy Nguyen, Thanh-Hung Nguyen, Kok-Seng Wong, Huy Hieu Pham, Truong Thao Nguyen, Phi Le Nguyen:\n",
      "FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection. IJCNN 2023: 1-10\n",
      "349 Jing Xu, Gorka Abad, Stjepan Picek:\n",
      "Rethinking the Trigger-injecting Position in Graph Backdoor Attack. IJCNN 2023: 1-8\n",
      "350 Donik Vrsnak, Ivan Sabolic, Marko Subasic, Sven Loncaric:\n",
      "Computational Color Constancy-Based Backdoor Attacks. ISPA 2023: 1-6\n",
      "351 Shuhua Deng, Xian Qing, Xiaofan Li, Xing Gao, Xieping Gao:\n",
      "SDN Application Backdoor: Disrupting the Service via Poisoning the Topology. INFOCOM 2023: 1-10\n",
      "352 Tian Dong, Ziyuan Zhang, Han Qiu, Tianwei Zhang, Hewu Li, Terry Wang:\n",
      "Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing. INFOCOM 2023: 1-10\n",
      "353 Priyesh Ranjan, Ashish Gupta, Federico Coro, Sajal K. Das:\n",
      "Robust Federated Learning against Backdoor Attackers. INFOCOM Workshops 2023: 1-6\n",
      "354 Yue Wang, Chao Yang, Ning Xi, Yulong Shen, Jianfeng Ma:\n",
      "SEBD: Sensor Emulation Based Backdoor for Autopilot. IoT 2023: 265-269\n",
      "355 Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng:\n",
      "Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks. KDD 2023: 4743-4755\n",
      "356 Matthew Yudin, Rauf Izmailov:\n",
      "DUBIOUS: Detecting Unknown Backdoored Input by Observing Unusual Signatures. MILCOM 2023: 696-702\n",
      "357 Lulu Dai, Mingyue Han:\n",
      "Robust Sentiment Classification Based on the Backdoor Adjustment. MLNLP 2023: 41-47\n",
      "358 Yusheng Guo, Nan Zhong, Zhenxing Qian, Xinpeng Zhang:\n",
      "Physical Invisible Backdoor Based on Camera Imaging. ACM Multimedia 2023: 7817-7825\n",
      "359 Shengshan Hu, Wei Liu, Minghui Li, Yechao Zhang, Xiaogeng Liu, Xianlong Wang, Leo Yu Zhang, Junhui Hou:\n",
      "PointCRT: Detecting Backdoor in 3D Point Cloud via Corruption Robustness. ACM Multimedia 2023: 666-675\n",
      "360 Yulin Jin, Xiaoyu Zhang, Jian Lou, Xiaofeng Chen:\n",
      "ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing. ACM Multimedia 2023: 5410-5418\n",
      "361 Hui Wei, Hanxun Yu, Kewei Zhang, Zhixiang Wang, Jianke Zhu, Zheng Wang:\n",
      "Moiré Backdoor Attack (MBA): A Novel Trigger for Pedestrian Detectors in the Physical World. ACM Multimedia 2023: 8828-8838\n",
      "362 Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu:\n",
      "PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. ACM Multimedia 2023: 9134-9142\n",
      "363 Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen:\n",
      "Model-Contrastive Learning for Backdoor Elimination. ACM Multimedia 2023: 8869-8880\n",
      "364 Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su:\n",
      "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning. ACM Multimedia 2023: 1577-1587\n",
      "365 Zhicong Zheng, Xinfeng Li, Chen Yan, Xiaoyu Ji, Wenyuan Xu:\n",
      "The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems. ACM Multimedia 2023: 7849-7858\n",
      "366 Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan:\n",
      "MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems. MobiCom 2023: 48:1-48:15\n",
      "367 Siyuan Cheng, Guanhong Tao, Yingqi Liu, Shengwei An, Xiangzhe Xu, Shiwei Feng, Guangyu Shen, Kaiyuan Zhang, Qiuling Xu, Shiqing Ma, Xiangyu Zhang:\n",
      "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense. NDSS 2023\n",
      "368 Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang:\n",
      "Backdoor Attacks Against Dataset Distillation. NDSS 2023\n",
      "369 Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang:\n",
      "The \"Beatrix\" Resurrections: Robust Backdoor Detection via Gram Matrices. NDSS 2023\n",
      "370 Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun:\n",
      "Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense. NeurIPS 2023\n",
      "371 Xuan Chen, Wenbo Guo, Guanhong Tao, Xiangyu Zhang, Dawn Song:\n",
      "BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning. NeurIPS 2023\n",
      "372 Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho:\n",
      "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. NeurIPS 2023\n",
      "373 Tiansheng Huang, Sihao Hu, Ka Ho Chow, Fatih Ilhan, Selim F. Tekin, Ling Liu:\n",
      "Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training. NeurIPS 2023\n",
      "374 Bin Huang, Jiaqian Yu, Yiwei Chen, Siyang Pan, Qiang Wang, Zhi Wang:\n",
      "BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking. NeurIPS 2023\n",
      "375 Jinyuan Jia, Zhuowen Yuan, Dinuka Sahabandu, Luyao Niu, Arezoo Rajabi, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran:\n",
      "FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning. NeurIPS 2023\n",
      "376 Rui Min, Zeyu Qin, Li Shen, Minhao Cheng:\n",
      "Towards Stable Backdoor Purification through Feature Shift Tuning. NeurIPS 2023\n",
      "377 Thuy Dung Nguyen, Tuan Nguyen, Anh Tran, Khoa D. Doan, Kok-Seng Wong:\n",
      "IBA: Towards Irreversible Backdoor Attacks in Federated Learning. NeurIPS 2023\n",
      "378 Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, Ninghao Liu:\n",
      "Black-box Backdoor Defense via Zero-shot Image Purification. NeurIPS 2023\n",
      "379 Ruixiang (Ryan) Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu:\n",
      "Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots. NeurIPS 2023\n",
      "380 Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu:\n",
      "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. NeurIPS 2023\n",
      "381 Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang:\n",
      "Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. NeurIPS 2023\n",
      "382 Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding:\n",
      "A Unified Detection Framework for Inference-Stage Backdoor Defenses. NeurIPS 2023\n",
      "383 Zhen Xiang, Zidi Xiong, Bo Li:\n",
      "CBD: A Certified Backdoor Detector Based on Local Dominant Probability. NeurIPS 2023\n",
      "384 Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman:\n",
      "Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks. NeurIPS 2023\n",
      "385 Hangfan Zhang, Jinyuan Jia, Jinghui Chen, Lu Lin, Dinghao Wu:\n",
      "A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning. NeurIPS 2023\n",
      "386 Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu:\n",
      "Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features. NeurIPS 2023\n",
      "387 Zhicheng Li, Piji Li, Xuan Sheng, Changchun Yin, Lu Zhou:\n",
      "IMTM: Invisible Multi-trigger Multimodal Backdoor Attack. NLPCC (2) 2023: 533-545\n",
      "388 Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, Piji Li:\n",
      "Punctuation Matters! Stealthy Backdoor Attack for Language Models. NLPCC (1) 2023: 524-536\n",
      "389 Najeeb Moharram Jebreel, Josep Domingo-Ferrer, Yiming Li:\n",
      "Defending Against Backdoor Attacks by Layer-wise Feature Analysis. PAKDD (2) 2023: 428-440\n",
      "390 Habib Ullah Manzoor, Ahsan Raza Khan, Tahir Sher, Wasim Ahmad, Ahmed Zoha:\n",
      "Defending Federated Learning from Backdoor Attacks: Anomaly-Aware FedAVG with Layer-Based Aggregation. PIMRC 2023: 1-6\n",
      "391 Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Bisheng Tang, Ye Dong:\n",
      "Practical and General Backdoor Attacks Against Vertical Federated Learning. ECML/PKDD (2) 2023: 402-417\n",
      "392 Cheng Chu, Fan Chen, Philip Richerme, Lei Jiang:\n",
      "QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks. QCE 2023: 1098-1106\n",
      "393 Ashim Gupta, Amrith Krishna:\n",
      "Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. RepL4NLP@ACL 2023: 1-12\n",
      "394 Waris Gill, Ali Anwar, Muhammad Ali Gulzar:\n",
      "FedDefender: Backdoor Attack Defense in Federated Learning. SE4SafeML@SIGSOFT FSE 2023: 6-9\n",
      "395 Mayank Kumar, Radha Agrawal, Priyanka Singh:\n",
      "BATFL: Battling Backdoor Attacks in Federated Learning. SIN 2023: 1-6\n",
      "396 Xueluan Gong, Yanjiao Chen, Wang Yang, Qian Wang, Yuzhe Gu, Huayang Huang, Chao Shen:\n",
      "Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation. SP 2023: 755-772\n",
      "397 Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza Jadliwala, Ahmad-Reza Sadeghi:\n",
      "BayBFed: Bayesian Backdoor Defense for Federated Learning. SP 2023: 737-754\n",
      "398 Haoyang Li, Qingqing Ye, Haibo Hu, Jin Li, Leixia Wang, Chengfang Fang, Jie Shi:\n",
      "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning. SP 2023: 1893-1907\n",
      "399 Tiantian Liu, Feng Lin, Zhangsen Wang, Chao Wang, Zhongjie Ba, Li Lu, Wenyao Xu, Kui Ren:\n",
      "MagBackdoor: Beware of Your Loudspeaker as A Backdoor For Magnetic Injection Attacks. SP 2023: 3416-3431\n",
      "400 Maximilian Noppel, Lukas Peter, Christian Wressnegger:\n",
      "Disguising Attacks with Explanation-Aware Backdoors. SP 2023: 664-681\n",
      "401 Behrad Tajalli, Oguzhan Ersoy, Stjepan Picek:\n",
      "On Feasibility of Server-side Backdoor Attacks on Split Learning. SP (Workshops) 2023: 84-93\n",
      "402 Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, Bo Li:\n",
      "RAB: Provable Robustness Against Backdoor Attacks. SP 2023: 1311-1328\n",
      "403 Limin Yang, Zhi Chen, Jacopo Cortellazzi, Feargus Pendlebury, Kevin Tu, Fabio Pierazzi, Lorenzo Cavallaro, Gang Wang:\n",
      "Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers. SP 2023: 719-736\n",
      "404 Hong Zhu, Shengzhi Zhang, Kai Chen:\n",
      "AI-Guardian: Defeating Adversarial Attacks using Backdoors. SP 2023: 701-718\n",
      "405 Rohit Raj, Biplab Roy, Abir Das, Mainack Mondal:\n",
      "\"We Must Protect the Transformers\": Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models. SPACE 2023: 242-260\n",
      "406 Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Surya Nepal, Derek Abbott:\n",
      "TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World. SRDS 2023: 82-92\n",
      "407 Xiangkai Yang, Wenjian Luo, Qi Zhou, Zhijian Chen:\n",
      "Training Data Leakage via Imperceptible Backdoor Attack. SSCI 2023: 1553-1559\n",
      "408 Marshall Ball, Yevgeniy Dodis, Eli Goldin:\n",
      "Immunizing Backdoored PRGs. TCC (3) 2023: 153-182\n",
      "409 Fabiola Espinoza Castellon, Deepika Singh, Aurelien Mayoue, Cedric Gouy-Pailler:\n",
      "FUBA: Federated Uncovering of Backdoor Attacks for Heterogeneous Data. TPS-ISA 2023: 55-63\n",
      "410 Masayoshi Tsutsui, Tatsuya Kaneko, Shinya Takamaeda-Yamazaki:\n",
      "Poison Egg: Scrambling Federated Learning with Delayed Backdoor Attack. UbiSec 2023: 191-204\n",
      "411 Zhuo Zhang, Guanhong Tao, Guangyu Shen, Shengwei An, Qiuling Xu, Yingqi Liu, Yapeng Ye, Yaoxuan Wu, Xiangyu Zhang:\n",
      "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis. USENIX Security Symposium 2023: 2365-2382\n",
      "412 Yijie Bai, Yanjiao Chen, Hanlei Zhang, Wenyuan Xu, Haiqin Weng, Dou Goodman:\n",
      "VILLAIN: Backdoor Attacks Against Vertical Split Learning. USENIX Security Symposium 2023: 2743-2760\n",
      "413 Peizhuo Lv, Chang Yue, Ruigang Liang, Yunfei Yang, Shengzhi Zhang, Hualong Ma, Kai Chen:\n",
      "A Data-free Backdoor Injection Approach in Neural Networks. USENIX Security Symposium 2023: 2671-2688\n",
      "414 Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia:\n",
      "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. USENIX Security Symposium 2023: 2725-2742\n",
      "415 Xiangyu Qi, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal:\n",
      "Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. USENIX Security Symposium 2023: 1685-1702\n",
      "416 Jianwen Tian, Kefan Qiu, Debin Gao, Zhi Wang, Xiaohui Kuang, Gang Zhao:\n",
      "Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks. USENIX Security Symposium 2023: 2689-2706\n",
      "417 Cheng'an Wei, Yeonjoon Lee, Kai Chen, Guozhu Meng, Peizhuo Lv:\n",
      "Aliasing Backdoor Attacks on Pre-trained Models. USENIX Security Symposium 2023: 2707-2724\n",
      "418 Esha Sarkar, Constantine Doumanidis, Michail Maniatakos:\n",
      "TRAPDOOR: Repurposing neural network backdoors to detect dataset bias in machine learning-based genomic analysis. VLSI-SoC 2023: 1-6\n",
      "419 Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang:\n",
      "Unnoticeable Backdoor Attacks on Graph Neural Networks. WWW 2023: 2263-2273\n",
      "420 Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen:\n",
      "Training-free Lexical Backdoor Attacks on Language Models. WWW 2023: 2198-2208\n",
      "421 Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammad Alkhrashi, Fahad Albalawi, Bernard Ghanem:\n",
      "Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition. CoRR abs/2301.00986 (2023)\n",
      "422 Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang:\n",
      "Backdoor Attacks Against Dataset Distillation. CoRR abs/2301.01197 (2023)\n",
      "423 Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo:\n",
      "Stealthy Backdoor Attack for Code Models. CoRR abs/2301.02496 (2023)\n",
      "424 Tzvi Lederer, Gallil Maimon, Lior Rokach:\n",
      "Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack. CoRR abs/2301.02615 (2023)\n",
      "425 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. CoRR abs/2301.04554 (2023)\n",
      "426 Siyuan Cheng, Guanhong Tao, Yingqi Liu, Shengwei An, Xiangzhe Xu, Shiwei Feng, Guangyu Shen, Kaiyuan Zhang, Qiuling Xu, Shiqing Ma, Xiangyu Zhang:\n",
      "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense. CoRR abs/2301.06241 (2023)\n",
      "427 Pei Fang, Jinghui Chen:\n",
      "On the Vulnerability of Backdoor Defenses for Federated Learning. CoRR abs/2301.08170 (2023)\n",
      "428 Soumyadeep Pal, Ren Wang, Yuguang Yao, Sijia Liu:\n",
      "Towards Understanding How Self-training Tolerates Data Backdoor Poisoning. CoRR abs/2301.08751 (2023)\n",
      "429 Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza Jadliwala, Ahmad-Reza Sadeghi:\n",
      "BayBFed: Bayesian Backdoor Defense for Federated Learning. CoRR abs/2301.09508 (2023)\n",
      "430 Gökberk Yar, Cristina Nita-Rotaru, Alina Oprea:\n",
      "Backdoor Attacks in Peer-to-Peer Federated Learning. CoRR abs/2301.09732 (2023)\n",
      "431 Jiali Wei, Ming Fan, Wenjing Jiao, Wuxia Jin, Ting Liu:\n",
      "BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing. CoRR abs/2301.10412 (2023)\n",
      "432 Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey:\n",
      "Distilling Cognitive Backdoor Patterns within an Image. CoRR abs/2301.10908 (2023)\n",
      "433 Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni:\n",
      "PECAN: A Deterministic Certified Defense Against Backdoor Attacks. CoRR abs/2301.11824 (2023)\n",
      "434 Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, XiaoFeng Wang, Haixu Tang:\n",
      "Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering. CoRR abs/2301.12318 (2023)\n",
      "435 Brandon B. May, N. Joseph Tatro, Piyush Kumar, Nathan Shnidman:\n",
      "Salient Conditional Diffusion for Defending Against Backdoor Attacks. CoRR abs/2301.13862 (2023)\n",
      "436 Xiaoyun Xu, Oguzhan Ersoy, Stjepan Picek:\n",
      "Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. CoRR abs/2302.00747 (2023)\n",
      "437 Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng:\n",
      "Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks. CoRR abs/2302.01677 (2023)\n",
      "438 Gorka Abad, Jing Xu, Stefanos Koffas, Behrad Tajalli, Stjepan Picek:\n",
      "A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification. CoRR abs/2302.01740 (2023)\n",
      "439 Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia:\n",
      "BackdoorBox: A Python Toolbox for Backdoor Learning. CoRR abs/2302.01762 (2023)\n",
      "440 Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu:\n",
      "SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency. CoRR abs/2302.03251 (2023)\n",
      "441 Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen:\n",
      "Training-free Lexical Backdoor Attacks on Language Models. CoRR abs/2302.04116 (2023)\n",
      "442 Jiliang Zhang, Jing Xu, Zhi Zhang, Yansong Gao:\n",
      "Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder. CoRR abs/2302.04457 (2023)\n",
      "443 Eugene Bagdasaryan, Vitaly Shmatikov:\n",
      "Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness. CoRR abs/2302.04977 (2023)\n",
      "444 Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta:\n",
      "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. CoRR abs/2302.06279 (2023)\n",
      "445 Marwan Omar:\n",
      "Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions. CoRR abs/2302.06801 (2023)\n",
      "446 Cheng Chu, Lei Jiang, Martin Swany, Fan Chen:\n",
      "QTrojan: A Circuit Backdoor Against Quantum Neural Networks. CoRR abs/2302.08090 (2023)\n",
      "447 Zenghui Yuan, Yixin Liu, Kai Zhang, Pan Zhou, Lichao Sun:\n",
      "Backdoor Attacks to Pre-trained Unified Foundation Models. CoRR abs/2302.09360 (2023)\n",
      "448 Marwan Omar:\n",
      "RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks. CoRR abs/2302.09420 (2023)\n",
      "449 Baoyuan Wu, Li Liu, Zihao Zhu, Qingshan Liu, Zhaofeng He, Siwei Lyu:\n",
      "Adversarial Machine Learning: A Systematic Survey of Backdoor Attack, Weight Attack and Adversarial Example. CoRR abs/2302.09457 (2023)\n",
      "450 Behrad Tajalli, Oguzhan Ersoy, Stjepan Picek:\n",
      "On Feasibility of Server-side Backdoor Attacks on Split Learning. CoRR abs/2302.09578 (2023)\n",
      "451 Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia:\n",
      "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. CoRR abs/2302.11408 (2023)\n",
      "452 Max Lamparth, Anka Reuel:\n",
      "Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. CoRR abs/2302.12461 (2023)\n",
      "453 Najeeb Moharram Jebreel, Josep Domingo-Ferrer, Yiming Li:\n",
      "Defending Against Backdoor Attacks by Layer-wise Feature Analysis. CoRR abs/2302.12758 (2023)\n",
      "454 Huasong Zhou, Zhenyu Wang, Xiaowei Xu:\n",
      "SATBA: An Invisible Backdoor Attack Based On Spatial Attention. CoRR abs/2302.13056 (2023)\n",
      "455 Jiazhu Dai, Zhipeng Xiong:\n",
      "A semantic backdoor attack against Graph Convolutional Networks. CoRR abs/2302.14353 (2023)\n",
      "456 Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot:\n",
      "Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger. CoRR abs/2302.14677 (2023)\n",
      "457 Mingjie Sun, Zico Kolter:\n",
      "Single Image Backdoor Inversion via Robust Smoothed Classifiers. CoRR abs/2303.00215 (2023)\n",
      "458 Yihang Lin, Pengyuan Zhou, Zhiqian Wu, Yong Liao:\n",
      "Mitigating Backdoors in Federated Learning with FLD. CoRR abs/2303.00302 (2023)\n",
      "459 Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang:\n",
      "Unnoticeable Backdoor Attacks on Graph Neural Networks. CoRR abs/2303.01263 (2023)\n",
      "460 Shangxi Wu, Qiuyang He, Fangzhao Wu, Jitao Sang, Yaowei Wang, Changsheng Xu:\n",
      "Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias. CoRR abs/2303.01504 (2023)\n",
      "461 Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, Zhonghai Wu:\n",
      "NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning. CoRR abs/2303.01742 (2023)\n",
      "462 Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa Doan, Kok-Seng Wong:\n",
      "Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions. CoRR abs/2303.02213 (2023)\n",
      "463 Henger Li, Chen Wu, Sencun Zhu, Zizhan Zheng:\n",
      "Learning to Backdoor Federated Learning. CoRR abs/2303.03320 (2023)\n",
      "464 Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, Ying Gao:\n",
      "Multi-metrics adaptively identifies backdoors in Federated learning. CoRR abs/2303.06601 (2023)\n",
      "465 Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu:\n",
      "Backdoor Defense via Deconfounded Representation Learning. CoRR abs/2303.06818 (2023)\n",
      "466 Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu:\n",
      "Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. CoRR abs/2303.11470 (2023)\n",
      "467 Haoheng Lan, Jindong Gu, Philip H. S. Torr, Hengshuang Zhao:\n",
      "Influencer Backdoor Attack on Semantic Segmentation. CoRR abs/2303.12054 (2023)\n",
      "468 Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Ninghao Liu:\n",
      "Black-box Backdoor Defense via Zero-shot Image Purification. CoRR abs/2303.12175 (2023)\n",
      "469 Yumeki Goto, Nami Ashizawa, Toshiki Shibahara, Naoto Yanai:\n",
      "Do Backdoors Assist Membership Inference Attacks? CoRR abs/2303.12589 (2023)\n",
      "470 Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia:\n",
      "Backdoor Defense via Adaptively Splitting Poisoned Dataset. CoRR abs/2303.12993 (2023)\n",
      "471 Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem:\n",
      "Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs. CoRR abs/2303.13211 (2023)\n",
      "472 Wenqing Li, Yue Wang, Muhammad Shafique, Saif Eddin Jabari:\n",
      "Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis. CoRR abs/2303.13992 (2023)\n",
      "473 Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu:\n",
      "PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems. CoRR abs/2303.14009 (2023)\n",
      "474 Yue Wang, Wending Li, Michail Maniatakos, Saif Eddin Jabari:\n",
      "Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. CoRR abs/2303.14197 (2023)\n",
      "475 Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He:\n",
      "Backdoor Attacks with Input-unique Triggers in NLP. CoRR abs/2303.14325 (2023)\n",
      "476 Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang:\n",
      "Detecting Backdoors in Pre-trained Encoders. CoRR abs/2303.15180 (2023)\n",
      "477 Tao Sun, Lu Pang, Chao Chen, Haibin Ling:\n",
      "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. CoRR abs/2303.15564 (2023)\n",
      "478 Haodong Zhao, Wei Du, Junjie Guo, Gongshen Liu:\n",
      "A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network. CoRR abs/2303.16031 (2023)\n",
      "479 Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao:\n",
      "Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. CoRR abs/2303.18191 (2023)\n",
      "480 Hao Chen, Chen Gong, Yizhe Wang, Xinwen Hou:\n",
      "Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. CoRR abs/2304.00252 (2023)\n",
      "481 Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan:\n",
      "Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning. CoRR abs/2304.01482 (2023)\n",
      "482 Jing Xu, Gorka Abad, Stjepan Picek:\n",
      "Rethinking the Trigger-injecting Position in Graph Backdoor Attack. CoRR abs/2304.02277 (2023)\n",
      "483 Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma:\n",
      "UNICORN: A Unified Backdoor Trigger Inversion Framework. CoRR abs/2304.02786 (2023)\n",
      "484 Alexander Warnecke, Julian Speith, Jan-Niklas Möller, Konrad Rieck, Christof Paar:\n",
      "Evil from Within: Machine Learning Backdoors through Hardware Trojans. CoRR abs/2304.08411 (2023)\n",
      "485 Mohammad Naseri, Yufei Han, Emiliano De Cristofaro:\n",
      "BadVFL: Backdoor Attacks in Vertical Federated Learning. CoRR abs/2304.08847 (2023)\n",
      "486 Manaar Alam, Hithem Lamri, Michail Maniatakos:\n",
      "Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning. CoRR abs/2304.10638 (2023)\n",
      "487 Ming Yi, Yixiao Xu, Kangyi Ding, Mingyong Yin, Xiaolei Liu:\n",
      "Launching a Robust Backdoor Attack under Capability Constrained Scenarios. CoRR abs/2304.10985 (2023)\n",
      "488 Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai:\n",
      "Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration. CoRR abs/2304.11432 (2023)\n",
      "489 Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu:\n",
      "Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization. CoRR abs/2304.11823 (2023)\n",
      "490 Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun:\n",
      "BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT. CoRR abs/2304.12298 (2023)\n",
      "491 Yanbo Dai, Songze Li:\n",
      "Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. CoRR abs/2304.12961 (2023)\n",
      "492 Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, Chaowei Xiao:\n",
      "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. CoRR abs/2304.14475 (2023)\n",
      "493 Thuy Dung Nguyen, Anh Duy Nguyen, Kok-Seng Wong, Huy Hieu Pham, Thanh Hung Nguyen, Phi Le Nguyen, Truong Thao Nguyen:\n",
      "FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection. CoRR abs/2305.00328 (2023)\n",
      "494 Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, Jie Fu:\n",
      "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. CoRR abs/2305.01219 (2023)\n",
      "495 Wenqiang Sun, Sen Li, Yuchang Sun, Jun Zhang:\n",
      "DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning. CoRR abs/2305.01267 (2023)\n",
      "496 Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V. G. Vinod Vydiswaran:\n",
      "Defending against Insertion-based Textual Backdoor Attacks via Attribution. CoRR abs/2305.02394 (2023)\n",
      "497 Lichang Chen, Minhao Cheng, Heng Huang:\n",
      "Backdoor Learning on Sequence to Sequence Models. CoRR abs/2305.02424 (2023)\n",
      "498 Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu:\n",
      "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks. CoRR abs/2305.03289 (2023)\n",
      "499 Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su:\n",
      "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning. CoRR abs/2305.04175 (2023)\n",
      "500 Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun:\n",
      "Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias. CoRR abs/2305.04547 (2023)\n",
      "501 Shiyi Qi, Yuanhang Yang, Shuzheng Gao, Cuiyun Gao, Zenglin Xu:\n",
      "BadCS: A Backdoor Attack Framework for Code search. CoRR abs/2305.05503 (2023)\n",
      "502 Xin-Yi Fan, Hai-Jun Zhou:\n",
      "Backdoor to the Hidden Ground State: Planted Vertex Cover Example. CoRR abs/2305.06610 (2023)\n",
      "503 Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu:\n",
      "UOR: Universal Backdoor Attacks on Pre-trained Language Models. CoRR abs/2305.09574 (2023)\n",
      "504 Xinrui Liu, Yu-an Tan, Yajie Wang, Kefan Qiu, Yuanzhang Li:\n",
      "Stealthy Low-frequency Backdoor Attack against Deep Neural Networks. CoRR abs/2305.09677 (2023)\n",
      "505 Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie:\n",
      "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. CoRR abs/2305.10036 (2023)\n",
      "506 Xinrui Liu, Yajie Wang, Yu-An Tan, Kefan Qiu, Yuanzhang Li:\n",
      "Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks. CoRR abs/2305.10596 (2023)\n",
      "507 Yihao Huang, Qing Guo, Felix Juefei-Xu:\n",
      "Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization. CoRR abs/2305.10701 (2023)\n",
      "508 Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn:\n",
      "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. CoRR abs/2305.11596 (2023)\n",
      "509 Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen:\n",
      "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. CoRR abs/2305.14710 (2023)\n",
      "510 Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang:\n",
      "Reconstructive Neuron Pruning for Backdoor Defense. CoRR abs/2305.14876 (2023)\n",
      "511 Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen:\n",
      "From Shortcuts to Triggers: Backdoor Defense with Denoised PoE. CoRR abs/2305.14910 (2023)\n",
      "512 Xuanli He, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn:\n",
      "IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks. CoRR abs/2305.16503 (2023)\n",
      "513 Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo:\n",
      "Backdooring Neural Code Search. CoRR abs/2305.17506 (2023)\n",
      "514 Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma:\n",
      "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models. CoRR abs/2305.17826 (2023)\n",
      "515 Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji:\n",
      "Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. CoRR abs/2305.18384 (2023)\n",
      "516 Zhen Xiang, Zidi Xiong, Bo Li:\n",
      "UMD: Unsupervised Model Detection for X2X Backdoor Attacks. CoRR abs/2305.18651 (2023)\n",
      "517 Ashim Gupta, Amrith Krishna:\n",
      "Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. CoRR abs/2305.19607 (2023)\n",
      "518 Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, Baoyuan Wu:\n",
      "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers. CoRR abs/2306.00816 (2023)\n",
      "519 Han Gao:\n",
      "Mitigating Backdoor Attack Via Prerequisite Transformation. CoRR abs/2306.01983 (2023)\n",
      "520 Hao Yu, Chuan Ma, Meng Liu, Xinwang Liu, Zhe Liu, Ming Ding:\n",
      "G2uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. CoRR abs/2306.04984 (2023)\n",
      "521 Yinghua Gao, Yiming Li, Xueluan Gong, Shu-Tao Xia, Qian Wang:\n",
      "Backdoor Attack with Sparse and Invisible Trigger. CoRR abs/2306.06209 (2023)\n",
      "522 Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho:\n",
      "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. CoRR abs/2306.06874 (2023)\n",
      "523 Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao:\n",
      "DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. CoRR abs/2306.08009 (2023)\n",
      "524 Haochen Mei, Gaolei Li, Jun Wu, Longfei Zheng:\n",
      "Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. CoRR abs/2306.08011 (2023)\n",
      "525 Ziqiang Li, Hong Sun, Pengfei Xia, Beihao Xia, Xue Rui, Wei Zhang, Bin Li:\n",
      "A Proxy-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks. CoRR abs/2306.08313 (2023)\n",
      "526 Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu:\n",
      "Multi-target Backdoor Attacks for Code Pre-trained Models. CoRR abs/2306.08350 (2023)\n",
      "527 Hong Sun, Ziqiang Li, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li:\n",
      "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios. CoRR abs/2306.08386 (2023)\n",
      "528 Fan Liu, Siqi Lai, Yansong Ning, Hao Liu:\n",
      "Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network. CoRR abs/2306.10351 (2023)\n",
      "529 Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Bisheng Tang, Ye Dong:\n",
      "Practical and General Backdoor Attacks against Vertical Federated Learning. CoRR abs/2306.10746 (2023)\n",
      "530 Mozhgan PourKeshavarz, Mohammad Sabokrou, Amir Rasouli:\n",
      "IMPOSITION: Implicit Backdoor Attack through Scenario Injection. CoRR abs/2306.15755 (2023)\n",
      "531 Zhe Ye, Terui Mao, Li Dong, Diqun Yan:\n",
      "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion. CoRR abs/2306.15875 (2023)\n",
      "532 Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu:\n",
      "Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound. CoRR abs/2306.16022 (2023)\n",
      "533 Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu:\n",
      "Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features. CoRR abs/2306.16697 (2023)\n",
      "534 Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Nazanin Rahnavard:\n",
      "Efficient Backdoor Removal Through Natural Gradient Fine-tuning. CoRR abs/2306.17441 (2023)\n",
      "535 Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin:\n",
      "Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data. CoRR abs/2307.00356 (2023)\n",
      "536 Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami:\n",
      "Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. CoRR abs/2307.05422 (2023)\n",
      "537 Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu:\n",
      "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. CoRR abs/2307.07328 (2023)\n",
      "538 Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li:\n",
      "Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound. CoRR abs/2307.08208 (2023)\n",
      "539 Waris Gill, Ali Anwar, Muhammad Ali Gulzar:\n",
      "FedDefender: Backdoor Attack Defense in Federated Learning. CoRR abs/2307.08672 (2023)\n",
      "540 Cheng Chu, Fan Chen, Philip Richerme, Lei Jiang:\n",
      "QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks. CoRR abs/2307.09529 (2023)\n",
      "541 Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry:\n",
      "Rethinking Backdoor Attacks. CoRR abs/2307.10163 (2023)\n",
      "542 Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang:\n",
      "A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. CoRR abs/2307.10184 (2023)\n",
      "543 Yize Cheng, Wenbin Hu, Minhao Cheng:\n",
      "Backdoor Attack against Object Detection with Clean Annotation. CoRR abs/2307.10487 (2023)\n",
      "544 Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu:\n",
      "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. CoRR abs/2307.10562 (2023)\n",
      "545 Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui:\n",
      "FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. CoRR abs/2307.11565 (2023)\n",
      "546 Baochen Yan, Jiahe Lan, Zheng Yan:\n",
      "Backdoor Attacks against Voice Recognition Systems: A Survey. CoRR abs/2307.13643 (2023)\n",
      "547 Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini:\n",
      "Backdoor Attacks for In-Context Learning with Language Models. CoRR abs/2307.14692 (2023)\n",
      "548 Min Liu, Alberto L. Sangiovanni-Vincentelli, Xiangyu Yue:\n",
      "Backdoor Defense with Non-Adversarial Backdoor. CoRR abs/2307.15539 (2023)\n",
      "549 Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao:\n",
      "You Can Backdoor Personalized Federated Learning. CoRR abs/2307.15971 (2023)\n",
      "550 Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian:\n",
      "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models. CoRR abs/2307.16489 (2023)\n",
      "551 Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha:\n",
      "TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. CoRR abs/2308.03906 (2023)\n",
      "552 Zihan Guan, Mengnan Du, Ninghao Liu:\n",
      "XGBD: Explanation-Guided Graph Backdoor Detection. CoRR abs/2308.04406 (2023)\n",
      "553 Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan:\n",
      "Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. CoRR abs/2308.04466 (2023)\n",
      "554 Hang Wang, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. CoRR abs/2308.04617 (2023)\n",
      "555 Jiyang Guan, Jian Liang, Ran He:\n",
      "Test-Time Adaptation for Backdoor Defense. CoRR abs/2308.06107 (2023)\n",
      "556 Xi Li, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Backdoor Mitigation by Correcting the Distribution of Neural Activations. CoRR abs/2308.09850 (2023)\n",
      "557 Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev:\n",
      "Hiding Backdoors within Event Sequence Data via Poisoning Attacks. CoRR abs/2308.10201 (2023)\n",
      "558 Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang:\n",
      "Backdooring Textual Inversion for Concept Censorship. CoRR abs/2308.10718 (2023)\n",
      "559 Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis:\n",
      "Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. CoRR abs/2308.11070 (2023)\n",
      "560 Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen:\n",
      "Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. CoRR abs/2308.11333 (2023)\n",
      "561 Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu:\n",
      "PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. CoRR abs/2308.11822 (2023)\n",
      "562 Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal:\n",
      "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. CoRR abs/2308.12439 (2023)\n",
      "563 Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen:\n",
      "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. CoRR abs/2308.13904 (2023)\n",
      "564 Haomiao Yang, Kunlan Xiang, Hongwei Li, Rongxing Lu:\n",
      "A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks. CoRR abs/2308.14367 (2023)\n",
      "565 Sze Jue Yang, Quang H. Nguyen, Chee Seng Chan, Khoa Doan:\n",
      "Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. CoRR abs/2308.16684 (2023)\n",
      "566 Yanqi Qiao, Dazhuang Liu, Congwen Chen, Rui Wang, Kaitai Liang:\n",
      "FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning. CoRR abs/2309.00127 (2023)\n",
      "567 Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan:\n",
      "BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment. CoRR abs/2309.01480 (2023)\n",
      "568 Guohong Wang, Hua Ma, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Wei Kang, Said F. Al-Sarawi, Gongxuan Zhang, Derek Abbott:\n",
      "One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning. CoRR abs/2309.04036 (2023)\n",
      "569 Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Gongshen Liu:\n",
      "Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review. CoRR abs/2309.06055 (2023)\n",
      "570 Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan:\n",
      "MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems. CoRR abs/2309.06981 (2023)\n",
      "571 Yusheng Guo, Nan Zhong, Zhenxing Qian, Xinpeng Zhang:\n",
      "Physical Invisible Backdoor Based on Camera Imaging. CoRR abs/2309.07428 (2023)\n",
      "572 Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang:\n",
      "Robust Backdoor Attacks on Object Detection in Real World. CoRR abs/2309.08953 (2023)\n",
      "573 Weina Dong, Jia Liu, Yan Ke, Lifeng Chen, Wenquan Sun, Xiaozhong Pan:\n",
      "Steganography for Neural Radiance Fields by Backdooring. CoRR abs/2309.10503 (2023)\n",
      "574 Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang:\n",
      "Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. CoRR abs/2309.13256 (2023)\n",
      "575 Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng:\n",
      "Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. CoRR abs/2309.16456 (2023)\n",
      "576 Hua Ma, Shang Wang, Yansong Gao:\n",
      "Horizontal Class Backdoor to Deep Learning. CoRR abs/2310.00542 (2023)\n",
      "577 Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin:\n",
      "GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning. CoRR abs/2310.00626 (2023)\n",
      "578 Rui Min, Zeyu Qin, Li Shen, Minhao Cheng:\n",
      "Towards Stable Backdoor Purification through Feature Shift Tuning. CoRR abs/2310.01875 (2023)\n",
      "579 Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad-Reza Sadeghi:\n",
      "FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks. CoRR abs/2310.02113 (2023)\n",
      "580 Xiruo Ding, Zhecheng Sheng, Meliha Yetisgen, Serguei Pakhomov, Trevor Cohen:\n",
      "Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes. CoRR abs/2310.02451 (2023)\n",
      "581 Pengfei He, Han Xu, Yue Xing, Jie Ren, Yingqian Cui, Shenglai Zeng, Jiliang Tang, Makoto Yamada, Mohammad Sabokrou:\n",
      "Confidence-driven Sampling for Backdoor Attacks. CoRR abs/2310.05263 (2023)\n",
      "582 Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman:\n",
      "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks. CoRR abs/2310.05862 (2023)\n",
      "583 Daniel Israel, Aditya Grover, Guy Van den Broeck:\n",
      "High Dimensional Causal Inference with Variational Backdoor Adjustment. CoRR abs/2310.06100 (2023)\n",
      "584 Mahmoud Nazzal, Nura Aljaafari, Ahmad Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Abdallah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed:\n",
      "Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. CoRR abs/2310.06855 (2023)\n",
      "585 Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang:\n",
      "Prompt Backdoors in Visual Prompt Learning. CoRR abs/2310.07632 (2023)\n",
      "586 Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang:\n",
      "Composite Backdoor Attacks Against Large Language Models. CoRR abs/2310.07676 (2023)\n",
      "587 Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek:\n",
      "Invisible Threats: Backdoor Attack in OCR Systems. CoRR abs/2310.08259 (2023)\n",
      "588 Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting:\n",
      "Defending Our Privacy With Backdoors. CoRR abs/2310.08320 (2023)\n",
      "589 Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li:\n",
      "Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks. CoRR abs/2310.09744 (2023)\n",
      "590 Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang:\n",
      "Backdoor Attack through Machine Unlearning. CoRR abs/2310.10659 (2023)\n",
      "591 Ganghua Wang, Xun Xian, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding:\n",
      "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. CoRR abs/2310.10780 (2023)\n",
      "592 Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong:\n",
      "Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. CoRR abs/2310.11594 (2023)\n",
      "593 Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen:\n",
      "WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks. CoRR abs/2310.11595 (2023)\n",
      "594 Hongwei Yao, Jian Lou, Zhan Qin:\n",
      "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. CoRR abs/2310.12439 (2023)\n",
      "595 Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen:\n",
      "Attention-Enhancing Backdoor Attacks Against BERT-based Models. CoRR abs/2310.14480 (2023)\n",
      "596 Zhen Xiang, Zidi Xiong, Bo Li:\n",
      "CBD: A Certified Backdoor Detector Based on Local Dominant Probability. CoRR abs/2310.17498 (2023)\n",
      "597 Wencong You, Zayd Hammoudeh, Daniel Lowd:\n",
      "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers. CoRR abs/2310.18603 (2023)\n",
      "598 Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu:\n",
      "Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots. CoRR abs/2310.18633 (2023)\n",
      "599 Xi Li, Songhe Wang, Chen Wu, Hao Zhou, Jiaqi Wang:\n",
      "Backdoor Threats from Compromised Foundation Models to Federated Learning. CoRR abs/2311.00144 (2023)\n",
      "600 Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu:\n",
      "From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models. CoRR abs/2311.02373 (2023)\n",
      "601 Fereshteh Razmi, Jian Lou, Li Xiong:\n",
      "Does Differential Privacy Prevent Backdoor Attacks in Practice? CoRR abs/2311.06227 (2023)\n",
      "602 Soroush Hashemifar, Saeed Parsa, Morteza Zakeri Nasrabadi:\n",
      "Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration. CoRR abs/2311.07417 (2023)\n",
      "603 Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha A. Larson, Stjepan Picek:\n",
      "Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. CoRR abs/2311.07550 (2023)\n",
      "604 Haoran Wang, Kai Shu:\n",
      "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment. CoRR abs/2311.09433 (2023)\n",
      "605 Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen:\n",
      "Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations. CoRR abs/2311.09763 (2023)\n",
      "606 Sheldon C. Ebron Jr., Kan Yang:\n",
      "FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework. CoRR abs/2311.10248 (2023)\n",
      "607 Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song:\n",
      "TextGuard: Provable Defense against Backdoor Attacks on Text Classification. CoRR abs/2311.11225 (2023)\n",
      "608 Javier Rando, Florian Tramèr:\n",
      "Universal Jailbreak Backdoors from Poisoned Human Feedback. CoRR abs/2311.14455 (2023)\n",
      "609 Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P. Dickerson, Jeff A. Bilmes:\n",
      "Effective Backdoor Mitigation Depends on the Pre-training Objective. CoRR abs/2311.14948 (2023)\n",
      "610 Yao Huang, Kongyang Chen, Jiannong Cao, Jiaxing Shen, Shaowei Wang, Yun Peng, Weilong Peng, Kechao Cai:\n",
      "BAGEL: Backdoor Attacks against Federated Contrastive Learning. CoRR abs/2311.16113 (2023)\n",
      "611 Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu:\n",
      "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP. CoRR abs/2311.16194 (2023)\n",
      "612 Ming-Yu Chung, Sheng-Yen Chou, Chia-Mu Yu, Pin-Yu Chen, Sy-Yen Kuo, Tsung-Yi Ho:\n",
      "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective. CoRR abs/2311.16646 (2023)\n",
      "613 Zihao Tan, Qingliang Chen, Yongjian Huang, Chen Liang:\n",
      "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. CoRR abs/2311.17429 (2023)\n",
      "614 Xi Li, Chen Wu, Jiaqi Wang:\n",
      "Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning. CoRR abs/2311.18350 (2023)\n",
      "615 Yuanpu Cao, Bochuan Cao, Jinghui Chen:\n",
      "Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections. CoRR abs/2312.00027 (2023)\n",
      "616 Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao, Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu Zhang:\n",
      "Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift. CoRR abs/2312.00050 (2023)\n",
      "617 Benjamin Schneider, Nils Lukas, Florian Kerschbaum:\n",
      "Universal Backdoor Attacks. CoRR abs/2312.00157 (2023)\n",
      "618 Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi:\n",
      "OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection. CoRR abs/2312.01585 (2023)\n",
      "619 Xiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao, Yang Xiang:\n",
      "Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics. CoRR abs/2312.02673 (2023)\n",
      "620 Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Eugene Bagdasaryan, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan:\n",
      "Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models. CoRR abs/2312.03419 (2023)\n",
      "621 Yiming Li, Mingyan Zhu, Junfeng Guo, Tao Wei, Shu-Tao Xia, Zhan Qin:\n",
      "Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. CoRR abs/2312.04584 (2023)\n",
      "622 Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang:\n",
      "BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting. CoRR abs/2312.04902 (2023)\n",
      "623 Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu:\n",
      "Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks. CoRR abs/2312.06230 (2023)\n",
      "624 Shengsheng Qian, Yifei Wang, Dizhan Xue, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu:\n",
      "Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking. CoRR abs/2312.07955 (2023)\n",
      "625 Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain:\n",
      "Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey. CoRR abs/2312.08667 (2023)\n",
      "626 Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang:\n",
      "On the Difficulty of Defending Contrastive Learning against Backdoor Attacks. CoRR abs/2312.09057 (2023)\n",
      "627 Jiahe Lan, Jie Wang, Baochen Yan, Zheng Yan, Elisa Bertino:\n",
      "FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge. CoRR abs/2312.09665 (2023)\n",
      "628 Bingyin Zhao, Yingjie Lao:\n",
      "UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks. CoRR abs/2312.10657 (2023)\n",
      "629 Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma:\n",
      "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models. CoRR abs/2312.11057 (2023)\n",
      "630 Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang:\n",
      "BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning. CoRR abs/2312.12585 (2023)\n",
      "631 Yiming Chen, Haiwei Wu, Jiantao Zhou:\n",
      "Progressive Poisoned Data Isolation for Training-time Backdoor Defense. CoRR abs/2312.12724 (2023)\n",
      "632 Kaouther Messaoud, Kathrin Grosse, Mickaël Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi:\n",
      "Manipulating Trajectory Prediction with Backdoors. CoRR abs/2312.13863 (2023)\n",
      "633 Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, Piji Li:\n",
      "Punctuation Matters! Stealthy Backdoor Attack for Language Models. CoRR abs/2312.15867 (2023)\n",
      "634 Marshall Ball, Yevgeniy Dodis, Eli Goldin:\n",
      "Immunizing Backdoored PRGs. IACR Cryptol. ePrint Arch. 2023: 1778 (2023)\n",
      "635 Hannah Davis, Matthew Green, Nadia Heninger, Keegan Ryan, Adam Suhl:\n",
      "On the Possibility of a Backdoor in the Micali-Schnorr Generator. IACR Cryptol. ePrint Arch. 2023: 440 (2023)\n",
      "636 Shah Fahd, Mehreen Afzal, Waseem Iqbal, Dawood Shah, Ijaz Khalid:\n",
      "The Reality of Backdoored S-Boxes - An Eye Opener. IACR Cryptol. ePrint Arch. 2023: 1073 (2023)\n",
      "637 Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang:\n",
      "(Withdrawn) BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning. CoRR abs/2311.12075 (2023)\n",
      "638 Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami:\n",
      "A Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation. IEEE Access 10: 5545-5558 (2022)\n",
      "639 Jiawei Zhu, Lin Chen, Dongwei Xu, Wenhong Zhao:\n",
      "Backdoor Defence for Voice Print Recognition Model Based on Speech Enhancement and Weight Pruning. IEEE Access 10: 114016-114023 (2022)\n",
      "640 Qin Liu, Liqiong Chen, Hongbo Jiang, Jie Wu, Tian Wang, Tao Peng, Guojun Wang:\n",
      "A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks. Ad Hoc Networks 124: 102727 (2022)\n",
      "641 Tasniem Nasser Al-Yahya, Mohamed El Bachir Menai, Hassan Mathkour:\n",
      "Boosting the Performance of CDCL-Based SAT Solvers by Exploiting Backbones and Backdoors. Algorithms 15(9): 302 (2022)\n",
      "642 Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "Active intellectual property protection for deep neural networks through stealthy backdoor and users' identities authentication. Appl. Intell. 52(14): 16497-16511 (2022)\n",
      "643 Guangrui Liu, Weizhe Zhang, Xinjie Li, Kaisheng Fan, Shui Yu:\n",
      "VulnerGAN: a backdoor attack through vulnerability amplification against machine learning-based network intrusion detection systems. Sci. China Inf. Sci. 65(7): 1-19 (2022)\n",
      "644 Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, Ian M. Molloy:\n",
      "Backdoor smoothing: Demystifying backdoor attacks on deep neural networks. Comput. Secur. 120: 102814 (2022)\n",
      "645 Shiwei Lu, Ruihu Li, Wenbin Liu, Xuan Chen:\n",
      "Defense against backdoor attack in federated learning. Comput. Secur. 121: 102819 (2022)\n",
      "646 Kun Shao, Yu Zhang, Junan Yang, Xiaoshuai Li, Hui Liu:\n",
      "The triggers that open the NLP model backdoors are hidden in the adversarial samples. Comput. Secur. 118: 102730 (2022)\n",
      "647 Mingfu Xue, Can He, Yinghao Wu, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "PTB: Robust physical backdoor attacks against deep neural networks in real world. Comput. Secur. 118: 102726 (2022)\n",
      "648 Shaofeng Li, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Suguo Du, Haojin Zhu:\n",
      "Backdoors Against Natural Language Processing: A Review. IEEE Secur. Priv. 20(5): 50-59 (2022)\n",
      "649 Tatsuya Oyama, Shunsuke Okura, Kota Yoshida, Takeshi Fujino:\n",
      "Experimental Study of Fault Injection Attack on Image Sensor Interface for Triggering Backdoored DNN Models. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 105-A(3): 336-343 (2022)\n",
      "650 Hyun Kwon:\n",
      "Multi-Model Selective Backdoor Attack with Different Trigger Positions. IEICE Trans. Inf. Syst. 105-D(1): 170-174 (2022)\n",
      "651 Xiao Yu, Liu Cong, Mingwen Zheng, Yajie Wang, Xinrui Liu, Song Shuxiao, Ma Yuexuan, Zheng Jun:\n",
      "A multitarget backdooring attack on deep neural networks with random location trigger. Int. J. Intell. Syst. 37(3): 2567-2583 (2022)\n",
      "652 Fatima Elhattab, Sara Bouchenak, Rania Talbi, Vlad Nitu:\n",
      "Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 6(4): 162:1-162:27 (2022)\n",
      "653 Ethan Brewer, Jason Lin, Daniel S. Miller Runfola:\n",
      "Susceptibility & defense of satellite image-trained convolutional networks to backdoor attacks. Inf. Sci. 603: 244-261 (2022)\n",
      "654 Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang:\n",
      "Robust backdoor injection with the capability of resisting network transfer. Inf. Sci. 612: 594-611 (2022)\n",
      "655 Yuyang Zhou, Liang Zhao, Yuqiao Jin, Fagen Li:\n",
      "Backdoor-resistant identity-based proxy re-encryption for cloud-assisted wireless body area networks. Inf. Sci. 604: 80-96 (2022)\n",
      "656 Nuria Rodríguez Barroso, Eugenio Martínez-Cámara, María Victoria Luzón, Francisco Herrera:\n",
      "Backdoor attacks-resilient aggregation based on Robust Filtering of Outliers in federated learning for image classification. Knowl. Based Syst. 245: 108588 (2022)\n",
      "657 Hyun Kwon, Yongchul Kim:\n",
      "BlindNet backdoor: Attack on deep neural network using blind watermark. Multim. Tools Appl. 81(5): 6217-6234 (2022)\n",
      "658 Xueluan Gong, Yanjiao Chen, Huayang Huang, Yuqing Liao, Shuai Wang, Qian Wang:\n",
      "Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers. IEEE Netw. 36(1): 84-90 (2022)\n",
      "659 Congcong Chen, Lifei Wei, Lei Zhang, Ya Peng, Jianting Ning:\n",
      "MP-BADNet+: Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs. Peer-to-Peer Netw. Appl. 15(6): 2457-2473 (2022)\n",
      "660 Yixiao Xu, Xiaolei Liu, Kangyi Ding, Bangzhou Xin:\n",
      "IBD: An Interpretable Backdoor-Detection Method via Multivariate Interactions. Sensors 22(22): 8697 (2022)\n",
      "661 Raphaël Khoury, Sylvain Hallé:\n",
      "Are Backdoor Mandates Ethical? - A Position Paper. IEEE Technol. Soc. Mag. 41(4): 63-70 (2022)\n",
      "662 Wei Jiang, Xiangyu Wen, Jinyu Zhan, Xupeng Wang, Ziwei Song:\n",
      "Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks. IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 41(8): 2611-2624 (2022)\n",
      "663 Zhengming Zhang, Ruming Yang, Xiangyu Zhang, Chunguo Li, Yongming Huang, Luxi Yang:\n",
      "Backdoor Federated Learning-Based mmWave Beam Selection. IEEE Trans. Commun. 70(10): 6563-6578 (2022)\n",
      "664 Chaoran Li, Xiao Chen, Derui Wang, Sheng Wen, Muhammad Ejaz Ahmed, Seyit Camtepe, Yang Xiang:\n",
      "Backdoor Attack on Machine Learning Based Android Malware Detectors. IEEE Trans. Dependable Secur. Comput. 19(5): 3357-3370 (2022)\n",
      "665 Mingfu Xue, Can He, Jian Wang, Weiqiang Liu:\n",
      "One-to-N & N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models. IEEE Trans. Dependable Secur. Comput. 19(3): 1562-1578 (2022)\n",
      "666 Zhenzhu Chen, Shang Wang, Anmin Fu, Yansong Gao, Shui Yu, Robert H. Deng:\n",
      "LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check. IEEE Trans. Inf. Forensics Secur. 17: 2000-2014 (2022)\n",
      "667 Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans:\n",
      "Stealthy Backdoors as Compression Artifacts. IEEE Trans. Inf. Forensics Secur. 17: 1372-1387 (2022)\n",
      "668 Yulong Wang, Minghui Zhao, Shenghong Li, Xin Yuan, Wei Ni:\n",
      "Dispersed Pixel Perturbation-Based Imperceptible Backdoor Trigger for Image Classifier Models. IEEE Trans. Inf. Forensics Secur. 17: 3091-3106 (2022)\n",
      "669 Boyu Hou, Jiqiang Gao, Xiaojie Guo, Thar Baker, Ying Zhang, Yanlong Wen, Zheli Liu:\n",
      "Mitigating the Backdoor Attack by Federated Filters for Industrial IoT Applications. IEEE Trans. Ind. Informatics 18(5): 3562-3571 (2022)\n",
      "670 Jie Zhang, Dongdong Chen, Qidong Huang, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu:\n",
      "Poison Ink: Robust and Invisible Backdoor Attack. IEEE Trans. Image Process. 31: 5691-5705 (2022)\n",
      "671 Chien-Lun Chen, Sara Babakniya, Marco Paolieri, Leana Golubchik:\n",
      "Defending against Poisoning Backdoor Attacks on Federated Meta-learning. ACM Trans. Intell. Syst. Technol. 13(5): 76:1-76:25 (2022)\n",
      "672 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Detection of Backdoors in Trained Classifiers Without Access to the Training Set. IEEE Trans. Neural Networks Learn. Syst. 33(3): 1177-1191 (2022)\n",
      "673 Derui Wang, Sheng Wen, Alireza Jolfaei, Mohammad Sayad Haghighi, Surya Nepal, Yang Xiang:\n",
      "On the Neural Backdoor of Federated Generative Models in Edge Computing. ACM Trans. Internet Techn. 22(2): 43:1-43:21 (2022)\n",
      "674 Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, Sudipta Chattopadhyay:\n",
      "Model Agnostic Defence Against Backdoor Attacks in Machine Learning. IEEE Trans. Reliab. 71(2): 880-895 (2022)\n",
      "675 Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen:\n",
      "Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models. IEEE Trans. Serv. Comput. 15(3): 1526-1539 (2022)\n",
      "676 Wolfgang Dvorák, Markus Hecher, Matthias König, André Schidler, Stefan Szeider, Stefan Woltran:\n",
      "Tractable Abstract Argumentation via Backdoor-Treewidth. AAAI 2022: 5608-5615\n",
      "677 Shihong Fang, Anna Choromanska:\n",
      "Backdoor Attacks on the DNN Interpretation System. AAAI 2022: 561-570\n",
      "678 Serge Gaspers, Andrew Kaploun:\n",
      "Faster Algorithms for Weak Backdoors. AAAI 2022: 3741-3748\n",
      "679 Jinyuan Jia, Yupei Liu, Xiaoyu Cao, Neil Zhenqiang Gong:\n",
      "Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks. AAAI 2022: 9575-9583\n",
      "680 Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina:\n",
      "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework. AAAI 2022: 3786-3795\n",
      "681 Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu, Chonggang Wang:\n",
      "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks. AAAI 2022: 10309-10318\n",
      "682 Alexander A. Semenov, Artem Pavlenko, Daniil Chivilikhin, Stepan Kochemazov:\n",
      "On Probabilistic Generalization of Backdoors in Boolean Satisfiability. AAAI 2022: 10353-10361\n",
      "683 Hadi M. Dolatabadi, Sarah M. Erfani, Christopher Leckie:\n",
      "COLLIDER: A Robust Training Framework for Backdoor Data. ACCV (6) 2022: 681-698\n",
      "684 Xutong Wang, Chaoge Liu, Xiaohui Hu, Zhi Wang, Jie Yin, Xiang Cui:\n",
      "Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks. ACSAC 2022: 267-278\n",
      "685 Jing Xu, Rui Wang, Stefanos Koffas, Kaitai Liang, Stjepan Picek:\n",
      "More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. ACSAC 2022: 684-698\n",
      "686 Menisov Artem:\n",
      "An Approach to Generation Triggers for Parrying Backdoor in Neural Networks. AGI 2022: 304-314\n",
      "687 Stefanos Koffas, Stjepan Picek, Mauro Conti:\n",
      "Dynamic Backdoors with Global Average Pooling. AICAS 2022: 320-323\n",
      "688 Yinghao Wu, Mingfu Xue, Dujuan Gu, Yushu Zhang, Weiqiang Liu:\n",
      "Sample-Specific Backdoor based Active Intellectual Property Protection for Deep Neural Networks. AICAS 2022: 316-319\n",
      "689 Nour Alhussien, Ahmed Aleroud, Reza Rahaeimehr, Alexander Schwarzmann:\n",
      "Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection. BDCAT 2022: 40-47\n",
      "690 Ricardo A. Calix, J. J. Ben-Joseph, Nina Lopatina, Ryan Ashley, Mona Gogia, George Sieniawski, Andrea Brennen:\n",
      "Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition. IEEE Big Data 2022: 2940-2949\n",
      "691 Saquib Irtiza, Latifur Khan, Kevin W. Hamlen:\n",
      "SentMod: Hidden Backdoor Attack on Unstructured Textual Data. BigDataSecurity/HPSC/IDS 2022: 224-231\n",
      "692 Jin Yan, Yingchi Mao, Hua Nie, Zijian Tu, Jianxin Huang:\n",
      "A Federated Learning Backdoor Attack Defense. BigDataService 2022: 143-148\n",
      "693 Alexander Unnervik, Sébastien Marcel:\n",
      "An anomaly detection approach for backdoored neural networks: face recognition as a case study. BIOSIG 2022: 80-88\n",
      "694 Hasan Abed Al Kader Hammoud, Bernard Ghanem:\n",
      "Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain. BMVC 2022: 259\n",
      "695 Long H. Pham, Jun Sun:\n",
      "Verifying Neural Networks Against Backdoor Attacks. CAV (1) 2022: 171-192\n",
      "696 Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta:\n",
      "Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets. CCS 2022: 3315-3317\n",
      "697 Tong Wu, Tianhao Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal:\n",
      "Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. AISec@CCS 2022: 91-102\n",
      "698 Jing Xu, Stjepan Picek:\n",
      "Poster: Clean-label Backdoor Attack on Graph Neural Networks. CCS 2022: 3491-3493\n",
      "699 Artem Pavlenko, Daniil Chivilikhin, Alexander A. Semenov:\n",
      "Asynchronous Evolutionary Algorithm for Finding Backdoors in Boolean Satisfiability. CEC 2022: 1-8\n",
      "700 Qi Wang, Wenxin Li, Kang Yang, Yiru Zhao, Lei Zhao, Lina Wang:\n",
      "Efficient DNN Backdoor Detection Guided by Static Weight Analysis. Inscrypt 2022: 408-428\n",
      "701 Heng-Yang Lu, Chenyou Fan, Jun Yang, Cong Hu, Wei Fang, Xiao-Jun Wu:\n",
      "Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications. COLING 2022: 984-993\n",
      "702 Wolfgang Dvorák, Matthias König, Stefan Woltran:\n",
      "Deletion-Backdoors for Argumentation Frameworks with Collective Attacks. SAFA@COMMA 2022: 98-110\n",
      "703 Aaron M. Ferber, Jialin Song, Bistra Dilkina, Yisong Yue:\n",
      "Learning Pseudo-Backdoors for Mixed Integer Programs. CPAIOR 2022: 91-102\n",
      "704 Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao:\n",
      "FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis. CVPR 2022: 20844-20853\n",
      "705 Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao:\n",
      "Few-shot Backdoor Defense Using Shapley Estimation. CVPR 2022: 13348-13357\n",
      "706 Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, Xiangyu Zhang:\n",
      "Complex Backdoor Detection by Symmetric Feature Differencing. CVPR 2022: 14983-14993\n",
      "707 Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu:\n",
      "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. CVPR 2022: 13337-13347\n",
      "708 Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash:\n",
      "Backdoor Attacks on Self-Supervised Learning. CVPR 2022: 13327-13336\n",
      "709 Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, Xiangyu Zhang:\n",
      "Better Trigger Inversion Optimization in Backdoor Scanning. CVPR 2022: 13358-13368\n",
      "710 Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha:\n",
      "Dual-Key Multimodal Backdoors for Visual Question Answering. CVPR 2022: 15354-15364\n",
      "711 Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, Kaitai Liang:\n",
      "DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints. CVPR 2022: 15192-15201\n",
      "712 Andrea Milakovic, Rudolf Mayer:\n",
      "Combining Defences Against Data-Poisoning Based Backdoor Attacks on Neural Networks. DBSec 2022: 28-47\n",
      "713 Nandish Chattopadhyay, Rajan Kataria, Anupam Chattopadhyay:\n",
      "TextBack: Watermarking Text Classifiers using Backdooring. DSD 2022: 340-347\n",
      "714 Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, Jun Zhou:\n",
      "BadDet: Backdoor Attacks on Object Detection. ECCV Workshops (1) 2022: 396-412\n",
      "715 Huy Phan, Cong Shi, Yi Xie, Tianfang Zhang, Zhuohang Li, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen, Bo Yuan:\n",
      "RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN. ECCV (4) 2022: 708-724\n",
      "716 Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang:\n",
      "An Invisible Black-Box Backdoor Attack Through Frequency Domain. ECCV (13) 2022: 396-413\n",
      "717 Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu:\n",
      "Data-Free Backdoor Removal Based on Channel Lipschitzness. ECCV (5) 2022: 175-191\n",
      "718 Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun:\n",
      "Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. EMNLP 2022: 11215-11221\n",
      "719 Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun:\n",
      "Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks. EMNLP (Findings) 2022: 668-683\n",
      "720 Lesheng Jin, Zihan Wang, Jingbo Shang:\n",
      "WeDef: Weakly Supervised Backdoor Defense for Text Classification. EMNLP 2022: 11614-11626\n",
      "721 KiYoon Yoo, Nojun Kwak:\n",
      "Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling. EMNLP 2022: 72-88\n",
      "722 Zhiyuan Zhang, Qi Su, Xu Sun:\n",
      "Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation. EMNLP (Findings) 2022: 339-354\n",
      "723 Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun:\n",
      "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models. EMNLP (Findings) 2022: 355-372\n",
      "724 Jan Dreier, Sebastian Ordyniak, Stefan Szeider:\n",
      "SAT Backdoors: Depth Beats Size. ESA 2022: 46:1-46:18\n",
      "725 Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu:\n",
      "Kallima: A Clean-Label Framework for Textual Backdoor Attacks. ESORICS (1) 2022: 447-466\n",
      "726 Ambrish Rawat, Killian Levacher, Mathieu Sinn:\n",
      "The Devil Is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models. ESORICS (3) 2022: 776-783\n",
      "727 Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Xiapu Luo, Ting Wang:\n",
      "TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors. EuroS&P 2022: 684-702\n",
      "728 Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, Yang Zhang:\n",
      "Dynamic Backdoor Attacks Against Machine Learning Models. EuroS&P 2022: 703-718\n",
      "729 Yu Wang, Haomiao Yang, Jiasheng Li, Mengyu Ge:\n",
      "A Pragmatic Label-Specific Backdoor Attack. FCS 2022: 149-162\n",
      "730 Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, Or Zamir:\n",
      "Planting Undetectable Backdoors in Machine Learning Models : [Extended Abstract]. FOCS 2022: 931-942\n",
      "731 Zheng Yang, Gaolei Li, Jun Wu, Wu Yang:\n",
      "Propagable Backdoors over Blockchain-based Federated Learning via Sample-Specific Eclipse. GLOBECOM 2022: 2579-2584\n",
      "732 Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, Xudong Feng:\n",
      "A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning. GLOBECOM 2022: 2710-2715\n",
      "733 Daniel Lokshtanov, Fahad Panolan, M. S. Ramanujan:\n",
      "Backdoor Sets on Nowhere Dense SAT. ICALP 2022: 91:1-91:20\n",
      "734 Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang:\n",
      "Stealthy Backdoor Attack with Adversarial Training. ICASSP 2022: 2969-2973\n",
      "735 Xi Li, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks. ICASSP 2022: 3333-3337\n",
      "736 Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li:\n",
      "Object-Oriented Backdoor Attack Against Image Captioning. ICASSP 2022: 2864-2868\n",
      "737 Lu Miao, Wei Yang, Rong Hu, Lu Li, Liusheng Huang:\n",
      "Against Backdoor Attacks In Federated Learning With Differential Privacy. ICASSP 2022: 2999-3003\n",
      "738 Huy Phan, Yi Xie, Jian Liu, Yingying Chen, Bo Yuan:\n",
      "Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks. ICASSP 2022: 96-100\n",
      "739 Vardaan Taneja, Pin-Yu Chen, Yuguang Yao, Sijia Liu:\n",
      "When Does Backdoor Attack Succeed in Image Reconstruction? A Study of Heuristics vs. Bi-Level Solution. ICASSP 2022: 4398-4402\n",
      "740 Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis:\n",
      "Detecting Backdoor Attacks against Point Cloud Classifiers. ICASSP 2022: 3159-3163\n",
      "741 Chen Wu, Xian Yang, Sencun Zhu, Prasenjit Mitra:\n",
      "Toward Cleansing Backdoored Neural Networks in Federated Learning. ICDCS 2022: 820-830\n",
      "742 Daizong Ding, Mi Zhang, Yuanmin Huang, Xudong Pan, Fuli Feng, Erling Jiang, Min Yang:\n",
      "Towards Backdoor Attack on Deep Learning based Time Series Classification. ICDE 2022: 1274-1287\n",
      "743 Xiangkai Yang, Wenjian Luo, Licai Zhang, Zhijian Chen, Jiahai Wang:\n",
      "Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models. ICDIS 2022: 61-66\n",
      "744 John T. Holodnak, Olivia M. Brown, Jason Matterer, Andrew Lemke:\n",
      "Backdoor Poisoning of Encrypted Traffic Classifiers. ICDM (Workshops) 2022: 577-585\n",
      "745 Nino Cauli, Alessandro Ortis, Sebastiano Battiato:\n",
      "Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack. ICIAP (2) 2022: 176-185\n",
      "746 Huxiao Ji, Jie Li, Chentao Wu:\n",
      "CRAB: Certified Patch Robustness Against Poisoning-Based Backdoor Attacks. ICIP 2022: 2486-2490\n",
      "747 Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia:\n",
      "Few-Shot Backdoor Attacks on Visual Object Tracking. ICLR 2022\n",
      "748 Nicholas Carlini, Andreas Terzis:\n",
      "Poisoning and Backdooring Contrastive Learning. ICLR 2022\n",
      "749 Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan:\n",
      "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models. ICLR 2022\n",
      "750 Junfeng Guo, Ang Li, Cong Liu:\n",
      "AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis. ICLR 2022\n",
      "751 Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren:\n",
      "Backdoor Defense via Decoupling the Training Process. ICLR 2022\n",
      "752 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios. ICLR 2022\n",
      "753 Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, Ruoxi Jia:\n",
      "Adversarial Unlearning of Backdoors via Implicit Hypergradient. ICLR 2022\n",
      "754 Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun:\n",
      "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data. ICLR 2022\n",
      "755 Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang:\n",
      "Unlabeled Backdoor Poisoning in Semi-Supervised Learning. ICME 2022: 1-6\n",
      "756 Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An, Shiqing Ma, Xiangyu Zhang:\n",
      "Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense. ICML 2022: 19879-19892\n",
      "757 Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael W. Mahoney, Prateek Mittal, Kannan Ramchandran, Joseph Gonzalez:\n",
      "Neurotoxin: Durable Backdoors in Federated Learning. ICML 2022: 26429-26446\n",
      "758 Jiarui Cao, Liehuang Zhu:\n",
      "A highly efficient, confidential, and continuous federated learning backdoor attack strategy. ICMLC 2022: 18-27\n",
      "759 Bilal Hussain Abbasi, Qi Zhong, Leo Yu Zhang, Shang Gao, Antonio Robles-Kelly, Robin Doss:\n",
      "A Generic Enhancer for Backdoor Attacks on Deep Neural Networks. ICONIP (7) 2022: 296-307\n",
      "760 Zhibin Zheng, Zhongyun Hua, Leo Yu Zhang:\n",
      "Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers. ICONIP (3) 2022: 216-227\n",
      "761 Junning Ze, Xinfeng Li, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu:\n",
      "UltraBD: Backdoor Attack against Automatic Speaker Verification Systems via Adversarial Ultrasound. ICPADS 2022: 193-200\n",
      "762 Giovanbattista Abbate, Irene Amerini, Roberto Caldelli:\n",
      "Image Watermarking Backdoor Attacks in CNN-Based Classification Tasks. ICPR Workshops (4) 2022: 3-16\n",
      "763 Peng Liu, Shuyi Zhang, Chuanjian Yao, Wenzhe Ye, Xianxian Li:\n",
      "Backdoor Attacks against Deep Neural Networks by Personalized Audio Steganography. ICPR 2022: 68-74\n",
      "764 Goutham Ramakrishnan, Aws Albarghouthi:\n",
      "Backdoors in Neural Models of Source Code. ICPR 2022: 2892-2899\n",
      "765 Mingyuan Ma, Hu Li, Xiaohui Kuang:\n",
      "Detecting Backdoor Attacks on Deep Neural Networks Based on Model Parameters Analysis. ICTAI 2022: 630-637\n",
      "766 Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, Shilin Wang:\n",
      "PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning. IJCAI 2022: 680-686\n",
      "767 Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang:\n",
      "Membership Inference via Backdooring. IJCAI 2022: 3832-3838\n",
      "768 Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen:\n",
      "Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation. IJCAI 2022: 1481-1487\n",
      "769 Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li:\n",
      "Data-Efficient Backdoor Attacks. IJCAI 2022: 3992-3998\n",
      "770 Nan Zhong, Zhenxing Qian, Xinpeng Zhang:\n",
      "Imperceptible Backdoor Attack: From Input Space to Feature Representation. IJCAI 2022: 1736-1742\n",
      "771 Adrian Kristanto, Shuo Wang, Carsten Rudolph:\n",
      "Latent Space-Based Backdoor Attacks Against Deep Neural Networks. IJCNN 2022: 1-10\n",
      "772 Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Yangyang Ding, Jianming Lv:\n",
      "ACTSS: Input Detection Defense against Backdoor Attacks via Activation Subset Scanning. IJCNN 2022: 1-8\n",
      "773 Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma:\n",
      "Backdoor Defense with Machine Unlearning. INFOCOM 2022: 280-289\n",
      "774 Rui Ning, Chunsheng Xin, Hongyi Wu:\n",
      "TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers. INFOCOM 2022: 1429-1438\n",
      "775 Haodong Zhao, Wei Du, Junjie Guo, Gongshen Liu:\n",
      "A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network. INTERSPEECH 2022: 4770-4774\n",
      "776 Yuang Zhou, Yichen Lei, Limin Yu, Xianyao Li, Dingding Chen, Tongpo Zhang:\n",
      "An Improved Method for Making CNN Immune to Backdoor Attack by Activating Clustering. ISCSIC 2022: 1-6\n",
      "777 Yuxiao Luo, Jianwei Tai, Xiaoqi Jia, Shengzhi Zhang:\n",
      "Practical Backdoor Attack Against Speaker Recognition System. ISPEC 2022: 468-484\n",
      "778 Xiangyu Gao, Meikang Qiu:\n",
      "Energy-Based Learning for Preventing Backdoor Attack. KSEM (3) 2022: 706-721\n",
      "779 Ruinan Jin, Xiaoxiao Li:\n",
      "Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis. SASHIMI@MICCAI 2022: 154-165\n",
      "780 Yue Gao, Jack W. Stokes, Manoj Ajith Prasad, Andrew T. Marshall, Kassem Fawaz, Emre Kiciman:\n",
      "I Know Your Triggers: Defending Against Textual Backdoor Attacks with Benign Backdoor Augmentation. MILCOM 2022: 442-449\n",
      "781 Jinwen Xin, Xixiang Lyu, Jing Ma:\n",
      "Natural Backdoor Attacks on Speech Recognition Models. ML4CS (1) 2022: 597-610\n",
      "782 Qiang Liu, Tongqing Zhou, Zhiping Cai, Yonghao Tang:\n",
      "Opportunistic Backdoor Attacks: Exploring Human-imperceptible Vulnerabilities on Speech Recognition Systems. ACM Multimedia 2022: 2390-2398\n",
      "783 Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang:\n",
      "Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving. ACM Multimedia 2022: 2957-2968\n",
      "784 Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan He, Hai Jin:\n",
      "BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. ACM Multimedia 2022: 678-686\n",
      "785 Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng, Lichao Sun:\n",
      "Backdoor Attacks on Crowd Counting. ACM Multimedia 2022: 5351-5360\n",
      "786 Xiaoyu Zhang, Yulin Jin, Tao Wang, Jian Lou, Xiaofeng Chen:\n",
      "Purifier: Plug-and-play Backdoor Mitigation for Pre-trained Models Via Anomaly Activation Suppression. ACM Multimedia 2022: 4291-4299\n",
      "787 Cong Shi, Tianfang Zhang, Zhuohang Li, Huy Phan, Tianming Zhao, Yan Wang, Jian Liu, Bo Yuan, Yingying Chen:\n",
      "Audio-domain position-independent backdoor attack via unnoticeable triggers. MobiCom 2022: 583-595\n",
      "788 Jijia Yang, Jiangang Shu, Xiaohua Jia:\n",
      "Breaking Distributed Backdoor Defenses for Federated Learning in Non-IID Settings. MSN 2022: 347-354\n",
      "789 Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, Chun Fan:\n",
      "Triggerless Backdoor Attack for NLP Tasks with Clean Labels. NAACL-HLT 2022: 2942-2952\n",
      "790 Agnideven Palanisamy Sundar, Feng Li, Xukai Zou, Tianchong Gao:\n",
      "Distributed Swift and Stealthy Backdoor Attack on Federated Learning. NAS 2022: 1-8\n",
      "791 Xueluan Gong, Yanjiao Chen, Jianshuo Dong, Qian Wang:\n",
      "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks. NDSS 2022\n",
      "792 Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, Ahmad-Reza Sadeghi:\n",
      "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection. NDSS 2022\n",
      "793 Sanghyun Hong, Nicholas Carlini, Alexey Kurakin:\n",
      "Handcrafted Backdoors in Deep Neural Networks. NeurIPS 2022\n",
      "794 Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Jerry Zhu:\n",
      "Provable Defense against Backdoor Policies in Reinforcement Learning. NeurIPS 2022\n",
      "795 Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan:\n",
      "BadPrompt: Backdoor Attacks on Continuous Prompts. NeurIPS 2022\n",
      "796 Ruisi Cai, Zhenyu Zhang, Tianlong Chen, Xiaohan Chen, Zhangyang Wang:\n",
      "Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets. NeurIPS 2022\n",
      "797 Shuwen Chai, Jinghui Chen:\n",
      "One-shot Neural Backdoor Erasing via Adversarial Weight Masking. NeurIPS 2022\n",
      "798 Weixin Chen, Baoyuan Wu, Haoqian Wang:\n",
      "Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples. NeurIPS 2022\n",
      "799 Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun:\n",
      "A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. NeurIPS 2022\n",
      "800 Khoa D. Doan, Yingjie Lao, Ping Li:\n",
      "Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class. NeurIPS 2022\n",
      "801 Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li:\n",
      "Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. NeurIPS 2022\n",
      "802 Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, Tom Goldstein:\n",
      "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. NeurIPS 2022\n",
      "803 Zhenting Wang, Hailun Ding, Juan Zhai, Shiqing Ma:\n",
      "Training with More Confidence: Mitigating Injected and Natural Backdoors During Training. NeurIPS 2022\n",
      "804 Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang:\n",
      "Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork. NeurIPS 2022\n",
      "805 Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Heather Zheng, Ben Y. Zhao:\n",
      "Finding Naturally Occurring Physical Backdoors in Image Datasets. NeurIPS 2022\n",
      "806 Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen:\n",
      "BackdoorBench: A Comprehensive Benchmark of Backdoor Learning. NeurIPS 2022\n",
      "807 Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu:\n",
      "Pre-activation Distributions Expose Backdoor Neurons. NeurIPS 2022\n",
      "808 Biru Zhu, Yujia Qin, Ganqu Cui, Yangyi Chen, Weilin Zhao, Chong Fu, Yangdong Deng, Zhiyuan Liu, Jingang Wang, Wei Wu, Maosong Sun, Ming Gu:\n",
      "Moderate-fitting as a Natural Backdoor Defender for Pre-trained Language Models. NeurIPS 2022\n",
      "809 Tobias Hemmert, Alexander May, Johannes Mittmann, Carl Richard Theodor Schneider:\n",
      "How to Backdoor (Classic) McEliece and How to Guard Against Backdoors. PQCrypto 2022: 24-44\n",
      "810 Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang:\n",
      "A Survey on Backdoor Attack and Defense in Natural Language Processing. QRS 2022: 809-820\n",
      "811 Shuiqiao Yang, Bao Gia Doan, Paul Montague, Olivier Y. de Vel, Tamas Abraham, Seyit Camtepe, Damith C. Ranasinghe, Salil S. Kanhere:\n",
      "Transferable Graph Backdoor Attack. RAID 2022: 321-332\n",
      "812 Nicholas Furth, Abdallah Khreishah, Guanxiong Liu, NhatHai Phan, Yaser Jararweh:\n",
      "Un-Fair Trojan: Targeted Backdoor Attacks Against Model Fairness. SDS 2022: 1-9\n",
      "813 Tian Liu, Xueyang Hu, Tao Shu:\n",
      "Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment in Mobile Edge Computing. SECON 2022: 416-424\n",
      "814 Yan Zhang, Yi Zhu, Zihao Liu, Chenglin Miao, Foad Hajiaghajani, Lu Su, Chunming Qiao:\n",
      "Towards Backdoor Attacks against LiDAR Object Detection in Autonomous Driving. SenSys 2022: 533-547\n",
      "815 Xiangyu Gao, Meikang Qiu:\n",
      "Energy-Based Learning for Polluted Outlier Detection in Backdoor. SmartCloud 2022: 47-52\n",
      "816 Chaohui Xu, Wenye Liu, Yue Zheng, Si Wang, Chip-Hong Chang:\n",
      "Inconspicuous Data Augmentation Based Backdoor Attack on Deep Neural Networks. SOCC 2022: 1-6\n",
      "817 Tao Wang, Xiaoyu Zhang, Yulin Jin, Chenyang Chen, Fei Zhu:\n",
      "Patch-Based Backdoors Detection and Mitigation with Feature Masking. SocialSec 2022: 229-246\n",
      "818 Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong:\n",
      "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. SP 2022: 2043-2059\n",
      "819 Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, Xiangyu Zhang:\n",
      "Piccolo: Exposing Complex Backdoors in NLP Transformer Models. SP 2022: 2025-2042\n",
      "820 Anubhab Baksi, Arghya Bhattacharjee, Jakub Breier, Takanori Isobe, Mridul Nandi:\n",
      "Big Brother Is Watching You: A Closer Look at Backdoor Construction. SPACE 2022: 81-96\n",
      "821 Hui Zeng, Tongqing Zhou, Xinyi Wu, Zhiping Cai:\n",
      "Never Too Late: Tracing and Mitigating Backdoor Attacks in Federated Learning. SRDS 2022: 69-81\n",
      "822 Luyao Chen, Na Yan, Boyang Zhang, Zhaoyang Wang, Yu Wen, Yanfei Hu:\n",
      "A General Backdoor Attack to Graph Neural Networks Based on Explanation Method. TrustCom 2022: 759-768\n",
      "823 Wanjia Zheng, Kazumasa Omote:\n",
      "Clean-label Backdoor Attack on Machine Learning-based Malware Detection Models and Countermeasures. TrustCom 2022: 1235-1242\n",
      "824 Peihao Li, Jie Huang, Shuaishuai Zhang, Chunyang Qi, Chuang Liang, Yang Peng:\n",
      "A Novel Backdoor Attack Adapted to Transfer Learning. SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta 2022: 1730-1735\n",
      "825 Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Möllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider:\n",
      "FLAME: Taming Backdoors in Federated Learning. USENIX Security Symposium 2022: 1415-1432\n",
      "826 Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, Min Yang:\n",
      "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation. USENIX Security Symposium 2022: 3611-3628\n",
      "827 Xiugui Yang, Xiangyun Qian, Rui Zhang, Ning Huang, Hui Xia:\n",
      "Low-Poisoning Rate Invisible Backdoor Attack Based on Important Neurons. WASA (2) 2022: 375-383\n",
      "828 Hyunsik Na, Daeseon Choi:\n",
      "Extracting a Minimal Trigger for an Efficient Backdoor Poisoning Attack Using the Activation Values of a Deep Neural Network. WDC@AsiaCCS 2022: 3-6\n",
      "829 Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek:\n",
      "Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers. WiseML@WiSec 2022: 57-62\n",
      "830 Shaofeng Li, Shiqing Ma, Minhui Xue, Benjamin Zi Hao Zhao:\n",
      "Deep Learning Backdoors. Security and Artificial Intelligence 2022: 313-334\n",
      "831 Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "Compression-Resistant Backdoor Attack against Deep Neural Networks. CoRR abs/2201.00672 (2022)\n",
      "832 Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, Ahmad-Reza Sadeghi:\n",
      "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection. CoRR abs/2201.00763 (2022)\n",
      "833 Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi:\n",
      "Rethink Stealthy Backdoor Attacks in Natural Language Processing. CoRR abs/2201.02993 (2022)\n",
      "834 Yongkang Wang, Dihua Zhai, Yufeng Zhan, Yuanqing Xia:\n",
      "RFLBAT: A Robust Federated Learning Algorithm against Backdoor Attack. CoRR abs/2201.03772 (2022)\n",
      "835 Liang Chen, Qibiao Peng, Jintang Li, Yang Liu, Jiawei Chen, Yong Li, Zibin Zheng:\n",
      "Neighboring Backdoor Attacks on Graph Convolutional Network. CoRR abs/2201.06202 (2022)\n",
      "836 Phung Lai, NhatHai Phan, Abdallah Khreishah, Issa Khalil, Xintao Wu:\n",
      "Model Transferring Attacks to Backdoor HyperNetwork in Personalized Federated Learning. CoRR abs/2201.07063 (2022)\n",
      "837 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios. CoRR abs/2201.08474 (2022)\n",
      "838 Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F. Al-Sarawi, Surya Nepal, Derek Abbott:\n",
      "Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. CoRR abs/2201.08619 (2022)\n",
      "839 Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma:\n",
      "Backdoor Defense with Machine Unlearning. CoRR abs/2201.09538 (2022)\n",
      "840 Siddhartha Datta, Nigel Shadbolt:\n",
      "Hiding Behind Backdoors: Self-Obfuscation Against Generative Models. CoRR abs/2201.09774 (2022)\n",
      "841 Siddhartha Datta, Nigel Shadbolt:\n",
      "Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire. CoRR abs/2201.12211 (2022)\n",
      "842 Marco Cesati:\n",
      "A new idea for RSA backdoors. CoRR abs/2201.13153 (2022)\n",
      "843 Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Jian Wang, Weiqiang Liu:\n",
      "Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks. CoRR abs/2201.13164 (2022)\n",
      "844 Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia:\n",
      "Few-Shot Backdoor Attacks on Visual Object Tracking. CoRR abs/2201.13178 (2022)\n",
      "845 Jing Xu, Rui Wang, Kaitai Liang, Stjepan Picek:\n",
      "More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. CoRR abs/2202.03195 (2022)\n",
      "846 Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren:\n",
      "Backdoor Defense via Decoupling the Training Process. CoRR abs/2202.03423 (2022)\n",
      "847 Junfeng Guo, Ang Li, Cong Liu:\n",
      "Backdoor Detection in Reinforcement Learning. CoRR abs/2202.03609 (2022)\n",
      "848 Yuxi Mi, Jihong Guan, Shuigeng Zhou:\n",
      "ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning. CoRR abs/2202.04311 (2022)\n",
      "849 Muhammad Umer, Robi Polikar:\n",
      "False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger. CoRR abs/2202.04479 (2022)\n",
      "850 Limin Yang, Zhi Chen, Jacopo Cortellazzi, Feargus Pendlebury, Kevin Tu, Fabio Pierazzi, Lorenzo Cavallaro, Gang Wang:\n",
      "Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers. CoRR abs/2202.05470 (2022)\n",
      "851 Bingxu Mu, Le Wang, Zhenxing Niu:\n",
      "Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples. CoRR abs/2202.06312 (2022)\n",
      "852 Jan Dreier, Sebastian Ordyniak, Stefan Szeider:\n",
      "SAT Backdoors: Depth Beats Size. CoRR abs/2202.08326 (2022)\n",
      "853 Reena Zelenkova, Jack Swallow, M. A. P. Chamikara, Dongxi Liu, Mohan Baruwal Chhetri, Seyit Camtepe, Marthie Grobler, Mahathir Almashor:\n",
      "Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches. CoRR abs/2202.10320 (2022)\n",
      "854 Shangxi Wu, Qiuyang He, Yi Zhang, Jitao Sang:\n",
      "Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias. CoRR abs/2202.10582 (2022)\n",
      "855 Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, Masashi Sugiyama:\n",
      "On the Effectiveness of Adversarial Training against Backdoor Attacks. CoRR abs/2202.10627 (2022)\n",
      "856 Yein Kim, Huili Chen, Farinaz Koushanfar:\n",
      "Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection. CoRR abs/2202.11196 (2022)\n",
      "857 Minlong Peng, Zidi Xiong, Mingming Sun, Ping Li:\n",
      "Label-Smoothed Backdoor Attack. CoRR abs/2202.11203 (2022)\n",
      "858 Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang:\n",
      "Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild. CoRR abs/2203.00858 (2022)\n",
      "859 Stefanos Koffas, Stjepan Picek, Mauro Conti:\n",
      "Dynamic Backdoors with Global Average Pooling. CoRR abs/2203.02079 (2022)\n",
      "860 Siddhartha Datta, Nigel Shadbolt:\n",
      "Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks. CoRR abs/2203.03692 (2022)\n",
      "861 Gorka Abad, Servio Paguada, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta:\n",
      "Client-Wise Targeted Backdoor in Federated Learning. CoRR abs/2203.08689 (2022)\n",
      "862 Yue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, Saif Eddin Jabari:\n",
      "PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks. CoRR abs/2203.09289 (2022)\n",
      "863 Arezoo Rajabi, Bhaskar Ramasubramanian, Radha Poovendran:\n",
      "Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning. CoRR abs/2203.15506 (2022)\n",
      "864 Xinqiao Zhang, Huili Chen, Ke Huang, Farinaz Koushanfar:\n",
      "An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks. CoRR abs/2204.04329 (2022)\n",
      "865 Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia:\n",
      "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information. CoRR abs/2204.05255 (2022)\n",
      "866 Shaik Mohammed Maqsood, Viveros Manuela Ceron, Addluri GowthamKrishna:\n",
      "Backdoor Attack against NLP models with Robustness-Aware Perturbation defense. CoRR abs/2204.05758 (2022)\n",
      "867 Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao:\n",
      "Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures. CoRR abs/2204.06273 (2022)\n",
      "868 Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, Or Zamir:\n",
      "Planting Undetectable Backdoors in Machine Learning Models. CoRR abs/2204.06974 (2022)\n",
      "869 Maximilian Noppel, Lukas Peter, Christian Wressnegger:\n",
      "Backdooring Explainable Machine Learning. CoRR abs/2204.09498 (2022)\n",
      "870 Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen:\n",
      "Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation. CoRR abs/2204.09975 (2022)\n",
      "871 Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li:\n",
      "Data-Efficient Backdoor Attacks. CoRR abs/2204.12281 (2022)\n",
      "872 Lukas Schulth, Christian Berghoff, Matthias Neu:\n",
      "Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering. CoRR abs/2204.12848 (2022)\n",
      "873 KiYoon Yoo, Nojun Kwak:\n",
      "Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling. CoRR abs/2204.14017 (2022)\n",
      "874 Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, Xudong Feng:\n",
      "A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning. CoRR abs/2205.02589 (2022)\n",
      "875 Nan Zhong, Zhenxing Qian, Xinpeng Zhang:\n",
      "Imperceptible Backdoor Attack: From Input Space to Feature Representation. CoRR abs/2205.03190 (2022)\n",
      "876 Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen:\n",
      "Model-Contrastive Learning for Backdoor Defense. CoRR abs/2205.04411 (2022)\n",
      "877 Hang Wang, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Universal Post-Training Backdoor Detection. CoRR abs/2205.06900 (2022)\n",
      "878 Long H. Pham, Jun Sun:\n",
      "Verifying Neural Networks Against Backdoor Attacks. CoRR abs/2205.06992 (2022)\n",
      "879 Zhixin Pan, Prabhat Mishra:\n",
      "Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution. CoRR abs/2205.09167 (2022)\n",
      "880 Shuaiqi Wang, Jonathan Hayase, Giulia Fanti, Sewoong Oh:\n",
      "Towards a Defense against Backdoor Attacks in Continual Federated Learning. CoRR abs/2205.11736 (2022)\n",
      "881 Lesheng Jin, Zihan Wang, Jingbo Shang:\n",
      "WeDef: Weakly Supervised Backdoor Defense for Text Classification. CoRR abs/2205.11803 (2022)\n",
      "882 Jun Yan, Vansh Gupta, Xiang Ren:\n",
      "Textual Backdoor Attacks with Iterative Trigger Injection. CoRR abs/2205.12700 (2022)\n",
      "883 Manaar Alam, Esha Sarkar, Michail Maniatakos:\n",
      "PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations. CoRR abs/2205.13523 (2022)\n",
      "884 Xiangyu Qi, Tinghao Xie, Saeed Mahloujifar, Prateek Mittal:\n",
      "Circumventing Backdoor Defenses That Are Based on Latent Separability. CoRR abs/2205.13613 (2022)\n",
      "885 Xiangyu Qi, Tinghao Xie, Saeed Mahloujifar, Prateek Mittal:\n",
      "Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations. CoRR abs/2205.13616 (2022)\n",
      "886 Sangeet Sagar, Abhinav Bhatt, Abhijith Srinivas Bidaralli:\n",
      "Defending Against Stealthy Backdoor Attacks. CoRR abs/2205.14246 (2022)\n",
      "887 Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, Jun Zhou:\n",
      "BadDet: Backdoor Attacks on Object Detection. CoRR abs/2205.14497 (2022)\n",
      "888 Shang Wang, Yansong Gao, Anmin Fu, Zhi Zhang, Yuqing Zhang, Willy Susilo:\n",
      "CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences. CoRR abs/2206.00145 (2022)\n",
      "889 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. CoRR abs/2206.01102 (2022)\n",
      "890 Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu:\n",
      "Kallima: A Clean-label Framework for Textual Backdoor Attacks. CoRR abs/2206.01832 (2022)\n",
      "891 Glenn Dawson, Muhammad Umer, Robi Polikar:\n",
      "Contributor-Aware Defenses Against Adversarial Backdoor Attacks. CoRR abs/2206.03583 (2022)\n",
      "892 Huiying Li, Arjun Nitin Bhagoji, Ben Y. Zhao, Haitao Zheng:\n",
      "Can Backdoor Attacks Survive Time-Varying Models? CoRR abs/2206.04677 (2022)\n",
      "893 Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang:\n",
      "Membership Inference via Backdooring. CoRR abs/2206.04823 (2022)\n",
      "894 Nan Luo, Yuanzhang Li, Yajie Wang, Shangbo Wu, Yu-An Tan, Quanxin Zhang:\n",
      "Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers. CoRR abs/2206.04881 (2022)\n",
      "895 Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert D. Mullins, Nicolas Papernot:\n",
      "Architectural Backdoors in Neural Networks. CoRR abs/2206.07840 (2022)\n",
      "896 Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash:\n",
      "Backdoor Attacks on Vision Transformers. CoRR abs/2206.08477 (2022)\n",
      "897 Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun:\n",
      "A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. CoRR abs/2206.08514 (2022)\n",
      "898 Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang:\n",
      "DECK: Model Hardening for Defending Pervasive Backdoors. CoRR abs/2206.09272 (2022)\n",
      "899 Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael W. Mahoney, Joseph E. Gonzalez, Kannan Ramchandran, Prateek Mittal:\n",
      "Neurotoxin: Durable Backdoors in Federated Learning. CoRR abs/2206.10341 (2022)\n",
      "900 Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, Ben Y. Zhao:\n",
      "Natural Backdoor Datasets. CoRR abs/2206.10673 (2022)\n",
      "901 Khoa D. Doan, Yingjie Lao, Peng Yang, Ping Li:\n",
      "Defending Backdoor Attacks on Vision Transformer via Patch Processing. CoRR abs/2206.12381 (2022)\n",
      "902 Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen, Hongyuan Zha:\n",
      "BackdoorBench: A Comprehensive Benchmark of Backdoor Learning. CoRR abs/2206.12654 (2022)\n",
      "903 Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan He, Hai Jin:\n",
      "BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. CoRR abs/2207.00278 (2022)\n",
      "904 Shuiqiao Yang, Bao Gia Doan, Paul Montague, Olivier Y. de Vel, Tamas Abraham, Seyit Camtepe, Damith C. Ranasinghe, Salil S. Kanhere:\n",
      "Transferable Graph Backdoor Attack. CoRR abs/2207.00425 (2022)\n",
      "905 Ruinan Jin, Xiaoxiao Li:\n",
      "Backdoor Attack is A Devil in Federated GAN-based Medical Image Synthesis. CoRR abs/2207.00762 (2022)\n",
      "906 Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen:\n",
      "Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain. CoRR abs/2207.04209 (2022)\n",
      "907 Shuwen Chai, Jinghui Chen:\n",
      "One-shot Neural Backdoor Erasing via Adversarial Weight Masking. CoRR abs/2207.04497 (2022)\n",
      "908 Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng, Lichao Sun:\n",
      "Backdoor Attacks on Crowd Counting. CoRR abs/2207.05641 (2022)\n",
      "909 Tong Wu, Tianhao Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal:\n",
      "Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. CoRR abs/2207.10825 (2022)\n",
      "910 Tian Liu, Xueyang Hu, Tao Shu:\n",
      "Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment. CoRR abs/2207.12327 (2022)\n",
      "911 Hui Xia, Xiugui Yang, Xiangyun Qian, Rui Zhang:\n",
      "FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair. CoRR abs/2207.12863 (2022)\n",
      "912 Guang Hua, Andrew Beng Jin Teoh:\n",
      "Backdoor Watermarking Deep Learning Classification Models With Deep Fidelity. CoRR abs/2208.00563 (2022)\n",
      "913 Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu:\n",
      "Data-free Backdoor Removal based on Channel Lipschitzness. CoRR abs/2208.03111 (2022)\n",
      "914 Yifan Wang, Wei Fan, Keke Yang, Naji Alhusaini, Jing Li:\n",
      "A Knowledge Distillation-Based Backdoor Attack in Federated Learning. CoRR abs/2208.06176 (2022)\n",
      "915 Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu, Wenzhong Guo:\n",
      "Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons. CoRR abs/2208.06537 (2022)\n",
      "916 Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang:\n",
      "Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer. CoRR abs/2208.06592 (2022)\n",
      "917 Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, Jinyin Chen:\n",
      "Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection. CoRR abs/2208.06776 (2022)\n",
      "918 Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia:\n",
      "Imperceptible and Robust Backdoor Attack in 3D Point Cloud. CoRR abs/2208.08052 (2022)\n",
      "919 Yulong Wang, Minghui Zhao, Shenghong Li, Xin Yuan, Wei Ni:\n",
      "Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models. CoRR abs/2208.09336 (2022)\n",
      "920 Alexander Unnervik, Sébastien Marcel:\n",
      "An anomaly detection approach for backdoored neural networks: face recognition as a case study. CoRR abs/2208.10231 (2022)\n",
      "921 Huy Phan, Cong Shi, Yi Xie, Tianfang Zhang, Zhuohang Li, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen, Bo Yuan:\n",
      "RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN. CoRR abs/2208.10608 (2022)\n",
      "922 Fangqi Li, Shilin Wang, Yun Zhu:\n",
      "Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers. CoRR abs/2208.14127 (2022)\n",
      "923 Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Surya Nepal, Derek Abbott:\n",
      "MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World. CoRR abs/2209.02339 (2022)\n",
      "924 Bingchen Jiang, Zhao Li:\n",
      "Defending Against Backdoor Attack on Graph Nerual Network by Explainability. CoRR abs/2209.02902 (2022)\n",
      "925 Yuhang Wang, Huafeng Shi, Rui Min, Ruijia Wu, Siyuan Liang, Yichao Wu, Ding Liang, Aishan Liu:\n",
      "Adaptive Perturbation Generation for Multiple Backdoors Detection. CoRR abs/2209.05244 (2022)\n",
      "926 Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Shu-Tao Xia:\n",
      "Black-box Ownership Verification for Dataset Protection via Backdoor Watermarking. CoRR abs/2209.06015 (2022)\n",
      "927 Mingrui He, Tianyu Chen, Haoyi Zhou, Shanghang Zhang, Jianxin Li:\n",
      "BadRes: Reveal the Backdoors through Residual Connection. CoRR abs/2209.07125 (2022)\n",
      "928 Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang:\n",
      "The \"Beatrix\" Resurrections: Robust Backdoor Detection via Gram Matrices. CoRR abs/2209.11715 (2022)\n",
      "929 Joseph Rance, Yiren Zhao, Ilia Shumailov, Robert D. Mullins:\n",
      "Augmentation Backdoors. CoRR abs/2209.15139 (2022)\n",
      "930 Tim Clifford, Ilia Shumailov, Yiren Zhao, Ross J. Anderson, Robert D. Mullins:\n",
      "ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. CoRR abs/2210.00108 (2022)\n",
      "931 Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li:\n",
      "Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. CoRR abs/2210.00875 (2022)\n",
      "932 Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang:\n",
      "Backdoor Attacks in the Supply Chain of Masked Image Modeling. CoRR abs/2210.01632 (2022)\n",
      "933 Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople:\n",
      "Invariant Aggregator for Defending Federated Backdoor Attacks. CoRR abs/2210.01834 (2022)\n",
      "934 Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Arunesh Sinha, Bowen Xu, Xinwen Hou, Guoliang Fan, David Lo:\n",
      "Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets. CoRR abs/2210.04688 (2022)\n",
      "935 Jonathan Hayase, Sewoong Oh:\n",
      "Few-shot Backdoor Attacks via Neural Tangent Kernels. CoRR abs/2210.05929 (2022)\n",
      "936 Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang:\n",
      "Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork. CoRR abs/2210.06428 (2022)\n",
      "937 Di Tang, Rui Zhu, XiaoFeng Wang, Haixu Tang, Yi Chen:\n",
      "Understanding Impacts of Task Similarity on Backdoor Attack and Detection. CoRR abs/2210.06509 (2022)\n",
      "938 Hadi M. Dolatabadi, Sarah M. Erfani, Christopher Leckie:\n",
      "COLLIDER: A Robust Training Framework for Backdoor Data. CoRR abs/2210.06704 (2022)\n",
      "939 Zhiyuan Zhang, Qi Su, Xu Sun:\n",
      "Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation. CoRR abs/2210.06894 (2022)\n",
      "940 Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, Cho-Jui Hsieh:\n",
      "Watermarking Pre-trained Language Models with Backdooring. CoRR abs/2210.07543 (2022)\n",
      "941 Phillip Rieger, Torsten Krauß, Markus Miettinen, Alexandra Dmitrienko, Ahmad-Reza Sadeghi:\n",
      "Close the Gate: Detecting Backdoored Models in Federated Learning based on Client-Side Deep Layer Output Analysis. CoRR abs/2210.07714 (2022)\n",
      "942 Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun:\n",
      "Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks. CoRR abs/2210.07907 (2022)\n",
      "943 Khoa D. Doan, Yingjie Lao, Ping Li:\n",
      "Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class. CoRR abs/2210.09194 (2022)\n",
      "944 Yuxin Wen, Jonas Geiping, Liam Fowl, Hossein Souri, Rama Chellappa, Micah Goldblum, Tom Goldstein:\n",
      "Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning. CoRR abs/2210.09305 (2022)\n",
      "945 Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun:\n",
      "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models. CoRR abs/2210.09545 (2022)\n",
      "946 Hang Wang, Sahar Karami, Ousmane Dia, H. Ritter, Ehsan Emamjomeh-Zadeh, Jiahui Chen, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Training set cleansing of backdoor poisoning by self-supervised representation learning. CoRR abs/2210.10272 (2022)\n",
      "947 Ruinan Jin, Xiaoxiao Li:\n",
      "Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. CoRR abs/2210.10886 (2022)\n",
      "948 Xiaoyi Chen, Baisong Xin, Shengfang Zhai, Shiqing Ma, Qingni Shen, Zhonghai Wu:\n",
      "Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning. CoRR abs/2210.11082 (2022)\n",
      "949 You Guo, Jun Wang, Trevor Cohn:\n",
      "Detecting Backdoors in Deep Text Classifiers. CoRR abs/2210.11264 (2022)\n",
      "950 Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang:\n",
      "The Dark Side of AutoML: Towards Architectural Backdoor Search. CoRR abs/2210.12179 (2022)\n",
      "951 Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang:\n",
      "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning. CoRR abs/2210.12873 (2022)\n",
      "952 Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, Guohan Huang:\n",
      "Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. CoRR abs/2210.13710 (2022)\n",
      "953 Virat Shejwalkar, Lingjuan Lyu, Amir Houmansadr:\n",
      "The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning. CoRR abs/2211.00453 (2022)\n",
      "954 Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "BATT: Backdoor Attack with Transformation-based Triggers. CoRR abs/2211.01806 (2022)\n",
      "955 Linshan Hou, Zhongyun Hua, Yuhong Li, Leo Yu Zhang:\n",
      "M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models. CoRR abs/2211.01875 (2022)\n",
      "956 Lukas Struppek, Dominik Hintersdorf, Kristian Kersting:\n",
      "Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models. CoRR abs/2211.02408 (2022)\n",
      "957 Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti:\n",
      "Going In Style: Audio Backdoors Through Stylistic Transformations. CoRR abs/2211.03117 (2022)\n",
      "958 Jianing Bai, Ren Wang, Zuyi Li:\n",
      "Physics-Constrained Backdoor Attacks on Power System Fault Localization. CoRR abs/2211.04445 (2022)\n",
      "959 Sheng Yang, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "Backdoor Defense via Suppressing Model Shortcuts. CoRR abs/2211.05631 (2022)\n",
      "960 Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia:\n",
      "Untargeted Backdoor Attack against Object Detection. CoRR abs/2211.05638 (2022)\n",
      "961 Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Zhihua Xia, Gerald Schaefer, Hui Fang:\n",
      "Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring. CoRR abs/2211.07138 (2022)\n",
      "962 Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey:\n",
      "Backdoor Attacks on Time Series: A Generative Approach. CoRR abs/2211.07915 (2022)\n",
      "963 Nikolaus Dräger, Yonghao Xu, Pedram Ghamisi:\n",
      "Backdoor Attacks for Remote Sensing Data with Wavelet Transform. CoRR abs/2211.08044 (2022)\n",
      "964 Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong:\n",
      "CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. CoRR abs/2211.08229 (2022)\n",
      "965 Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji:\n",
      "PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking. CoRR abs/2211.08697 (2022)\n",
      "966 Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Xiaojin Zhu:\n",
      "Provable Defense against Backdoor Policies in Reinforcement Learning. CoRR abs/2211.10530 (2022)\n",
      "967 Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao:\n",
      "Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification. CoRR abs/2211.10933 (2022)\n",
      "968 Shuo Chen, Yue Qiu, Jie Zhang:\n",
      "Backdoor Attacks on Multiagent Collaborative Systems. CoRR abs/2211.11455 (2022)\n",
      "969 Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang:\n",
      "A Survey on Backdoor Attack and Defense in Natural Language Processing. CoRR abs/2211.11958 (2022)\n",
      "970 Lu Pang, Tao Sun, Haibin Ling, Chao Chen:\n",
      "Backdoor Cleansing with Unlabeled Data. CoRR abs/2211.12044 (2022)\n",
      "971 Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan:\n",
      "BadPrompt: Backdoor Attacks on Continuous Prompts. CoRR abs/2211.14719 (2022)\n",
      "972 Guanhong Tao, Zhenting Wang, Siyuan Cheng, Shiqing Ma, Shengwei An, Yingqi Liu, Guangyu Shen, Zhuo Zhang, Yunshu Mao, Xiangyu Zhang:\n",
      "Backdoor Vulnerabilities in Normally Trained Deep Learning Models. CoRR abs/2211.15929 (2022)\n",
      "973 Linkun Fan, Fazhi He, Qing Guo, Wei Tang, Xiaolin Hong, Bing Li:\n",
      "Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape. CoRR abs/2211.16192 (2022)\n",
      "974 Marissa Connor, Vincent Emanuele:\n",
      "Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning. CoRR abs/2212.02582 (2022)\n",
      "975 Rui Zhu, Di Tang, Siyuan Tang, XiaoFeng Wang, Haixu Tang:\n",
      "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models. CoRR abs/2212.04687 (2022)\n",
      "976 Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho:\n",
      "How to Backdoor Diffusion Models? CoRR abs/2212.05400 (2022)\n",
      "977 Akshay Dhonthi, Ernst Moritz Hahn, Vahid Hashemi:\n",
      "Backdoor Mitigation in Deep Neural Networks via Strategic Retraining. CoRR abs/2212.07278 (2022)\n",
      "978 Khondoker Murad Hossain, Tim Oates:\n",
      "Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks. CoRR abs/2212.08121 (2022)\n",
      "979 Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, Yang Zhang:\n",
      "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks. CoRR abs/2212.09067 (2022)\n",
      "980 Tianrui Qin, Xianghuan He, Xitong Gao, Yiren Zhao, Kejiang Ye, Cheng-Zhong Xu:\n",
      "Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation. CoRR abs/2212.09979 (2022)\n",
      "981 Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji:\n",
      "VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion. CoRR abs/2212.10103 (2022)\n",
      "982 Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener:\n",
      "Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks. CoRR abs/2212.11205 (2022)\n",
      "983 Tian Dong, Ziyuan Zhang, Han Qiu, Tianwei Zhang, Hewu Li, Terry Wang:\n",
      "Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing. CoRR abs/2212.11751 (2022)\n",
      "984 Jianyi Zhang, Fangjiao Zhang, Qichao Jin, Zhiqiang Wang, Xiaodong Lin, Xiali Hei:\n",
      "XMAM: X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning. CoRR abs/2212.13675 (2022)\n",
      "985 Anubhab Baksi, Arghya Bhattacharjee, Jakub Breier, Takanori Isobe, Mridul Nandi:\n",
      "Big Brother Is Watching You: A Closer Look At Backdoor Construction. IACR Cryptol. ePrint Arch. 2022: 953 (2022)\n",
      "986 Tobias Hemmert:\n",
      "How to backdoor LWE-like cryptosystems. IACR Cryptol. ePrint Arch. 2022: 1381 (2022)\n",
      "987 Alexander May, Carl Richard Theodor Schneider:\n",
      "How to Backdoor (Classical) McEliece and How to Guard Against Backdoors. IACR Cryptol. ePrint Arch. 2022: 362 (2022)\n",
      "988 Prasanna Ravi, Shivam Bhasin, Anupam Chattopadhyay, Aikata, Sujoy Sinha Roy:\n",
      "Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs. IACR Cryptol. ePrint Arch. 2022: 1681 (2022)\n",
      "989 Xuan Chen, Yuena Ma, Shiwei Lu:\n",
      "Use Procedural Noise to Achieve Backdoor Attack. IEEE Access 9: 127204-127216 (2021)\n",
      "990 William Aiken, Hyoungshick Kim, Simon S. Woo, Jungwoo Ryoo:\n",
      "Neural network laundering: Removing black-box backdoor watermarks from deep neural networks. Comput. Secur. 106: 102277 (2021)\n",
      "991 Kun Shao, Junan Yang, Yang Ai, Hui Liu, Yu Zhang:\n",
      "BDDR: An Effective Defense Against Textual Backdoor Attacks. Comput. Secur. 110: 102433 (2021)\n",
      "992 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing. Comput. Secur. 106: 102280 (2021)\n",
      "993 Soheil Hashemi, Mani Zarei:\n",
      "Internet of Things backdoors: Resource management issues, security challenges, and detection methods. Trans. Emerg. Telecommun. Technol. 32(2) (2021)\n",
      "994 Sara Kaviani, Insoo Sohn:\n",
      "Study of scale-free structures in feed-forward neural networks against backdoor attacks. ICT Express 7(2): 265-268 (2021)\n",
      "995 Chuanshuai Chen, Jiazhu Dai:\n",
      "Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification. Neurocomputing 452: 253-262 (2021)\n",
      "996 Xueluan Gong, Yanjiao Chen, Qian Wang, Huayang Huang, Lingshuo Meng, Chao Shen, Qian Zhang:\n",
      "Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment. IEEE J. Sel. Areas Commun. 39(8): 2617-2631 (2021)\n",
      "997 Zhen Xiang, David J. Miller, Hang Wang, George Kesidis:\n",
      "Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set. Neural Comput. 33(5): 1329-1371 (2021)\n",
      "998 Yi Zhao, Ke Xu, Haiyang Wang, Bo Li, Ruoxi Jia:\n",
      "Stability-Based Analysis and Defense against Backdoor Attacks on Edge Computing Services. IEEE Netw. 35(1): 163-169 (2021)\n",
      "999 Lane A. Hemaspaandra, David E. Narváez:\n",
      "Existence versus exploitation: the opacity of backdoors and backbones. Prog. Artif. Intell. 10(3): 297-308 (2021)\n",
      "1000 Mingfu Xue, Can He, Jian Wang, Weiqiang Liu:\n",
      "Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems. Peer-to-Peer Netw. Appl. 14(3): 1458-1474 (2021)\n",
      "1001 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "A Master Key backdoor for universal impersonation attack against DNN-based face verification. Pattern Recognit. Lett. 144: 61-67 (2021)\n",
      "1002 Yong Fang, Mingyu Xie, Cheng Huang:\n",
      "PBDT: Python Backdoor Detection Model Based on Combined Features. Secur. Commun. Networks 2021: 9923234:1-9923234:13 (2021)\n",
      "1003 Kang Liu, Benjamin Tan, Ramesh Karri, Siddharth Garg:\n",
      "Training Data Poisoning in ML-CAD: Backdooring DL-Based Lithographic Hotspot Detectors. IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 40(6): 1244-1257 (2021)\n",
      "1004 Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri:\n",
      "Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors Against Backdooring Attacks. IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 40(10): 2077-2089 (2021)\n",
      "1005 Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang:\n",
      "Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization. IEEE Trans. Dependable Secur. Comput. 18(5): 2088-2105 (2021)\n",
      "1006 Ming Fan, Ziliang Si, Xiaofei Xie, Yang Liu, Ting Liu:\n",
      "Text Backdoor Detection Using an Interpretable RNN Abstract Model. IEEE Trans. Inf. Forensics Secur. 16: 4117-4132 (2021)\n",
      "1007 Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, Saif Eddin Jabari:\n",
      "Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems. IEEE Trans. Inf. Forensics Secur. 16: 4772-4787 (2021)\n",
      "1008 Zhicong Yan, Jun Wu, Gaolei Li, Shenghong Li, Mohsen Guizani:\n",
      "Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures. IEEE Trans. Inf. Forensics Secur. 16: 4827-4842 (2021)\n",
      "1009 Aleksandra V. Markelova:\n",
      "Embedding asymmetric backdoors into the RSA key generator. J. Comput. Virol. Hacking Tech. 17(1): 37-46 (2021)\n",
      "1010 Petr Kucera, Petr Savický:\n",
      "Backdoor Decomposable Monotone Circuits and Propagation Complete Encodings. AAAI 2021: 3832-3840\n",
      "1011 Mustafa Safa Özdayi, Murat Kantarcioglu, Yulia R. Gel:\n",
      "Defending against Backdoors in Federated Learning with Robust Learning Rate. AAAI 2021: 9268-9276\n",
      "1012 Zhicong Yan, Gaolei Li, Yuan Tian, Jun Wu, Shenghong Li, Mingzhe Chen, H. Vincent Poor:\n",
      "DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation. AAAI 2021: 10585-10593\n",
      "1013 Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun:\n",
      "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger. ACL/IJCNLP (1) 2021: 443-453\n",
      "1014 Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun:\n",
      "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution. ACL/IJCNLP (1) 2021: 4873-4883\n",
      "1015 Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun:\n",
      "Rethinking Stealthiness of Backdoor Attack against NLP Models. ACL/IJCNLP (1) 2021: 5543-5557\n",
      "1016 Congcong Chen, Lifei Wei, Lei Zhang, Jianting Ning:\n",
      "MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks. ACM TUR-C 2021: 104-109\n",
      "1017 Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang:\n",
      "BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements. ACSAC 2021: 554-569\n",
      "1018 Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, Bhavani Thuraisingham:\n",
      "DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation. AsiaCCS 2021: 363-377\n",
      "1019 Yifan Guo, Qianlong Wang, Tianxi Ji, Xufei Wang, Pan Li:\n",
      "Resisting Distributed Backdoor Attacks in Federated Learning: A Dynamic Norm Clipping Approach. IEEE BigData 2021: 1172-1182\n",
      "1020 Guanxiong Liu, Issa Khalil, Abdallah Khreishah, NhatHai Phan:\n",
      "A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples. IEEE BigData 2021: 834-846\n",
      "1021 Nishit Patel, David Cancel, Moitrayee Chatterjee, Md Shahinoor Rahman:\n",
      "Covid-19 digital Contact-tracing: a doorway to well-being or a backdoor to security vulnerabilities? IEEE BigData 2021: 4297-4302\n",
      "1022 Shintaro Narisada, Yuki Matsumoto, Seira Hidano, Toshihiro Uchibayashi, Takuo Suganuma, Masahiro Hiji, Shinsaku Kiyomoto:\n",
      "Countermeasures Against Backdoor Attacks Towards Malware Detectors. CANS 2021: 295-314\n",
      "1023 Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu:\n",
      "Hidden Backdoors in Human-Centric Language Models. CCS 2021: 3123-3140\n",
      "1024 Tatsuya Oyama, Shunsuke Okura, Kota Yoshida, Takeshi Fujino:\n",
      "Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface. ASHES@CCS 2021: 63-72\n",
      "1025 Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, Ting Wang:\n",
      "Backdoor Pre-trained Models Can Transfer to All. CCS 2021: 3141-3158\n",
      "1026 Yue Wang, Michail Maniatakos, Saif Eddin Jabari:\n",
      "A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems. CDC 2021: 4394-4399\n",
      "1027 Chen Bian, Wei Jiang, Jinyu Zhan, Ziwei Song, Xiangyu Wen, Hong Lei:\n",
      "A physically realizable backdoor attack on 3D point cloud deep learning: work-in-progress. CODES+ISSS 2021: 27-28\n",
      "1028 Yu Sheng, Rong Chen, Guanyu Cai, Li Kuang:\n",
      "Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger. CollaborateCom (2) 2021: 276-296\n",
      "1029 Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak:\n",
      "Reasoning Short Cuts in Infinite Domain Constraint Satisfaction: Algorithms and Lower Bounds for Backdoors. CP 2021: 32:1-32:20\n",
      "1030 Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao:\n",
      "Backdoor Attacks Against Deep Learning Systems in the Physical World. CVPR 2021: 6206-6215\n",
      "1031 Fang-Qi Li, Shi-Lin Wang, Zhen-Hai Wang:\n",
      "Protecting Deep Cerebrospinal Fluid Cell Image Processing Models with Backdoor and Semi-Distillation. DICTA 2021: 1-7\n",
      "1032 Xinrui Liu, Xiao Yu, Zhibin Zhang, Quanxin Zhang, Yuanzhang Li, Yu-an Tan:\n",
      "A Random Multi-target Backdooring Attack on Deep Neural Networks. DMBD (2) 2021: 45-52\n",
      "1033 Ziqi Wei, Junjian Shi, Yihe Duan, Ranyang Liu, Ye Han, Zheli Liu:\n",
      "Backdoor Filter: Mitigating Visible Backdoor Triggers in Dataset. DTPI 2021: 102-105\n",
      "1034 Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang:\n",
      "BFClass: A Backdoor-free Text Classification Framework. EMNLP (Findings) 2021: 444-453\n",
      "1035 Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu:\n",
      "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. EMNLP (1) 2021: 3023-3032\n",
      "1036 Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun:\n",
      "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. EMNLP (1) 2021: 9558-9566\n",
      "1037 Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun:\n",
      "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. EMNLP (1) 2021: 4569-4580\n",
      "1038 Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun:\n",
      "RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models. EMNLP (1) 2021: 8365-8381\n",
      "1039 Xiangyu Wen, Wei Jiang, Jinyu Zhan, Chen Bian, Ziwei Song:\n",
      "Generative strategy based backdoor attacks to 3D point clouds: work-in-progress. EMSOFT 2021: 23-24\n",
      "1040 Iram Arshad, Mamoona Naveed Asghar, Yuansong Qiao, Brian Lee, Yuhang Ye:\n",
      "Pixdoor: A Pixel-space Backdoor Attack on Deep Learning Models. EUSIPCO 2021: 681-685\n",
      "1041 Shuang Li, Hongwei Li, Hanxiao Chen:\n",
      "Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack. GLOBECOM 2021: 1-6\n",
      "1042 Xingbo Hu, Yibing Lan, Ruimin Gao, Guozhu Meng, Kai Chen:\n",
      "Why is Your Trojan NOT Responding? A Quantitative Analysis of Failures in Backdoor Attacks of Neural Networks. ICA3PP (3) 2021: 754-771\n",
      "1043 Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, Arjun Gupta:\n",
      "Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff. ICASSP 2021: 3855-3859\n",
      "1044 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access to the Training Set. ICASSP 2021: 3745-3749\n",
      "1045 Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia:\n",
      "Backdoor Attack Against Speaker Verification. ICASSP 2021: 2560-2564\n",
      "1046 Khoa D. Doan, Yingjie Lao, Weijie Zhao, Ping Li:\n",
      "LIRA: Learnable, Imperceptible and Robust Backdoor Attacks. ICCV 2021: 11946-11956\n",
      "1047 Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu:\n",
      "Black-box Detection of Backdoor Attacks with Limited Information and Data. ICCV 2021: 16462-16471\n",
      "1048 Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou:\n",
      "PointBA: Towards Backdoor Attacks in 3D Point Cloud. ICCV 2021: 16472-16481\n",
      "1049 Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu:\n",
      "Invisible Backdoor Attack with Sample-Specific Triggers. ICCV 2021: 16443-16452\n",
      "1050 Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis:\n",
      "A Backdoor Attack against 3D Point Cloud Classifiers. ICCV 2021: 7577-7587\n",
      "1051 Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia:\n",
      "Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective. ICCV 2021: 16453-16461\n",
      "1052 Liuwan Zhu, Rui Ning, Chunsheng Xin, Chonggang Wang, Hongyi Wu:\n",
      "CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks. ICCV 2021: 16433-16442\n",
      "1053 Sébastien Andreina, Giorgia Azzurra Marson, Helen Möllering, Ghassan Karame:\n",
      "BaFFLe: Backdoor Detection via Feedback-based Federated Learning. ICDCS 2021: 852-863\n",
      "1054 Anthony Cheuk Tung Lai, Ken Wai Kin Wong, Johnny Tsz Wun Wong, Austin Tsz Wai Lau, Alan Po Lun Ho, Shuai Wang, Jogesh Muppala:\n",
      "Backdoor Investigation and Incident Response: From Zero to Profit. ICDF2C 2021: 229-247\n",
      "1055 Ankita Raj, Ambar Pal, Chetan Arora:\n",
      "Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks. ICIP 2021: 3023-3027\n",
      "1056 Yankun Ren, Longfei Li, Jun Zhou:\n",
      "Simtrojan: Stealthy Backdoor Attack. ICIP 2021: 819-823\n",
      "1057 Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma:\n",
      "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks. ICLR 2021\n",
      "1058 Tuan Anh Nguyen, Anh Tuan Tran:\n",
      "WaNet - Imperceptible Warping-based Backdoor Attack. ICLR 2021\n",
      "1059 Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh:\n",
      "Defense against backdoor attacks via robust covariance estimation. ICML 2021: 4129-4139\n",
      "1060 Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P. Dickerson, Tom Goldstein:\n",
      "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. ICML 2021: 9389-9398\n",
      "1061 Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, Xiangyu Zhang:\n",
      "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. ICML 2021: 9525-9536\n",
      "1062 Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li:\n",
      "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. ICML 2021: 11372-11382\n",
      "1063 Nandish Chattopadhyay, Anupam Chattopadhyay:\n",
      "ROWBACK: RObust Watermarking for neural networks using BACKdoors. ICMLA 2021: 1728-1735\n",
      "1064 Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu:\n",
      "DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection. ICSE 2021: 263-274\n",
      "1065 Masoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, Florian Kerschbaum:\n",
      "On the Robustness of Backdoor-based Watermarking in Deep Neural Networks. IH&MMSec 2021: 177-188\n",
      "1066 Chen Zhao, Yu Wen, Shuailou Li, Fucheng Liu, Dan Meng:\n",
      "FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning. IH&MMSec 2021: 51-62\n",
      "1067 Sebastian Ordyniak, André Schidler, Stefan Szeider:\n",
      "Backdoor DNFs. IJCAI 2021: 1403-1409\n",
      "1068 Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song:\n",
      "BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning. IJCAI 2021: 3699-3705\n",
      "1069 Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu:\n",
      "Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks. INFOCOM 2021: 1-10\n",
      "1070 Quan Zhang, Yifeng Ding, Yongqiang Tian, Jianmin Guo, Min Yuan, Yu Jiang:\n",
      "AdvDoor: adversarial backdoor attack of deep learning system. ISSTA 2021: 127-138\n",
      "1071 Xiaobo Yu, Weizhi Meng, Lei Zhao, Yining Liu:\n",
      "TridentShell: a Covert and Scalable Backdoor Injection Attack on Web Applications. ISC 2021: 177-194\n",
      "1072 Ching-Yuan Chen, Krishnendu Chakrabarty:\n",
      "On-line Functional Testing of Memristor-mapped Deep Neural Networks using Backdoored Checksums. ITC 2021: 83-92\n",
      "1073 Zahra Ashktorab, Casey Dugan, James Johnson, Aabhas Sharma, Dustin Ramsey Torres, Ingrid Lange, Benjamin Hoover, Heiko Ludwig, Bryant Chen, Nathalie Baracaldo, Werner Geyer, Qian Pan:\n",
      "The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game. IUI 2021: 423-433\n",
      "1074 Binhan Xi, Shaofeng Li, Jiachun Li, Hui Liu, Hong Liu, Haojin Zhu:\n",
      "BatFL: Backdoor Detection on Federated Learning in e-Health. IWQoS 2021: 1-10\n",
      "1075 Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik:\n",
      "What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors. KDD 2021: 1027-1035\n",
      "1076 Nikolas Mählmann, Sebastian Siebertz, Alexandre Vigny:\n",
      "Recursive Backdoors for SAT. MFCS 2021: 73:1-73:18\n",
      "1077 Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, Cong Wang:\n",
      "Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation. ACM Multimedia 2021: 826-834\n",
      "1078 Khoa D. Doan, Yingjie Lao, Ping Li:\n",
      "Backdoor Attack with Imperceptible Input and Latent Modification. NeurIPS 2021: 18944-18957\n",
      "1079 Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma:\n",
      "Anti-Backdoor Learning: Training Clean Models on Poisoned Data. NeurIPS 2021: 14900-14912\n",
      "1080 Naren Manoj, Avrim Blum:\n",
      "Excess Capacity and Backdoor Poisoning. NeurIPS 2021: 20373-20384\n",
      "1081 Dongxian Wu, Yisen Wang:\n",
      "Adversarial Neuron Pruning Purifies Backdoored Deep Models. NeurIPS 2021: 16913-16925\n",
      "1082 Enkli Ylli, Julian Fejzaj, Igli Tafa:\n",
      "Identifying and blocking the backdoors in Linux. RTA-CSIT 2021: 193-197\n",
      "1083 Zaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong:\n",
      "Backdoor Attacks to Graph Neural Networks. SACMAT 2021: 15-26\n",
      "1084 Fahri Anil Yerlikaya, Serif Bahtiyar:\n",
      "A Textual Clean-Label Backdoor Attack Strategy against Spam Detection. SIN 2021: 1-8\n",
      "1085 Xiyao Liu, Shuo Shao, Yue Yang, Kangming Wu, Wenyuan Yang, Hui Fang:\n",
      "Secure Federated Learning Model Verification: A Client-side Backdoor Triggered Watermarking Scheme. SMC 2021: 2414-2419\n",
      "1086 Aaron M. Ferber, Jialin Song, Bistra Dilkina, Yisong Yue:\n",
      "Learning Pseudo-Backdoors for Mixed Integer Programs. SOCS 2021: 170-172\n",
      "1087 Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu:\n",
      "Robust Backdoor Attacks against Deep Neural Networks in Real Physical World. TrustCom 2021: 620-626\n",
      "1088 Meirong Liu, Hong Zheng, Qin Liu, Xiaofei Xing, Yinglong Dai:\n",
      "A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks. UbiSec 2021: 1-12\n",
      "1089 Eugene Bagdasaryan, Vitaly Shmatikov:\n",
      "Blind Backdoors in Deep Learning Models. USENIX Security Symposium 2021: 1505-1521\n",
      "1090 Giorgio Severi, Jim Meyer, Scott E. Coull, Alina Oprea:\n",
      "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers. USENIX Security Symposium 2021: 1487-1504\n",
      "1091 Di Tang, XiaoFeng Wang, Haixu Tang, Kehuan Zhang:\n",
      "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection. USENIX Security Symposium 2021: 1541-1558\n",
      "1092 Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang:\n",
      "Graph Backdoor. USENIX Security Symposium 2021: 1523-1540\n",
      "1093 Morriel Kasher, Michael Zhao, Aryeh Greenberg, Devin Gulati, Silvija Kokalj-Filipovic, Predrag Spasojevic:\n",
      "Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper. WiseML@WiSec 2021: 37-42\n",
      "1094 Jing Xu, Minhui Xue, Stjepan Picek:\n",
      "Explainability-based Backdoor Attacks Against Graph Neural Networks. WiseML@WiSec 2021: 31-36\n",
      "1095 Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos, Farah Shamout:\n",
      "Explainability Matters: Backdoor Attacks on Medical Imaging. CoRR abs/2101.00008 (2021)\n",
      "1096 Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma:\n",
      "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks. CoRR abs/2101.05930 (2021)\n",
      "1097 Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu:\n",
      "DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection. CoRR abs/2101.06896 (2021)\n",
      "1098 Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun:\n",
      "Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks. CoRR abs/2101.06969 (2021)\n",
      "1099 Ximing Qiao, Yuhua Bai, Siping Hu, Ang Li, Yiran Chen, Hai Li:\n",
      "On Provable Backdoor Defense in Collaborative Learning. CoRR abs/2101.08177 (2021)\n",
      "1100 Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya:\n",
      "SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation. CoRR abs/2102.02402 (2021)\n",
      "1101 Nikolas Mählmann, Sebastian Siebertz, Alexandre Vigny:\n",
      "Recursive Backdoors for SAT. CoRR abs/2102.04707 (2021)\n",
      "1102 Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, Xiangyu Zhang:\n",
      "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. CoRR abs/2102.05123 (2021)\n",
      "1103 Tuan Anh Nguyen, Anh Tuan Tran:\n",
      "WaNet - Imperceptible Warping-based Backdoor Attack. CoRR abs/2102.10369 (2021)\n",
      "1104 Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, Tom Goldstein:\n",
      "What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors. CoRR abs/2102.13624 (2021)\n",
      "1105 Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein:\n",
      "DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations. CoRR abs/2103.02079 (2021)\n",
      "1106 Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia:\n",
      "Hidden Backdoor Attack against Semantic Segmentation Models. CoRR abs/2103.04038 (2021)\n",
      "1107 Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, Xiangyu Zhang:\n",
      "EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry. CoRR abs/2103.08820 (2021)\n",
      "1108 Todd Huster, Emmanuel Ekwedike:\n",
      "TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation. CoRR abs/2103.10274 (2021)\n",
      "1109 Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu:\n",
      "Black-box Detection of Backdoor Attacks with Limited Information and Data. CoRR abs/2103.13127 (2021)\n",
      "1110 Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou:\n",
      "PointBA: Towards Backdoor Attacks in 3D Point Cloud. CoRR abs/2103.16074 (2021)\n",
      "1111 Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong:\n",
      "RABA: A Robust Avatar Backdoor Attack on Deep Neural Network. CoRR abs/2104.01026 (2021)\n",
      "1112 Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia:\n",
      "Backdoor Attack in the Physical World. CoRR abs/2104.02361 (2021)\n",
      "1113 Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia:\n",
      "Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective. CoRR abs/2104.03413 (2021)\n",
      "1114 Jing Xu, Minhui Xue, Stjepan Picek:\n",
      "Explainability-based Backdoor Attacks Against Graph Neural Networks. CoRR abs/2104.03674 (2021)\n",
      "1115 Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis:\n",
      "A Backdoor Attack against 3D Point Cloud Classifiers. CoRR abs/2104.05808 (2021)\n",
      "1116 Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu:\n",
      "Robust Backdoor Attacks against Deep Neural Networks in Real Physical World. CoRR abs/2104.07395 (2021)\n",
      "1117 Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh:\n",
      "SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics. CoRR abs/2104.11315 (2021)\n",
      "1118 Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans:\n",
      "Stealthy Backdoors as Compression Artifacts. CoRR abs/2104.15129 (2021)\n",
      "1119 Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu:\n",
      "Hidden Backdoors in Human-Centric Language Models. CoRR abs/2105.00164 (2021)\n",
      "1120 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification. CoRR abs/2105.00249 (2021)\n",
      "1121 Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song:\n",
      "BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning. CoRR abs/2105.00579 (2021)\n",
      "1122 Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu:\n",
      "Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds. CoRR abs/2105.04839 (2021)\n",
      "1123 Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash:\n",
      "Backdoor Attacks on Self-Supervised Learning. CoRR abs/2105.10123 (2021)\n",
      "1124 Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun:\n",
      "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger. CoRR abs/2105.12400 (2021)\n",
      "1125 Mingfu Xue, Yinghao Wu, Zhiyu Wu, Jian Wang, Yushu Zhang, Weiqiang Liu:\n",
      "Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. CoRR abs/2105.14259 (2021)\n",
      "1126 Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, Tianwei Zhang:\n",
      "Defending against Backdoor Attacks in Natural Language Generation. CoRR abs/2106.01810 (2021)\n",
      "1127 Sanghyun Hong, Nicholas Carlini, Alexey Kurakin:\n",
      "Handcrafted Backdoors in Deep Neural Networks. CoRR abs/2106.04690 (2021)\n",
      "1128 Aaron M. Ferber, Jialin Song, Bistra Dilkina, Yisong Yue:\n",
      "Learning Pseudo-Backdoors for Mixed Integer Programs. CoRR abs/2106.05080 (2021)\n",
      "1129 Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun:\n",
      "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution. CoRR abs/2106.06361 (2021)\n",
      "1130 Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo:\n",
      "Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions. CoRR abs/2106.07214 (2021)\n",
      "1131 Byunggill Joe, Akshay Mehra, Insik Shin, Jihun Hamm:\n",
      "Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks. CoRR abs/2106.07925 (2021)\n",
      "1132 Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li:\n",
      "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. CoRR abs/2106.08283 (2021)\n",
      "1133 Hossein Souri, Micah Goldblum, Liam Fowl, Rama Chellappa, Tom Goldstein:\n",
      "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. CoRR abs/2106.08970 (2021)\n",
      "1134 Nicholas Carlini, Andreas Terzis:\n",
      "Poisoning and Backdooring Contrastive Learning. CoRR abs/2106.09667 (2021)\n",
      "1135 Xiangyu Qi, Jifeng Zhu, Chulin Xie, Yong Yang:\n",
      "Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. CoRR abs/2107.07240 (2021)\n",
      "1136 Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek:\n",
      "Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. CoRR abs/2107.14569 (2021)\n",
      "1137 Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong:\n",
      "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. CoRR abs/2108.00352 (2021)\n",
      "1138 Ambrish Rawat, Killian Levacher, Mathieu Sinn:\n",
      "The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks. CoRR abs/2108.01644 (2021)\n",
      "1139 Jie Zhang, Dongdong Chen, Jing Liao, Qidong Huang, Gang Hua, Weiming Zhang, Nenghai Yu:\n",
      "Poison Ink: Robust and Invisible Backdoor Attack. CoRR abs/2108.02488 (2021)\n",
      "1140 Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Derek Abbott:\n",
      "Quantization Backdoors to Deep Learning Models. CoRR abs/2108.09187 (2021)\n",
      "1141 Esha Sarkar, Michail Maniatakos:\n",
      "TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis. CoRR abs/2108.10132 (2021)\n",
      "1142 Tobias Lorenz, Marta Kwiatkowska, Mario Fritz:\n",
      "Backdoor Attacks on Network Certification via Data Poisoning. CoRR abs/2108.11299 (2021)\n",
      "1143 Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu:\n",
      "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. CoRR abs/2108.13888 (2021)\n",
      "1144 Naren Sarayu Manoj, Avrim Blum:\n",
      "Excess Capacity and Backdoor Poisoning. CoRR abs/2109.00685 (2021)\n",
      "1145 Guanxiong Liu, Issa Khalil, Abdallah Khreishah, NhatHai Phan:\n",
      "A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples. CoRR abs/2109.01275 (2021)\n",
      "1146 Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun:\n",
      "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data. CoRR abs/2109.01300 (2021)\n",
      "1147 Xi Li, George Kesidis, David J. Miller, Vladimir Lucic:\n",
      "Backdoor Attack and Defense for Deep Regression. CoRR abs/2109.02381 (2021)\n",
      "1148 Hasan Abed Al Kader Hammoud, Bernard Ghanem:\n",
      "Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. CoRR abs/2109.05507 (2021)\n",
      "1149 Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia:\n",
      "Clean-label Backdoor Attack against Deep Hashing based Retrieval. CoRR abs/2109.08868 (2021)\n",
      "1150 Zeyuan Yin, Ye Yuan, Panfeng Guo, Pan Zhou:\n",
      "Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis. CoRR abs/2109.10512 (2021)\n",
      "1151 Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang:\n",
      "BFClass: A Backdoor-free Text Classification Framework. CoRR abs/2109.10855 (2021)\n",
      "1152 Jakub Breier, Xiaolu Hou, Martín Ochoa, Jesus Solano:\n",
      "FooBaR: Fault Fooling Backdoor Attack on Neural Network Training. CoRR abs/2109.11249 (2021)\n",
      "1153 Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan:\n",
      "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models. CoRR abs/2110.02467 (2021)\n",
      "1154 Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia:\n",
      "Adversarial Unlearning of Backdoors via Implicit Hypergradient. CoRR abs/2110.03735 (2021)\n",
      "1155 Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Guodong Jiang, Yi Liu:\n",
      "Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction. CoRR abs/2110.03875 (2021)\n",
      "1156 Siddhartha Datta, Giulio Lovisotto, Ivan Martinovic, Nigel Shadbolt:\n",
      "Widen The Backdoor To Let More Attackers In. CoRR abs/2110.04571 (2021)\n",
      "1157 Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun:\n",
      "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. CoRR abs/2110.07139 (2021)\n",
      "1158 M. Caner Tol, Saad Islam, Berk Sunar, Ziming Zhang:\n",
      "An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware. CoRR abs/2110.07683 (2021)\n",
      "1159 Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun:\n",
      "RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models. CoRR abs/2110.07831 (2021)\n",
      "1160 Yangyi Chen, Fanchao Qi, Zhiyuan Liu, Maosong Sun:\n",
      "Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. CoRR abs/2110.08247 (2021)\n",
      "1161 Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina:\n",
      "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework. CoRR abs/2110.08423 (2021)\n",
      "1162 Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis:\n",
      "Detecting Backdoor Attacks Against Point Cloud Classifiers. CoRR abs/2110.10354 (2021)\n",
      "1163 Jing Xu, Stjepan Picek:\n",
      "Watermarking Graph Neural Networks based on Backdoor Attacks. CoRR abs/2110.11024 (2021)\n",
      "1164 Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma:\n",
      "Anti-Backdoor Learning: Training Clean Models on Poisoned Data. CoRR abs/2110.11571 (2021)\n",
      "1165 Dongxian Wu, Yisen Wang:\n",
      "Adversarial Neuron Pruning Purifies Backdoored Deep Models. CoRR abs/2110.14430 (2021)\n",
      "1166 Junfeng Guo, Ang Li, Cong Liu:\n",
      "AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis. CoRR abs/2110.14880 (2021)\n",
      "1167 Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, Ting Wang:\n",
      "Backdoor Pre-trained Models Can Transfer to All. CoRR abs/2111.00197 (2021)\n",
      "1168 Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li:\n",
      "A Statistical Difference Reduction Method for Escaping Backdoor Detection. CoRR abs/2111.05077 (2021)\n",
      "1169 Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, Chun Fan:\n",
      "Triggerless Backdoor Attack for NLP Tasks with Clean Labels. CoRR abs/2111.07970 (2021)\n",
      "1170 Wei Guo, Benedetta Tondi, Mauro Barni:\n",
      "An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences. CoRR abs/2111.08429 (2021)\n",
      "1171 Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang:\n",
      "Backdoor Attack through Frequency Domain. CoRR abs/2111.10991 (2021)\n",
      "1172 Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott:\n",
      "NTD: Non-Transferability Enabled Backdoor Detection. CoRR abs/2111.11157 (2021)\n",
      "1173 Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, Yunfei Yang:\n",
      "DBIA: Data-free Backdoor Injection Attack against Transformer Networks. CoRR abs/2111.11870 (2021)\n",
      "1174 Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu:\n",
      "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. CoRR abs/2111.12965 (2021)\n",
      "1175 Xiaofei Sun, Jiwei Li, Xiaoya Li, Ziyao Wang, Tianwei Zhang, Han Qiu, Fei Wu, Chun Fan:\n",
      "A General Framework for Defending Against Backdoor Attacks via Influence Graph. CoRR abs/2111.14309 (2021)\n",
      "1176 Zeki Bilgin:\n",
      "Anomaly Localization in Model Gradients Under Backdoor Attacks Against Federated Learning. CoRR abs/2111.14683 (2021)\n",
      "1177 Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao:\n",
      "FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis. CoRR abs/2112.01148 (2021)\n",
      "1178 Xi Li, Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks. CoRR abs/2112.03350 (2021)\n",
      "1179 Yang Liu, Zhihao Yi, Yan Kang, Yuanqin He, Wenhan Liu, Tianyuan Zou, Qiang Yang:\n",
      "Defending Label Inference and Backdoor Attacks in Vertical Federated Learning. CoRR abs/2112.05409 (2021)\n",
      "1180 Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha:\n",
      "Dual-Key Multimodal Backdoors for Visual Question Answering. CoRR abs/2112.07668 (2021)\n",
      "1181 Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao:\n",
      "Few-shot Backdoor Defense Using Shapley Estimation. CoRR abs/2112.14889 (2021)\n",
      "1182 Raluca Posteuca, Tomer Ashur:\n",
      "How to Backdoor a Cipher. IACR Cryptol. ePrint Arch. 2021: 442 (2021)\n",
      "1183 Giuseppe Vitto:\n",
      "Factoring Primes to Factor Moduli: Backdooring and Distributed Generation of Semiprimes. IACR Cryptol. ePrint Arch. 2021: 1610 (2021)\n",
      "1184 Eugene Bagdasaryan, Vitaly Shmatikov:\n",
      "(Withdrawn) Spinning Sequence-to-Sequence Models with Meta-Backdoors. CoRR abs/2107.10443 (2021)\n",
      "1185 Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming:\n",
      "(Withdrawn) CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing. CoRR abs/2112.13064 (2021)\n",
      "1186 Sogol Mazaheri:\n",
      "Cryptographic Primitives that Resist Backdooring and Subversion. Technical University of Darmstadt, Germany, 2020\n",
      "1187 Hyun Kwon:\n",
      "Detecting Backdoor Attacks via Class Difference in Deep Neural Networks. IEEE Access 8: 191049-191056 (2020)\n",
      "1188 Esha Sarkar, Yousif Alkindi, Michail Maniatakos:\n",
      "Backdoor Suppression in Neural Networks using Input Fuzzing and Majority Voting. IEEE Des. Test 37(2): 103-110 (2020)\n",
      "1189 Cecilia Pasquini, Rainer Böhme:\n",
      "Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition. EURASIP J. Inf. Secur. 2020: 12 (2020)\n",
      "1190 Hyun Kwon, Hyunsoo Yoon, Ki-Woong Park:\n",
      "Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks. IEICE Trans. Inf. Syst. 103-D(4): 883-887 (2020)\n",
      "1191 Jan Johannsen:\n",
      "Backdoors into Two Occurrences. J. Satisf. Boolean Model. Comput. 12(1): 1-15 (2020)\n",
      "1192 Yanjiao Chen, Xueluan Gong, Qian Wang, Xing Di, Huayang Huang:\n",
      "Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments. IEEE Netw. 34(5): 141-147 (2020)\n",
      "1193 Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash:\n",
      "Hidden Trigger Backdoor Attacks. AAAI 2020: 11957-11965\n",
      "1194 Can He, Mingfu Xue, Jian Wang, Weiqiang Liu:\n",
      "Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems. ACM TUR-C 2020: 231-235\n",
      "1195 Khaled Alrawashdeh, Stephen Goldsmith:\n",
      "Optimizing Deep Learning Based Intrusion Detection Systems Defense Against White-Box and Backdoor Adversarial Attacks Through a Genetic Algorithm. AIPR 2020: 1-8\n",
      "1196 Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov:\n",
      "How To Backdoor Federated Learning. AISTATS 2020: 2938-2948\n",
      "1197 Limor Gultchin, Matt J. Kusner, Varun Kanade, Ricardo Silva:\n",
      "Differentiable Causal Backdoor Discovery. AISTATS 2020: 3970-3979\n",
      "1198 Junyu Lin, Lei Xu, Yingqi Liu, Xiangyu Zhang:\n",
      "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features. CCS 2020: 113-131\n",
      "1199 Kota Yoshida, Takeshi Fujino:\n",
      "Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks. AISec@CCS 2020: 117-127\n",
      "1200 Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang:\n",
      "Can Adversarial Weight Perturbations Inject Neural Backdoors. CIKM 2020: 2029-2032\n",
      "1201 Haoti Zhong, Cong Liao, Anna Cinzia Squicciarini, Sencun Zhu, David J. Miller:\n",
      "Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. CODASPY 2020: 97-108\n",
      "1202 Thomas Peyrin, Haoyang Wang:\n",
      "The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers. CRYPTO (3) 2020: 249-278\n",
      "1203 Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann:\n",
      "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs. CVPR 2020: 298-307\n",
      "1204 Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor:\n",
      "Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers. CVPR Workshops 2020: 3422-3431\n",
      "1205 Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, Yu-Gang Jiang:\n",
      "Clean-Label Backdoor Attacks on Video Recognition Models. CVPR 2020: 14431-14440\n",
      "1206 Panagiota Kiourti, Kacper Wardega, Susmit Jha, Wenchao Li:\n",
      "TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning. DAC 2020: 1-6\n",
      "1207 Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, Zhuowen Tu:\n",
      "One-Pixel Signature: Characterizing CNN Models for Backdoor Detection. ECCV (27) 2020: 326-341\n",
      "1208 Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu:\n",
      "Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks. ECCV (10) 2020: 182-199\n",
      "1209 Xiangyu Wen, Wei Jiang, Jinyu Zhan, Xupeng Wang, Zhiyuan He:\n",
      "Interpretability Derived Backdoor Attacks Detection in Deep Neural Networks: Work-in-Progress. EMSOFT 2020: 13-14\n",
      "1210 Giulio Lovisotto, Simon Eberz, Ivan Martinovic:\n",
      "Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating. EuroS&P 2020: 184-197\n",
      "1211 Te Juin Lester Tan, Reza Shokri:\n",
      "Bypassing Backdoor Detection Algorithms in Deep Learning. EuroS&P 2020: 175-183\n",
      "1212 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification. ICASSP 2020: 3827-3831\n",
      "1213 Jacob Dumford, Walter J. Scheirer:\n",
      "Backdooring Convolutional Neural Networks via Targeted Weight Perturbations. IJCB 2020: 1-9\n",
      "1214 Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, Dawn Song:\n",
      "Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks. ICDM 2020: 162-171\n",
      "1215 Hyun Kwon, Jungmin Roh, Hyunsoo Yoon, Ki-Woong Park:\n",
      "TargetNet Backdoor: Attack on Deep Neural Network with Use of Different Triggers. ICIIT 2020: 140-145\n",
      "1216 Yansong Gao, Surya Nepal:\n",
      "A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks. ICISS 2020: 69-80\n",
      "1217 Mauro Barni:\n",
      "Backdooring Deep Learning Architectures: Threats and (some) Opportunities. ICISSP 2020: 15-16\n",
      "1218 Min Du, Ruoxi Jia, Dawn Song:\n",
      "Robust anomaly detection and backdoor attack detection via differential privacy. ICLR 2020\n",
      "1219 Chulin Xie, Keli Huang, Pin-Yu Chen, Bo Li:\n",
      "DBA: Distributed Backdoor Attacks against Federated Learning. ICLR 2020\n",
      "1220 Natasha Kees, Yaxuan Wang, Yiling Jiang, Fang Lue, Patrick P. K. Chan:\n",
      "Segmentation Based Backdoor Attack Detection. ICMLC 2020: 298-302\n",
      "1221 Xuankai Liu, Fengting Li, Bihan Wen, Qi Li:\n",
      "Removing Backdoor-Based Watermarks in Neural Networks with Limited Data. ICPR 2020: 10149-10156\n",
      "1222 Hyun Kwon, Hyunsoo Yoon, Ki-Woong Park:\n",
      "FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network. ICSIM 2020: 53-57\n",
      "1223 Sara Kaviani, Insoo Sohn, Huaping Liu:\n",
      "Application of complex systems in neural networks against Backdoor attacks. ICTC 2020: 57-59\n",
      "1224 Muhammad Umer, Glenn Dawson, Robi Polikar:\n",
      "Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks. IJCNN 2020: 1-8\n",
      "1225 Khaled Alrawashdeh, Stephen Goldsmith:\n",
      "Defending Deep Learning Based Anomaly Detection Systems Against White-Box Adversarial Examples and Backdoor Attacks. ISTAS 2020: 294-301\n",
      "1226 Riddhiman Adib, Paul M. Griffin, Sheikh Iqbal Ahamed, Mohammad Adibuzzaman:\n",
      "A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model. MLHC 2020: 376-396\n",
      "1227 Zhen Xiang, David J. Miller, Hang Wang, George Kesidis:\n",
      "Revealing Perceptible Backdoors in DNNs, Without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic. MLSP 2020: 1-6\n",
      "1228 Liuwan Zhu, Rui Ning, Cong Wang, Chunsheng Xin, Hongyi Wu:\n",
      "GangSweep: Sweep out Neural Backdoors by GAN. ACM Multimedia 2020: 3173-3181\n",
      "1229 Tuan Anh Nguyen, Anh Tuan Tran:\n",
      "Input-Aware Dynamic Backdoor Attack. NeurIPS 2020\n",
      "1230 Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris S. Papailiopoulos:\n",
      "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning. NeurIPS 2020\n",
      "1231 Cheng-Hsin Weng, Yan-Ting Lee, Shan-Hung Wu:\n",
      "On the Trade-off between Adversarial and Backdoor Robustness. NeurIPS 2020\n",
      "1232 Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, Svetha Venkatesh:\n",
      "Scalable Backdoor Detection in Neural Networks. ECML/PKDD (2) 2020: 289-304\n",
      "1233 Yayuan Xiong, Fengyuan Xu, Sheng Zhong, Qun Li:\n",
      "Escaping Backdoor Attack Detection of Deep Learning. SEC 2020: 431-445\n",
      "1234 Erwin Quiring, Konrad Rieck:\n",
      "Backdooring and Poisoning Neural Networks with Image-Scaling Attacks. SP (Workshops) 2020: 41-47\n",
      "1235 Yevgeniy Dodis, Pooya Farshim, Sogol Mazaheri, Stefano Tessaro:\n",
      "Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity. TCC (3) 2020: 241-273\n",
      "1236 Silvija Kokalj-Filipovic, Morriel Kasher, Michael Zhao, Predrag Spasojevic:\n",
      "Detecting acoustic backdoor transmission of inaudible messages using deep learning. WiseML@WiSec 2020: 80-85\n",
      "1237 Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen:\n",
      "Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models. CoRR abs/2001.03274 (2020)\n",
      "1238 Muhammad Umer, Glenn Dawson, Robi Polikar:\n",
      "Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks. CoRR abs/2002.07111 (2020)\n",
      "1239 Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg:\n",
      "NNoculation: Broad Spectrum and Targeted Treatment of Backdoored DNNs. CoRR abs/2002.08313 (2020)\n",
      "1240 Binghui Wang, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong:\n",
      "On Certifying Robustness against Backdoor Attacks via Randomized Smoothing. CoRR abs/2002.11750 (2020)\n",
      "1241 Hao Cheng, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, Xue Lin:\n",
      "Defending against Backdoor Attack on Deep Neural Networks. CoRR abs/2002.12162 (2020)\n",
      "1242 Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, Andreas Zeller:\n",
      "Exposing Backdoors in Robust Machine Learning Models. CoRR abs/2003.00865 (2020)\n",
      "1243 Giorgio Severi, Jim Meyer, Scott E. Coull, Alina Oprea:\n",
      "Exploring Backdoor Poisoning Attacks Against Malware Classifiers. CoRR abs/2003.01031 (2020)\n",
      "1244 Limor Gultchin, Matt J. Kusner, Varun Kanade, Ricardo Silva:\n",
      "Differentiable Causal Backdoor Discovery. CoRR abs/2003.01461 (2020)\n",
      "1245 Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, Yu-Gang Jiang:\n",
      "Clean-Label Backdoor Attacks on Video Recognition Models. CoRR abs/2003.03030 (2020)\n",
      "1246 Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, Yang Zhang:\n",
      "Dynamic Backdoor Attacks Against Machine Learning Models. CoRR abs/2003.03675 (2020)\n",
      "1247 Yue Wang, Esha Sarkar, Michail Maniatakos, Saif Eddin Jabari:\n",
      "Watch your back: Backdoor Attacks in Deep Reinforcement Learning-based Autonomous Vehicle Control Systems. CoRR abs/2003.07859 (2020)\n",
      "1248 Erwin Quiring, Konrad Rieck:\n",
      "Backdooring and Poisoning Neural Networks with Image-Scaling Attacks. CoRR abs/2003.08633 (2020)\n",
      "1249 Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, Bo Li:\n",
      "RAB: Provable Robustness Against Backdoor Attacks. CoRR abs/2003.08904 (2020)\n",
      "1250 Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia:\n",
      "Rethinking the Trigger of Backdoor Attack. CoRR abs/2004.04692 (2020)\n",
      "1251 William Aiken, Hyoungshick Kim, Simon S. Woo:\n",
      "Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks. CoRR abs/2004.11368 (2020)\n",
      "1252 Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor:\n",
      "Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers. CoRR abs/2004.11514 (2020)\n",
      "1253 Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri:\n",
      "Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks. CoRR abs/2004.12492 (2020)\n",
      "1254 Eugene Bagdasaryan, Vitaly Shmatikov:\n",
      "Blind Backdoors in Deep Learning Models. CoRR abs/2005.03823 (2020)\n",
      "1255 Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Yang Zhang:\n",
      "BadNL: Backdoor Attacks Against NLP Models. CoRR abs/2006.01043 (2020)\n",
      "1256 Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, Svetha Venkatesh:\n",
      "Scalable Backdoor Detection in Neural Networks. CoRR abs/2006.05646 (2020)\n",
      "1257 Kathrin Grosse, Taesung Lee, Youngja Park, Michael Backes, Ian M. Molloy:\n",
      "A new measure for overfitting and its implications for backdooring of deep learning. CoRR abs/2006.06721 (2020)\n",
      "1258 Goutham Ramakrishnan, Aws Albarghouthi:\n",
      "Backdoors in Neural Models of Source Code. CoRR abs/2006.06841 (2020)\n",
      "1259 Chien-Lun Chen, Leana Golubchik, Marco Paolieri:\n",
      "Backdoor Attacks on Federated Meta-Learning. CoRR abs/2006.07026 (2020)\n",
      "1260 Zaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong:\n",
      "Backdoor Attacks to Graph Neural Networks. CoRR abs/2006.11165 (2020)\n",
      "1261 Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos:\n",
      "FaceHack: Triggering backdoored facial recognition systems using facial characteristics. CoRR abs/2006.11623 (2020)\n",
      "1262 Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang:\n",
      "Graph Backdoor. CoRR abs/2006.11890 (2020)\n",
      "1263 Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P. Dickerson, Tom Goldstein:\n",
      "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. CoRR abs/2006.12557 (2020)\n",
      "1264 Riddhiman Adib, Paul M. Griffin, Sheikh Iqbal Ahamed, Mohammad Adibuzzaman:\n",
      "A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model. CoRR abs/2006.12573 (2020)\n",
      "1265 Emily Wenger, Josephine Passananti, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao:\n",
      "Backdoor Attacks on Facial Recognition in the Physical World. CoRR abs/2006.14580 (2020)\n",
      "1266 Lichao Sun:\n",
      "Natural Backdoor Attack on Text Data. CoRR abs/2006.16176 (2020)\n",
      "1267 Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu:\n",
      "Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks. CoRR abs/2007.02343 (2020)\n",
      "1268 Yang Liu, Zhihao Yi, Tianjian Chen:\n",
      "Backdoor attacks and defenses in feature-partitioned collaborative learning. CoRR abs/2007.03608 (2020)\n",
      "1269 Mustafa Safa Özdayi, Murat Kantarcioglu, Yulia R. Gel:\n",
      "Defending Against Backdoors in Federated Learning with Robust Learning Rate. CoRR abs/2007.03767 (2020)\n",
      "1270 Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris S. Papailiopoulos:\n",
      "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning. CoRR abs/2007.05084 (2020)\n",
      "1271 Shaofeng Li, Shiqing Ma, Minhui Xue, Benjamin Zi Hao Zhao:\n",
      "Deep Learning Backdoors. CoRR abs/2007.08273 (2020)\n",
      "1272 Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia:\n",
      "Backdoor Learning: A Survey. CoRR abs/2007.08745 (2020)\n",
      "1273 Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, Hyoungshick Kim:\n",
      "Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review. CoRR abs/2007.10760 (2020)\n",
      "1274 Chuanshuai Chen, Jiazhu Dai:\n",
      "Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification. CoRR abs/2007.12070 (2020)\n",
      "1275 Takayuki Sasaki, Yusuke Shimada:\n",
      "Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection. CoRR abs/2007.14748 (2020)\n",
      "1276 N. Benjamin Erichson, Dane Taylor, Qixuan Wu, Michael W. Mahoney:\n",
      "Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks. CoRR abs/2008.00123 (2020)\n",
      "1277 Xuankai Liu, Fengting Li, Bihan Wen, Qi Li:\n",
      "Removing Backdoor-Based Watermarks in Neural Networks with Limited Data. CoRR abs/2008.00407 (2020)\n",
      "1278 Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang:\n",
      "Can Adversarial Weight Perturbations Inject Neural Backdoors? CoRR abs/2008.01761 (2020)\n",
      "1279 Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, Zhuowen Tu:\n",
      "One-pixel Signature: Characterizing CNN Models for Backdoor Detection. CoRR abs/2008.07711 (2020)\n",
      "1280 Haoliang Li, Yufei Wang, Xiaofei Xie, Yang Liu, Shiqi Wang, Renjie Wan, Lap-Pui Chau, Alex C. Kot:\n",
      "Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems. CoRR abs/2009.06996 (2020)\n",
      "1281 Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik:\n",
      "What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors. CoRR abs/2009.10639 (2020)\n",
      "1282 Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, Yang Zhang:\n",
      "BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models. CoRR abs/2010.03007 (2020)\n",
      "1283 Ahmed Salem, Michael Backes, Yang Zhang:\n",
      "Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks. CoRR abs/2010.03282 (2020)\n",
      "1284 Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao Xia:\n",
      "Open-sourced Dataset Protection via Backdoor Watermarking. CoRR abs/2010.05821 (2020)\n",
      "1285 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing. CoRR abs/2010.07489 (2020)\n",
      "1286 Anh Nguyen, Anh Tuan Tran:\n",
      "Input-Aware Dynamic Backdoor Attack. CoRR abs/2010.08138 (2020)\n",
      "1287 Mingjie Sun, Siddhant Agarwal, J. Zico Kolter:\n",
      "Poisoned classifiers are not only backdoored, they are fundamentally broken. CoRR abs/2010.09080 (2020)\n",
      "1288 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set. CoRR abs/2010.09987 (2020)\n",
      "1289 Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia:\n",
      "Backdoor Attack against Speaker Verification. CoRR abs/2010.11607 (2020)\n",
      "1290 Akshaj Kumar Veldanda, Siddharth Garg:\n",
      "On Evaluating Neural Network Backdoor Defenses. CoRR abs/2010.12186 (2020)\n",
      "1291 Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu:\n",
      "EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks. CoRR abs/2011.00101 (2020)\n",
      "1292 Chen Wu, Xian Yang, Sencun Zhu, Prasenjit Mitra:\n",
      "Mitigating Backdoor Attacks in Federated Learning. CoRR abs/2011.01767 (2020)\n",
      "1293 Sébastien Andreina, Giorgia Azzurra Marson, Helen Möllering, Ghassan Karame:\n",
      "BaFFLe: Backdoor detection via Feedback-based Federated Learning. CoRR abs/2011.02167 (2020)\n",
      "1294 Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami:\n",
      "Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection. CoRR abs/2011.02526 (2020)\n",
      "1295 Anbu Huang:\n",
      "Dynamic backdoor attacks against federated learning. CoRR abs/2011.07429 (2020)\n",
      "1296 Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, Arjun Gupta:\n",
      "Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff. CoRR abs/2011.09527 (2020)\n",
      "1297 Fanchao Qi, Yangyi Chen, Mukai Li, Zhiyuan Liu, Maosong Sun:\n",
      "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. CoRR abs/2011.10369 (2020)\n",
      "1298 Shihong Fang, Anna Choromanska:\n",
      "Backdoor Attacks on the DNN Interpretation System. CoRR abs/2011.10698 (2020)\n",
      "1299 Henry D. Chacon, Paul Rad:\n",
      "Effect of backdoor attacks over the complexity of the latent space distribution. CoRR abs/2012.01931 (2020)\n",
      "1300 Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu:\n",
      "Backdoor Attack with Sample-Specific Triggers. CoRR abs/2012.03816 (2020)\n",
      "1301 Yi Zeng, Han Qiu, Shangwei Guo, Tianwei Zhang, Meikang Qiu, Bhavani Thuraisingham:\n",
      "DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation. CoRR abs/2012.07006 (2020)\n",
      "1302 Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay Jha:\n",
      "HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios. CoRR abs/2012.07474 (2020)\n",
      "1303 Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Ting Wang:\n",
      "TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask). CoRR abs/2012.09302 (2020)\n",
      "1304 Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein:\n",
      "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. CoRR abs/2012.10544 (2020)\n",
      "1305 Yevgeniy Dodis, Pooya Farshim, Sogol Mazaheri, Stefano Tessaro:\n",
      "Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity. IACR Cryptol. ePrint Arch. 2020: 1199 (2020)\n",
      "1306 Thomas Peyrin, Haoyang Wang:\n",
      "The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers. IACR Cryptol. ePrint Arch. 2020: 986 (2020)\n",
      "1307 Jiazhu Dai, Chuanshuai Chen, Yufeng Li:\n",
      "A Backdoor Attack Against LSTM-Based Text Classification Systems. IEEE Access 7: 138872-138878 (2019)\n",
      "1308 Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg:\n",
      "BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. IEEE Access 7: 47230-47244 (2019)\n",
      "1309 Martin Kronegger, Sebastian Ordyniak, Andreas Pfandler:\n",
      "Backdoors to planning. Artif. Intell. 269: 49-75 (2019)\n",
      "1310 Arne Meier, Sebastian Ordyniak, M. S. Ramanujan, Irena Schindler:\n",
      "Backdoors for Linear Temporal Logic. Algorithmica 81(2): 476-496 (2019)\n",
      "1311 Kai-Min Chung, Marios Georgiou, Ching-Yi Lai, Vassilis Zikas:\n",
      "Cryptography with Disposable Backdoors. Cryptogr. 3(3): 22 (2019)\n",
      "1312 Anze Mihelic, Matej Jevscek, Simon Vrhovec, Igor Bernik:\n",
      "Testing the Human Backdoor: Organizational Response to a Phishing Campaign. J. Univers. Comput. Sci. 25(11): 1458-1477 (2019)\n",
      "1313 Tim Eisert, Christian Eufinger:\n",
      "Interbank Networks and Backdoor Bailouts: Benefiting from Other Banks' Government Guarantees. Manag. Sci. 65(8): 3673-3693 (2019)\n",
      "1314 Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian M. Molloy, Biplav Srivastava:\n",
      "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. SafeAI@AAAI 2019\n",
      "1315 Yuanshun Yao, Huiying Li, Haitao Zheng, Ben Y. Zhao:\n",
      "Latent Backdoor Attacks on Deep Neural Networks. CCS 2019: 2041-2055\n",
      "1316 Huma Rehman, Andreas Ekelhart, Rudolf Mayer:\n",
      "Backdoor Attacks in Neural Networks - A Systematic Evaluation on Multiple Traffic Sign Datasets. CD-MAKE 2019: 285-300\n",
      "1317 Maximilian Bachl, Alexander Hartl, Joachim Fabini, Tanja Zseby:\n",
      "Walling up Backdoors in Intrusion Detection Systems. Big-DAMA@CoNEXT 2019: 8-13\n",
      "1318 Vladimir Sedlacek, Dusan Klinec, Marek Sýs, Petr Svenda, Vashek Matyas:\n",
      "I Want to Break Square-free: The 4p - 1 Factorization Method and Its RSA Backdoor Viability. ICETE (2) 2019: 25-36\n",
      "1319 Mauro Barni, Kassem Kallas, Benedetta Tondi:\n",
      "A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning. ICIP 2019: 101-105\n",
      "1320 Thibaut Horel, Sunoo Park, Silas Richelson, Vinod Vaikuntanathan:\n",
      "How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts. ITCS 2019: 42:1-42:20\n",
      "1321 Nidish Vashistha, M. Tanjidur Rahman, Olivia P. Paradis, Navid Asadizanjani:\n",
      "Is Backside the New Backdoor in Modern SoCs?: Invited Paper. ITC 2019: 1-10\n",
      "1322 Michael Sioutis, Tomi Janhunen:\n",
      "Towards Leveraging Backdoors in Qualitative Constraint Networks. KI 2019: 308-315\n",
      "1323 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense. MLSP 2019: 1-6\n",
      "1324 Abhir Bhalerao, Kassem Kallas, Benedetta Tondi, Mauro Barni:\n",
      "Luminance-based video backdoor attack against anti-spoofing rebroadcast detection. MMSP 2019: 1-6\n",
      "1325 Ximing Qiao, Yukun Yang, Hai Li:\n",
      "Defending Neural Backdoors via Generative Distribution Modeling. NeurIPS 2019: 14004-14013\n",
      "1326 Shoichiro Sasaki, Seira Hidano, Toshihiro Uchibayashi, Takuo Suganuma, Masahiro Hiji, Shinsaku Kiyomoto:\n",
      "On Embedding Backdoor in Malware Detectors Using Machine Learning. PST 2019: 1-5\n",
      "1327 Lane A. Hemaspaandra, David E. Narváez:\n",
      "Existence Versus Exploitation: The Opacity of Backdoors and Backbones Under a Weak Assumption. SOFSEM 2019: 247-259\n",
      "1328 Emma Dauterman, Henry Corrigan-Gibbs, David Mazières, Dan Boneh, Dominic Rizzo:\n",
      "True2F: Backdoor-Resistant Authentication Tokens. IEEE Symposium on Security and Privacy 2019: 398-416\n",
      "1329 Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Ben Y. Zhao:\n",
      "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. IEEE Symposium on Security and Privacy 2019: 707-723\n",
      "1330 Zhaoyuan Yang, Naresh Iyer, Johan Reimann, Nurali Virani:\n",
      "Design of intentional backdoors in sequential models. CoRR abs/1902.09972 (2019)\n",
      "1331 Mauro Barni, Kassem Kallas, Benedetta Tondi:\n",
      "A new Backdoor Attack in CNNs by training set corruption without label poisoning. CoRR abs/1902.11237 (2019)\n",
      "1332 Eric Filiol:\n",
      "BSEA-1 - A Stream Cipher Backdooring Technique. CoRR abs/1903.11063 (2019)\n",
      "1333 Yehao Kong, Jiliang Zhang:\n",
      "Adversarial Audio: A New Information Hiding Method and Backdoor for DNN-based Speech Recognition Models. CoRR abs/1904.03829 (2019)\n",
      "1334 Giulio Lovisotto, Simon Eberz, Ivan Martinovic:\n",
      "Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating. CoRR abs/1905.09162 (2019)\n",
      "1335 Yuanshun Yao, Huiying Li, Haitao Zheng, Ben Y. Zhao:\n",
      "Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks. CoRR abs/1905.10447 (2019)\n",
      "1336 Jiazhu Dai, Chuanshuai Chen, Yike Guo:\n",
      "A backdoor attack against LSTM-based text classification systems. CoRR abs/1905.12457 (2019)\n",
      "1337 Te Juin Lester Tan, Reza Shokri:\n",
      "Bypassing Backdoor Detection Algorithms in Deep Learning. CoRR abs/1905.13409 (2019)\n",
      "1338 Masoumeh Shafieinejad, Jiaqi Wang, Nils Lukas, Florian Kerschbaum:\n",
      "On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks. CoRR abs/1906.07745 (2019)\n",
      "1339 Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann:\n",
      "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs. CoRR abs/1906.10842 (2019)\n",
      "1340 Di Tang, Xiaofeng Wang, Haixu Tang, Kehuan Zhang:\n",
      "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection. CoRR abs/1908.00686 (2019)\n",
      "1341 Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, Dawn Song:\n",
      "TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems. CoRR abs/1908.01763 (2019)\n",
      "1342 Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, Sudipta Chattopadhyay:\n",
      "Model Agnostic Defence against Backdoor Attacks in Machine Learning. CoRR abs/1908.02203 (2019)\n",
      "1343 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification. CoRR abs/1908.10498 (2019)\n",
      "1344 Shaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, Haojin Zhu:\n",
      "Invisible Backdoor Attacks Against Deep Neural Networks. CoRR abs/1909.02742 (2019)\n",
      "1345 Maximilian Bachl, Alexander Hartl, Joachim Fabini, Tanja Zseby:\n",
      "Walling up Backdoors in Intrusion Detection Systems. CoRR abs/1909.07866 (2019)\n",
      "1346 Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash:\n",
      "Hidden Trigger Backdoor Attacks. CoRR abs/1910.00033 (2019)\n",
      "1347 Ximing Qiao, Yukun Yang, Hai Li:\n",
      "Defending Neural Backdoors via Generative Distribution Modeling. CoRR abs/1910.04749 (2019)\n",
      "1348 Ren Pang, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Ting Wang:\n",
      "The Tale of Evil Twins: Adversarial Inputs versus Backdoored Models. CoRR abs/1911.01559 (2019)\n",
      "1349 Min Du, Ruoxi Jia, Dawn Song:\n",
      "Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy. CoRR abs/1911.07116 (2019)\n",
      "1350 Xijie Huang, Moustafa Alzantot, Mani B. Srivastava:\n",
      "NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations. CoRR abs/1911.07399 (2019)\n",
      "1351 Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, H. Brendan McMahan:\n",
      "Can You Really Backdoor Federated Learning? CoRR abs/1911.07963 (2019)\n",
      "1352 Zhen Xiang, David J. Miller, George Kesidis:\n",
      "Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic. CoRR abs/1911.07970 (2019)\n",
      "1353 Alvin Chan, Yew-Soon Ong:\n",
      "Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks. CoRR abs/1911.08040 (2019)\n",
      "1354 Alexander Turner, Dimitris Tsipras, Aleksander Madry:\n",
      "Label-Consistent Backdoor Attacks. CoRR abs/1912.02771 (2019)\n",
      "1355 Sam L. Thomas:\n",
      "Backdoor detection systems for embedded devices. University of Birmingham, UK, 2018\n",
      "1356 Markus G. Kuhn:\n",
      "Technical perspective: Backdoor engineering. Commun. ACM 61(11): 147 (2018)\n",
      "1357 Alexander A. Semenov, Oleg Zaikin, Ilya V. Otpuschennikov, Stepan Kochemazov, Alexey Ignatiev:\n",
      "On Cryptographic Attacks Using Backdoors for SAT. AAAI 2018: 6641-6648\n",
      "1358 Yao Yao, Lipeng Zhu, He Wang:\n",
      "Real-time Detection of Passive Backdoor Behaviors on Android System. CNS 2018: 1-9\n",
      "1359 Edward Zulkoski, Ruben Martins, Christoph M. Wintersteiger, Robert Robere, Jia Hui Liang, Krzysztof Czarnecki, Vijay Ganesh:\n",
      "Learning-Sensitive Backdoors with Restarts. CP 2018: 453-469\n",
      "1360 Guillaume Escamocher, Mohamed Siala, Barry O'Sullivan:\n",
      "From Backdoor Key to Backdoor Completability: Improving a Known Measure of Hardness for the Satisfiable CSP. CPAIOR 2018: 198-214\n",
      "1361 Balthazar Bauer, Pooya Farshim, Sogol Mazaheri:\n",
      "Combiners for Backdoored Random Oracles. CRYPTO (2) 2018: 272-302\n",
      "1362 Marc Fischlin, Christian Janson, Sogol Mazaheri:\n",
      "Backdoored Hash Functions: Immunizing HMAC and HKDF. CSF 2018: 105-118\n",
      "1363 Yaswanth Kolli, Tauheed Khan Mohd, Ahmad Y. Javaid:\n",
      "Remote Desktop Backdoor Implementation with Reverse TCP Payload Using Open Source Tools for Instructional Use. EIT 2018: 249-254\n",
      "1364 Joseph Clements, Yingjie Lao:\n",
      "Backdoor Attacks on Neural Network Operations. GlobalSIP 2018: 1154-1158\n",
      "1365 Chin-Wei Tien, Tsung-Ta Tsai, Ing-Yi Chen, Sy-Yen Kuo:\n",
      "UFO - Hidden Backdoor Discovery and Security Verification in IoT Device Firmware. ISSRE Workshops 2018: 18-23\n",
      "1366 Brandon Tran, Jerry Li, Aleksander Madry:\n",
      "Spectral Signatures in Backdoor Attacks. NeurIPS 2018: 8011-8021\n",
      "1367 Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg:\n",
      "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. RAID 2018: 273-294\n",
      "1368 Sam L. Thomas, Aurélien Francillon:\n",
      "Backdoors: Definition, Deniability and Detection. RAID 2018: 92-113\n",
      "1369 Stepan Kochemazov, Oleg Zaikin:\n",
      "ALIAS: A Modular Tool for Finding Backdoors for SAT. SAT 2018: 419-427\n",
      "1370 Yossi Adi, Carsten Baum, Moustapha Cissé, Benny Pinkas, Joseph Keshet:\n",
      "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring. USENIX Security Symposium 2018: 1615-1631\n",
      "1371 Yossi Adi, Carsten Baum, Moustapha Cissé, Benny Pinkas, Joseph Keshet:\n",
      "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring. CoRR abs/1802.04633 (2018)\n",
      "1372 Thibaut Horel, Sunoo Park, Silas Richelson, Vinod Vaikuntanathan:\n",
      "How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts. CoRR abs/1802.07381 (2018)\n",
      "1373 Alexander A. Semenov, Oleg Zaikin, Ilya V. Otpuschennikov, Stepan Kochemazov, Alexey Ignatiev:\n",
      "On Cryptographic Attacks Using Backdoors for SAT. CoRR abs/1803.04646 (2018)\n",
      "1374 Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg:\n",
      "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. CoRR abs/1805.12185 (2018)\n",
      "1375 Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov:\n",
      "How To Backdoor Federated Learning. CoRR abs/1807.00459 (2018)\n",
      "1376 Cong Liao, Haoti Zhong, Anna Cinzia Squicciarini, Sencun Zhu, David J. Miller:\n",
      "Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. CoRR abs/1808.10307 (2018)\n",
      "1377 Emma Dauterman, Henry Corrigan-Gibbs, David Mazières, Dan Boneh, Dominic Rizzo:\n",
      "True2F: Backdoor-resistant authentication tokens. CoRR abs/1810.04660 (2018)\n",
      "1378 Brandon Tran, Jerry Li, Aleksander Madry:\n",
      "Spectral Signatures in Backdoor Attacks. CoRR abs/1811.00636 (2018)\n",
      "1379 Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian M. Molloy, Biplav Srivastava:\n",
      "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. CoRR abs/1811.03728 (2018)\n",
      "1380 Petr Kucera, Petr Savický:\n",
      "Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings. CoRR abs/1811.09435 (2018)\n",
      "1381 Baptiste David:\n",
      "How a simple bug in ML compiler could be exploited for backdoors? CoRR abs/1811.10851 (2018)\n",
      "1382 Jacob Dumford, Walter J. Scheirer:\n",
      "Backdooring Convolutional Neural Networks via Targeted Weight Perturbations. CoRR abs/1812.03128 (2018)\n",
      "1383 Balthazar Bauer, Pooya Farshim, Sogol Mazaheri:\n",
      "Combiners for Backdoored Random Oracles. IACR Cryptol. ePrint Arch. 2018: 770 (2018)\n",
      "1384 Kai-Min Chung, Marios Georgiou, Ching-Yi Lai, Vassilis Zikas:\n",
      "Cryptography with Dispensable Backdoors. IACR Cryptol. ePrint Arch. 2018: 352 (2018)\n",
      "1385 Nicolas T. Courtois:\n",
      "On the Existence of Non-Linear Invariants and Algebraic Polynomial Constructive Approach to Backdoors in Block Ciphers. IACR Cryptol. ePrint Arch. 2018: 807 (2018)\n",
      "1386 Marc Fischlin, Christian Janson, Sogol Mazaheri:\n",
      "Backdoored Hash Functions: Immunizing HMAC and HKDF. IACR Cryptol. ePrint Arch. 2018: 362 (2018)\n",
      "1387 Thibaut Horel, Sunoo Park, Silas Richelson, Vinod Vaikuntanathan:\n",
      "How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts. IACR Cryptol. ePrint Arch. 2018: 212 (2018)\n",
      "1388 Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider, Stanislav Zivný:\n",
      "Backdoors into heterogeneous classes of SAT and CSP. J. Comput. Syst. Sci. 85: 38-56 (2017)\n",
      "1389 Junsung Cho, Geumhwan Cho, Sangwon Hyun, Hyoungshick Kim:\n",
      "Open Sesame! Design and Implementation of Backdoor to Secretly Unlock Android Devices. J. Internet Serv. Inf. Secur. 7(4): 35-44 (2017)\n",
      "1390 Nirupam Roy, Haitham Hassanieh, Romit Roy Choudhury:\n",
      "BackDoor: Sounds that a microphone can record, but that humans can't hear. GetMobile Mob. Comput. Commun. 21(4): 25-29 (2017)\n",
      "1391 Yujie Ji, Xinyang Zhang, Ting Wang:\n",
      "Backdoor attacks against learning systems. CNS 2017: 1-9\n",
      "1392 Sam L. Thomas, Tom Chothia, Flavio D. Garcia:\n",
      "Stringer: Measuring the Importance of Static Data Comparisons to Detect Backdoors and Undocumented Functionality. ESORICS (2) 2017: 513-531\n",
      "1393 Arnaud Bannier, Eric Filiol:\n",
      "Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher. ICISSP 2017: 622-631\n",
      "1394 Huicong Loi, Aspen Olmsted:\n",
      "Low-cost detection of backdoor malware. ICITST 2017: 197-198\n",
      "1395 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoor Trees for Answer Set Programming. ASPOCP@LPNMR 2017\n",
      "1396 Nirupam Roy, Haitham Hassanieh, Romit Roy Choudhury:\n",
      "BackDoor: Making Microphones Hear Inaudible Sounds. MobiSys 2017: 2-14\n",
      "1397 Kristen Dorey, Nicholas Chang-Fong, Aleksander Essex:\n",
      "Indiscreet Logs: Diffie-Hellman Backdoors in TLS. NDSS 2017\n",
      "1398 Robert Ganian, M. S. Ramanujan, Stefan Szeider:\n",
      "Backdoor Treewidth for SAT. SAT 2017: 20-37\n",
      "1399 Robert Ganian, M. S. Ramanujan, Stefan Szeider:\n",
      "Combining Treewidth and Backdoors for CSP. STACS 2017: 36:1-36:17\n",
      "1400 Serge Gaspers, Sebastian Ordyniak, Stefan Szeider:\n",
      "Backdoor Sets for CSP. The Constraint Satisfaction Problem 2017: 137-157\n",
      "1401 Arnaud Bannier, Eric Filiol:\n",
      "Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher. CoRR abs/1702.06475 (2017)\n",
      "1402 Lane A. Hemaspaandra, David E. Narváez:\n",
      "The Opacity of Backbones and Backdoors Under a Weak Assumption. CoRR abs/1706.04582 (2017)\n",
      "1403 Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song:\n",
      "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. CoRR abs/1712.05526 (2017)\n",
      "1404 Serge Gaspers, Sebastian Ordyniak, M. S. Ramanujan, Saket Saurabh, Stefan Szeider:\n",
      "Backdoors to q-Horn. Algorithmica 74(1): 540-557 (2016)\n",
      "1405 CACM Staff:\n",
      "No backdoor required or expected. Commun. ACM 59(6): 8-9 (2016)\n",
      "1406 Felix von Eye, Michael Grabatin, Wolfgang Hommel:\n",
      "Detecting Stealthy Backdoors and Port Knocking Sequences through Flow Analysis. Prax. Inf.verarb. Kommun. 38(3-4): 97-104 (2016)\n",
      "1407 Robert Ganian, M. S. Ramanujan, Stefan Szeider:\n",
      "Backdoors to Tractable Valued CSP. CP 2016: 233-250\n",
      "1408 Jean Paul Degabriele, Kenneth G. Paterson, Jacob C. N. Schuldt, Joanne Woodage:\n",
      "Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results. CRYPTO (1) 2016: 403-432\n",
      "1409 Arne Meier, Sebastian Ordyniak, Ramanujan Sridharan, Irena Schindler:\n",
      "Backdoors for Linear Temporal Logic. IPEC 2016: 23:1-23:17\n",
      "1410 Lucjan Hanzlik, Kamil Kluczniak, Miroslaw Kutylowski:\n",
      "Controlled Randomness - A Defense Against Backdoors in Cryptographic Devices. Mycrypt 2016: 215-232\n",
      "1411 Johannes Klaus Fichte, Arne Meier, Irina Schindler:\n",
      "Strong Backdoors for Default Logic. SAT 2016: 45-59\n",
      "1412 Serge Gaspers:\n",
      "Backdoors to SAT. Encyclopedia of Algorithms 2016: 167-170\n",
      "1413 Johannes Klaus Fichte, Arne Meier, Irina Schindler:\n",
      "Strong Backdoors for Default Logic. CoRR abs/1602.06052 (2016)\n",
      "1414 Robert Ganian, M. S. Ramanujan, Stefan Szeider:\n",
      "Combining Treewidth and Backdoors for CSP. CoRR abs/1610.03298 (2016)\n",
      "1415 Robert Ganian, M. S. Ramanujan, Stefan Szeider:\n",
      "Backdoors to Tractable Valued CSP. CoRR abs/1612.05733 (2016)\n",
      "1416 Arne Meier, Sebastian Ordyniak, M. S. Ramanujan, Irena Schindler:\n",
      "Strong Backdoors for Linear Temporal Logic. CoRR abs/1602.04934 (2016)\n",
      "1417 Jean Paul Degabriele, Kenneth G. Paterson, Jacob C. N. Schuldt, Joanne Woodage:\n",
      "Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results. IACR Cryptol. ePrint Arch. 2016: 577 (2016)\n",
      "1418 Yevgeniy Dodis, Chaya Ganesh, Alexander Golovnev, Ari Juels, Thomas Ristenpart:\n",
      "A Formal Treatment of Backdoored Pseudorandom Generators. IACR Cryptol. ePrint Arch. 2016: 306 (2016)\n",
      "1419 Kristen Dorey, Nicholas Chang-Fong, Aleksander Essex:\n",
      "Indiscreet Logs: Persistent Diffie-Hellman Backdoors in TLS. IACR Cryptol. ePrint Arch. 2016: 999 (2016)\n",
      "1420 Peter Linder:\n",
      "DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors. IACR Cryptol. ePrint Arch. 2016: 245 (2016)\n",
      "1421 David Wong:\n",
      "How to Backdoor Diffie-Hellman. IACR Cryptol. ePrint Arch. 2016: 644 (2016)\n",
      "1422 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to tractable answer set programming. Artif. Intell. 220: 64-103 (2015)\n",
      "1423 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to Normality for Disjunctive Logic Programs. ACM Trans. Comput. Log. 17(1): 7 (2015)\n",
      "1424 Martin Kronegger, Sebastian Ordyniak, Andreas Pfandler:\n",
      "Variable-Deletion Backdoors to Planning. AAAI 2015: 3305-3312\n",
      "1425 Johannes Klick, Stephan Lau, Daniel Marzin, Jan-Ole Malchow, Volker Roth:\n",
      "Internet-facing PLCs as a network backdoor. CNS 2015: 524-532\n",
      "1426 Felix von Eye, Michael Grabatin, Wolfgang Hommel:\n",
      "Netzbasierte Erkennung von mittels Port Knocking verstecksten Dienstern und Backdoors. DFN-Forum Kommunikationstechnologien 2015: 57-67\n",
      "1427 Xuan Thuy Ngo, Zakaria Najm, Shivam Bhasin, Debapriya Basu Roy, Jean-Luc Danger, Sylvain Guilley:\n",
      "Integrated Sensor: A Backdoor for Hardware Trojan Insertions? DSD 2015: 415-422\n",
      "1428 Yevgeniy Dodis, Chaya Ganesh, Alexander Golovnev, Ari Juels, Thomas Ristenpart:\n",
      "A Formal Treatment of Backdoored Pseudorandom Generators. EUROCRYPT (1) 2015: 101-126\n",
      "1429 Florian Kerber, Dominik Teubert, Ulrike Meyer:\n",
      "Covert remote syscall communication at kernel level: A SPOOKY backdoor. MALWARE 2015: 74-81\n",
      "1430 Seongyeol Oh, Joon-Sung Yang, Andrea Bianchi, Hyoungshick Kim:\n",
      "Devil in a box: Installing backdoors in electronic door locks. PST 2015: 139-144\n",
      "1431 Fedor V. Fomin, Daniel Lokshtanov, Neeldhara Misra, M. S. Ramanujan, Saket Saurabh:\n",
      "Solving d-SAT via Backdoors to Small Treewidth. SODA 2015: 630-641\n",
      "1432 Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider, Stanislav Zivný:\n",
      "Backdoors into Heterogeneous Classes of SAT and CSP. CoRR abs/1509.05725 (2015)\n",
      "1433 Bistra Dilkina, Carla P. Gomes, Ashish Sabharwal:\n",
      "Tradeoffs in the complexity of backdoors to satisfiability: dynamic sub-solvers and learning during search. Ann. Math. Artif. Intell. 70(4): 399-431 (2014)\n",
      "1434 Dirk Fox:\n",
      "Backdoor. Datenschutz und Datensicherheit 38(2): 119 (2014)\n",
      "1435 Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider, Stanislav Zivný:\n",
      "Backdoors into Heterogeneous Classes of SAT and CSP. AAAI 2014: 2652-2658\n",
      "1436 Martin Kronegger, Sebastian Ordyniak, Andreas Pfandler:\n",
      "Backdoors to Planning. AAAI 2014: 2300-2307\n",
      "1437 Clément Carbonnel, Martin C. Cooper, Emmanuel Hebrard:\n",
      "On Backdoors to Tractable Constraint Languages. CP 2014: 224-239\n",
      "1438 Emilia Oikarinen, Matti Järvisalo:\n",
      "Answer Set Solver Backdoors. JELIA 2014: 674-683\n",
      "1439 Clément Carbonnel, Martin C. Cooper, Emmanuel Hebrard:\n",
      "On Backdoors To Tractable Constraint Languages. CoRR abs/1404.3675 (2014)\n",
      "1440 Matteo Fischetti, Michele Monaci:\n",
      "Backdoor Branching. INFORMS J. Comput. 25(4): 693-700 (2013)\n",
      "1441 Johannes Klaus Fichte:\n",
      "Backdoors to the Tractability of Answer Set Programming. Theory Pract. Log. Program. 13(4-5-Online-Supplement) (2013)\n",
      "1442 Johannes Klaus Fichte:\n",
      "Backdoors to Tractability of Answer-Set Programming. AAAI 2013: 1662-1663\n",
      "1443 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to Normality for Disjunctive Logic Programs. AAAI 2013: 320-327\n",
      "1444 Jonas Zaddach, Anil Kurmus, Davide Balzarotti, Erik-Oliver Blass, Aurélien Francillon, Travis Goodspeed, Moitrayee Gupta, Ioannis Koltsidas:\n",
      "Implementation and implications of a stealth hard-drive backdoor. ACSAC 2013: 279-288\n",
      "1445 Felix Schuster, Thorsten Holz:\n",
      "Towards reducing the attack surface of software backdoors. CCS 2013: 851-862\n",
      "1446 Felix Schuster, Stefan Rüster, Thorsten Holz:\n",
      "Preventing Backdoors in Server Applications with a Separated Software Architecture - (Short Paper). DIMVA 2013: 197-206\n",
      "1447 Serge Gaspers, Stefan Szeider:\n",
      "Strong Backdoors to Bounded Treewidth SAT. FOCS 2013: 489-498\n",
      "1448 Ronan LeBras, Richard Bernstein, Carla P. Gomes, Bart Selman, R. Bruce van Dover:\n",
      "Crowdsourcing Backdoor Identification for Combinatorial Optimization. IJCAI 2013: 2840-2847\n",
      "1449 Andreas Pfandler, Stefan Rümmele, Stefan Szeider:\n",
      "Backdoors to Abduction. IJCAI 2013: 1046-1052\n",
      "1450 Neeldhara Misra, Sebastian Ordyniak, Venkatesh Raman, Stefan Szeider:\n",
      "Upper and Lower Bounds for Weak Backdoor Set Detection. SAT 2013: 394-402\n",
      "1451 Kai Chen, Yingjun Zhang, Yifeng Lian:\n",
      "Vulnerability-Based Backdoors: Threats from Two-step Trojans. SERE 2013: 169-177\n",
      "1452 Serge Gaspers, Sebastian Ordyniak, M. S. Ramanujan, Saket Saurabh, Stefan Szeider:\n",
      "Backdoors to q-Horn. STACS 2013: 67-79\n",
      "1453 Marloes H. Maathuis, Diego Colombo:\n",
      "A generalized backdoor criterion. CoRR abs/1307.5636 (2013)\n",
      "1454 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to Normality for Disjunctive Logic Programs. CoRR abs/1301.1391 (2013)\n",
      "1455 Neeldhara Misra, Sebastian Ordyniak, Venkatesh Raman, Stefan Szeider:\n",
      "Upper and Lower Bounds for Weak Backdoor Set Detection. CoRR abs/1304.5518 (2013)\n",
      "1456 Andreas Pfandler, Stefan Rümmele, Stefan Szeider:\n",
      "Backdoors to Abduction. CoRR abs/1304.5961 (2013)\n",
      "1457 Serge Gaspers, Stefan Szeider:\n",
      "Backdoors to Satisfaction. The Multivariate Algorithmic Revolution and Beyond 2012: 287-317\n",
      "1458 Sergei Skorobogatov, Christopher Woods:\n",
      "Breakthrough Silicon Scanning Discovers Backdoor in Military Chip. CHES 2012: 23-40\n",
      "1459 Serge Gaspers, Stefan Szeider:\n",
      "Backdoors to Acyclic SAT. ICALP (1) 2012: 363-374\n",
      "1460 Stefan Hommes, Radu State, Thomas Engel:\n",
      "Detecting Stealthy Backdoors with Association Rule Mining. Networking (2) 2012: 161-171\n",
      "1461 Serge Gaspers, Stefan Szeider:\n",
      "Strong Backdoors to Nested Satisfiability. SAT 2012: 72-85\n",
      "1462 Shuaifu Dai, Tao Wei, Chao Zhang, Tielei Wang, Yu Ding, Zhenkai Liang, Wei Zou:\n",
      "A Framework to Eliminate Backdoors from Response-Computable Authentication. IEEE Symposium on Security and Privacy 2012: 3-17\n",
      "1463 Serge Gaspers, Stefan Szeider:\n",
      "Strong Backdoors to Nested Satisfiability. CoRR abs/1202.4331 (2012)\n",
      "1464 Serge Gaspers, Stefan Szeider:\n",
      "Strong Backdoors to Bounded Treewidth SAT. CoRR abs/1204.6233 (2012)\n",
      "1465 Zijie Li, Peter van Beek:\n",
      "Finding Small Backdoors in SAT Instances. Canadian AI 2011: 269-280\n",
      "1466 Farinaz Koushanfar:\n",
      "Trusting the open latent IC backdoors. STC@CCS 2011: 1-2\n",
      "1467 Stéphane Huot, Olivier Chapuis, Pierre Dragicevic:\n",
      "TorusDesktop: pointing via the backdoor is sometimes shorter. CHI 2011: 829-838\n",
      "1468 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to Tractable Answer-Set Programming. IJCAI 2011: 863-868\n",
      "1469 Matteo Fischetti, Michele Monaci:\n",
      "Backdoor Branching. IPCO 2011: 183-191\n",
      "1470 Adam Waksman, Simha Sethumadhavan:\n",
      "Silencing Hardware Backdoors. IEEE Symposium on Security and Privacy 2011: 49-63\n",
      "1471 Johannes Klaus Fichte, Stefan Szeider:\n",
      "Backdoors to Tractable Answer-Set Programming. CoRR abs/1104.2788 (2011)\n",
      "1472 Serge Gaspers, Stefan Szeider:\n",
      "Backdoors to Acyclic SAT. CoRR abs/1110.6384 (2011)\n",
      "1473 Serge Gaspers, Stefan Szeider:\n",
      "Backdoors to Satisfaction. CoRR abs/1110.6387 (2011)\n",
      "1474 Chris Wysopal, Chris Eng, Tyler Shields:\n",
      "Static detection of application backdoors - Detecting both malicious software behavior and malicious indicators from the static analysis of executable code. Datenschutz und Datensicherheit 34(3): 149-155 (2010)\n",
      "1475 Hung-Min Sun, Mu-En Wu, Cheng-Ta Yang:\n",
      "Simple Backdoors on RSA Modulus by Using RSA Vulnerability. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 92-A(9): 2326-2332 (2009)\n",
      "1476 Marko Samer, Stefan Szeider:\n",
      "Backdoor Sets of Quantified Boolean Formulas. J. Autom. Reason. 42(1): 77-97 (2009)\n",
      "1477 Stefan Szeider:\n",
      "Matched Formulas and Backdoor Sets. J. Satisf. Boolean Model. Comput. 6(1-3): 1-12 (2009)\n",
      "1478 Loïc Duflot:\n",
      "CPU bugs, CPU backdoors and consequences on security. J. Comput. Virol. 5(2): 91-104 (2009)\n",
      "1479 Sherri Sparks, Shawn Embleton, Cliff Changchun Zou:\n",
      "A chipset level network backdoor: bypassing host-based firewall & IDS. AsiaCCS 2009: 125-134\n",
      "1480 Bistra Dilkina, Carla P. Gomes, Yuri Malitsky, Ashish Sabharwal, Meinolf Sellmann:\n",
      "Backdoors to Combinatorial Optimization: Feasibility and Optimality. CPAIOR 2009: 56-70\n",
      "1481 Jin Taek Kim, Jeong-Ho Kho, Min-Seok Hong, Choul Woong Son, Do-Won Lee, Sang-Jo Youk, Geuk Lee:\n",
      "A study on intrusion protection techniques against Linux kernel backdoor. ICHIT 2009: 86-90\n",
      "1482 Bistra Dilkina, Carla P. Gomes, Ashish Sabharwal:\n",
      "Backdoors in the Context of Learning. SAT 2009: 73-79\n",
      "1483 Marko Samer, Stefan Szeider:\n",
      "Backdoor Trees. AAAI 2008: 363-368\n",
      "1484 Peter Gregory, Maria Fox, Derek Long:\n",
      "A New Empirical Study of Weak Backdoors. CP 2008: 618-623\n",
      "1485 Loïc Duflot:\n",
      "CPU Bugs, CPU Backdoors and Consequences on Security. ESORICS 2008: 580-599\n",
      "1486 Bistra Dilkina, Carla P. Gomes, Ashish Sabharwal:\n",
      "Tradeoffs in Backdoors: Inconsistency Detection, Dynamic Simplification, and Preprocessing. ISAIM 2008\n",
      "1487 Stephan Kottler, Michael Kaufmann, Carsten Sinz:\n",
      "Computation of Renameable Horn Backdoors. SAT 2008: 154-160\n",
      "1488 Stephan Kottler, Michael Kaufmann, Carsten Sinz:\n",
      "A New Bound for an NP-Hard Subclass of 3-SAT Using Backdoors. SAT 2008: 161-167\n",
      "1489 Cheolho Lee, Kiwook Sohn:\n",
      "Detecting and Guarding against Kernel Backdoors through Packet Flow Differentials. IEICE Trans. Commun. 90-B(10): 2638-2645 (2007)\n",
      "1490 Adam L. Young, Moti Yung:\n",
      "A Timing-Resistant Elliptic Curve Backdoor in RSA. Inscrypt 2007: 427-441\n",
      "1491 Bistra Dilkina, Carla P. Gomes, Ashish Sabharwal:\n",
      "Tradeoffs in the Complexity of Backdoor Detection. CP 2007: 256-270\n",
      "1492 Lionel Paris, Richard Ostrowski, Pierre Siegel, Lakhdar Sais:\n",
      "From Horn Strong Backdoor Sets to Ordered Strong Backdoor Sets. MICAI 2007: 105-117\n",
      "1493 Marko Samer, Stefan Szeider:\n",
      "Backdoor Sets of Quantified Boolean Formulas. SAT 2007: 230-243\n",
      "1494 Stefan Szeider:\n",
      "Matched Formulas and Backdoor Sets. SAT 2007: 94-99\n",
      "1495 Rebecca T. Mercuri, Vincent J. Lipsio, Beth Feehan:\n",
      "COTS and other electronic voting backdoors. Commun. ACM 49(11): 112 (2006)\n",
      "1496 Lionel Paris, Richard Ostrowski, Pierre Siegel, Lakhdar Sais:\n",
      "Computing Horn Strong Backdoor Sets Thanks to Local Search. ICTAI 2006: 139-143\n",
      "1497 Adam L. Young, Moti Yung:\n",
      "An Elliptic Curve Backdoor Algorithm for RSASSA. Information Hiding 2006: 355-374\n",
      "1498 Stefan Szeider:\n",
      "Backdoor Sets for DLL Subsolvers. J. Autom. Reason. 35(1-3): 73-88 (2005)\n",
      "1499 Philip Kilby, John K. Slaney, Sylvie Thiébaux, Toby Walsh:\n",
      "Backbones and Backdoors in Satisfiability. AAAI 2005: 1368-1373\n",
      "1500 Adam L. Young, Moti Yung:\n",
      "A Space Efficient Backdoor in RSA and Its Applications. Selected Areas in Cryptography 2005: 128-143\n",
      "1501 Mohammed Fadle Abdulla, C. P. Ravikumar:\n",
      "A self-checking signature scheme for checking backdoor security attacks in Internet. J. High Speed Networks 13(4): 309-317 (2004)\n",
      "1502 Yongshao Ruan, Henry A. Kautz, Eric Horvitz:\n",
      "The Backdoor Key: A Path to Understanding Problem Hardness. AAAI 2004: 124-130\n",
      "1503 Hillevi Sundholm, Henrik Artman, Robert Ramberg:\n",
      "Backdoor Creativity: Collaborative Creativity in Technology Supported Teams. COOP 2004: 99-114\n",
      "1504 Aniruddha Bohra, Iulian Neamtiu, Pascal Gallard, Florin Sultan, Liviu Iftode:\n",
      "Remote Repair of Operating System State Using Backdoors. ICAC 2004: 256-263\n",
      "1505 Naomi Nishimura, Prabhakar Ragde, Stefan Szeider:\n",
      "Detecting Backdoor Sets with Respect to Horn and Binary Clauses. SAT 2004\n",
      "1506 Adam L. Young, Moti Yung:\n",
      "Backdoor Attacks on Black-Box Ciphers Exploiting Low-Entropy Plaintexts. ACISP 2003: 297-311\n",
      "1507 Claude Crépeau, Alain Slakmon:\n",
      "Simple Backdoors for RSA Key Generation. CT-RSA 2003: 403-416\n",
      "1508 Jukka Juslin, Teemupekka Virtanen:\n",
      "Automatic Backdoor Analysis with Network Intrusion Detection System and Integrated Service Checker. IAW 2003: 122-126\n",
      "1509 Ryan Williams, Carla P. Gomes, Bart Selman:\n",
      "Backdoors To Typical Case Complexity. IJCAI 2003: 1173-1178\n",
      "1510 Claude Crépeau, Alain Slakmon:\n",
      "Simple backdoors to RSA key generation. IACR Cryptol. ePrint Arch. 2002: 183 (2002)\n",
      "1511 Yin Zhang, Vern Paxson:\n",
      "Detecting Backdoors. USENIX Security Symposium 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./dblp-raw-1512.txt', 'r') as fw:\n",
    "    data = fw.read()\n",
    "\n",
    "papers = data.split(\"\\n\t\t\")\n",
    "len(data), len(papers)\n",
    "\n",
    "data_papers = {\n",
    "    'title': [],\n",
    "    'author': [],\n",
    "    'info': [],\n",
    "}\n",
    "for idp, paper in enumerate(papers):\n",
    "    print(idp, paper)\n",
    "    # try:\n",
    "    author, title_venue = paper.split(':\\n', 1)\n",
    "    # split title and venue by first '.'\n",
    "    if '? ' in title_venue:\n",
    "        if '. ' in title_venue:\n",
    "            title, venue = title_venue.split('. ', 1)\n",
    "        else:\n",
    "            title, venue = title_venue.split('? ', 1)\n",
    "    else:\n",
    "        title, venue = title_venue.split('. ', 1)\n",
    "\n",
    "    data_papers['title'].append(title.strip())\n",
    "    data_papers['author'].append(author.strip())\n",
    "    data_papers['info'].append(venue.strip())\n",
    "\n",
    "papers_csv = pd.DataFrame(data_papers)\n",
    "papers_csv.to_csv('./tmp/backdoor2405095.csv')\n",
    "papers_csv.to_excel('./tmp/backdoor2405095.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>Backdoor Attacks to Deep Neural Networks: A Su...</td>\n",
       "      <td>Orson Mengara, Anderson R. Avila, Tiago H. Falk</td>\n",
       "      <td>IEEE Access 12: 29004-29023 (2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>A Comprehensive Survey on Backdoor Attacks and...</td>\n",
       "      <td>Quentin Le Roux, Eric Bourbao, Yannick Teglia,...</td>\n",
       "      <td>IEEE Access 12: 47433-47468 (2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>Defending Against Backdoor Attacks by Quaranti...</td>\n",
       "      <td>Chengxu Yu, Yulai Zhang</td>\n",
       "      <td>IEEE Access 12: 10681-10689 (2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>Imperceptible and multi-channel backdoor attack</td>\n",
       "      <td>Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhan...</td>\n",
       "      <td>Appl. Intell. 54(1): 1099-1116 (2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>Universal adversarial backdoor attacks to fool...</td>\n",
       "      <td>Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai</td>\n",
       "      <td>Comput. Secur. 137: 103601 (2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>1225</td>\n",
       "      <td>2003</td>\n",
       "      <td>Simple Backdoors for RSA Key Generation</td>\n",
       "      <td>Claude Crépeau, Alain Slakmon</td>\n",
       "      <td>CT-RSA 2003: 403-416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>1226</td>\n",
       "      <td>2003</td>\n",
       "      <td>Automatic Backdoor Analysis with Network Intru...</td>\n",
       "      <td>Jukka Juslin, Teemupekka Virtanen</td>\n",
       "      <td>IAW 2003: 122-126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>1227</td>\n",
       "      <td>2003</td>\n",
       "      <td>Backdoors To Typical Case Complexity</td>\n",
       "      <td>Ryan Williams, Carla P. Gomes, Bart Selman</td>\n",
       "      <td>IJCAI 2003: 1173-1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>1228</td>\n",
       "      <td>2002</td>\n",
       "      <td>Simple backdoors to RSA key generation</td>\n",
       "      <td>Claude Crépeau, Alain Slakmon</td>\n",
       "      <td>IACR Cryptol. ePrint Arch. 2002: 183 (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>1229</td>\n",
       "      <td>2000</td>\n",
       "      <td>Detecting Backdoors</td>\n",
       "      <td>Yin Zhang, Vern Paxson</td>\n",
       "      <td>USENIX Security Symposium 2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1230 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  pub_time                                              title  \\\n",
       "0              0      2024  Backdoor Attacks to Deep Neural Networks: A Su...   \n",
       "1              1      2024  A Comprehensive Survey on Backdoor Attacks and...   \n",
       "2              2      2024  Defending Against Backdoor Attacks by Quaranti...   \n",
       "3              3      2024    Imperceptible and multi-channel backdoor attack   \n",
       "4              4      2024  Universal adversarial backdoor attacks to fool...   \n",
       "...          ...       ...                                                ...   \n",
       "1225        1225      2003            Simple Backdoors for RSA Key Generation   \n",
       "1226        1226      2003  Automatic Backdoor Analysis with Network Intru...   \n",
       "1227        1227      2003               Backdoors To Typical Case Complexity   \n",
       "1228        1228      2002             Simple backdoors to RSA key generation   \n",
       "1229        1229      2000                                Detecting Backdoors   \n",
       "\n",
       "                                                 author  \\\n",
       "0       Orson Mengara, Anderson R. Avila, Tiago H. Falk   \n",
       "1     Quentin Le Roux, Eric Bourbao, Yannick Teglia,...   \n",
       "2                               Chengxu Yu, Yulai Zhang   \n",
       "3     Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhan...   \n",
       "4           Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai   \n",
       "...                                                 ...   \n",
       "1225                      Claude Crépeau, Alain Slakmon   \n",
       "1226                  Jukka Juslin, Teemupekka Virtanen   \n",
       "1227         Ryan Williams, Carla P. Gomes, Bart Selman   \n",
       "1228                      Claude Crépeau, Alain Slakmon   \n",
       "1229                             Yin Zhang, Vern Paxson   \n",
       "\n",
       "                                             info  \n",
       "0              IEEE Access 12: 29004-29023 (2024)  \n",
       "1              IEEE Access 12: 47433-47468 (2024)  \n",
       "2              IEEE Access 12: 10681-10689 (2024)  \n",
       "3           Appl. Intell. 54(1): 1099-1116 (2024)  \n",
       "4               Comput. Secur. 137: 103601 (2024)  \n",
       "...                                           ...  \n",
       "1225                         CT-RSA 2003: 403-416  \n",
       "1226                            IAW 2003: 122-126  \n",
       "1227                        IJCAI 2003: 1173-1178  \n",
       "1228  IACR Cryptol. ePrint Arch. 2002: 183 (2002)  \n",
       "1229               USENIX Security Symposium 2000  \n",
       "\n",
       "[1230 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_venue = {}\n",
    "for venue in papers_csv['info']:\n",
    "    venue_txt = ''\n",
    "    if 'corr ' in venue.lower():\n",
    "        venue_txt = 'CoRR'\n",
    "    else:\n",
    "        venue_txt = venue.split(':')[0]\n",
    "    if venue_txt not in dict_venue:\n",
    "        dict_venue[venue_txt] = 0\n",
    "    dict_venue[venue_txt] += 1\n",
    "print(json.dumps(dict_venue, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_duplicate = {}\n",
    "\n",
    "for _, data_row in papers_csv.iterrows():\n",
    "    # title = data_row['title']\n",
    "    # tt = title.lower()\n",
    "    tt = data_row['title'].lower()\n",
    "    if tt not in cnt_duplicate:\n",
    "        cnt_duplicate[tt] = []\n",
    "    cnt_duplicate[tt].append(data_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_year(venue_text):\n",
    "    year = 1900\n",
    "    year_match = re.search(r'\\((\\d{4})\\)', venue_text)\n",
    "    if year_match:\n",
    "        year = year_match.group(1)\n",
    "    else:\n",
    "        year_match = re.search(r'\\d{4}(?=:)', venue_text)\n",
    "        if year_match:\n",
    "            year = year_match.group()\n",
    "        else:\n",
    "            year = venue_text[-4:]\n",
    "    return int(year) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "paper_filters = {\n",
    "    'pub_time': [],\n",
    "    'title': [],\n",
    "    'author': [],\n",
    "    'info': [],\n",
    "}\n",
    "\n",
    "\n",
    "for title, value in cnt_duplicate.items():\n",
    "    if len(value) > 1:\n",
    "\n",
    "        for v in value:\n",
    "            # print( v['info'], v['title'], v['author'])\n",
    "            if 'corr' in v['info'].lower():\n",
    "                continue\n",
    "            pub_time = get_year(v['info'])\n",
    "            paper_filters['pub_time'].append(pub_time)\n",
    "            paper_filters['title'].append(v['title'])\n",
    "            paper_filters['author'].append(v['author'])\n",
    "            paper_filters['info'].append(v['info'])\n",
    "\n",
    "    else:\n",
    "        v = value[0]\n",
    "        pub_time = get_year(v['info'])\n",
    "        paper_filters['pub_time'].append(pub_time)\n",
    "        paper_filters['title'].append(v['title'])\n",
    "        paper_filters['author'].append(v['author'])\n",
    "        paper_filters['info'].append(v['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230\n"
     ]
    }
   ],
   "source": [
    "print(len(paper_filters['info']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper = pd.DataFrame(paper_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper.to_csv('./tmp/backdoor_clean_2405095.csv')\n",
    "df_paper.to_excel('./tmp/backdoo_clean_2405095.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey 9\n",
      "Federated Learning (FL) 136\n",
      "Natural Language Processing (NLP) 56\n",
      "Automatic Speech Recognition (ASR) 16\n",
      "Watermarking 21\n",
      "Reinforcement Learning 15\n",
      "Few-Shot Learning 7\n",
      "Graph Learning 44\n",
      "3D 10\n",
      "Unlearning 8\n",
      "Physical Backdoor 15\n",
      "Diffusion 13\n",
      "Transformer 9\n",
      "Clean-label Backdoor 14\n",
      "Video 3\n",
      "Segmentation 5\n",
      "Machine Learning (ML) 18\n",
      "RSA 54\n",
      "Others 777\n"
     ]
    }
   ],
   "source": [
    "keywords = {\n",
    "    'Survey': ['survey'],\n",
    "    'Federated Learning (FL)': ['federated'],\n",
    "    'Natural Language Processing (NLP)': ['language', 'nlp'],\n",
    "    'Automatic Speech Recognition (ASR)': ['speech', 'audio', 'voice'],\n",
    "    'Watermarking': ['watermarking', 'watermarks'],\n",
    "    'Reinforcement Learning': ['reinforcement learning'],\n",
    "    'Few-Shot Learning': ['few-shot', 'zero-shot'],\n",
    "    'Graph Learning': ['graph'],\n",
    "    '3D': ['3d'],\n",
    "    'Unlearning': ['unlearning'],\n",
    "    'Physical Backdoor': ['physical'],\n",
    "    'Diffusion': ['diffusion'],\n",
    "    'Transformer': ['transformer'],\n",
    "    'Clean-label Backdoor': ['clean-label'],\n",
    "    'Video': ['video'],\n",
    "    'Segmentation': ['segmentation'],\n",
    "    'Machine Learning (ML)': ['machine learning'],\n",
    "    'RSA': ['rsa'],\n",
    "    'Others': [],\n",
    "}\n",
    "\n",
    "data_cat = {k: [] for k in keywords.keys()}\n",
    "\n",
    "for _, paper in df_paper.iterrows():\n",
    "    default_cat = 'Others'\n",
    "    for cat in data_cat.keys():\n",
    "        kw =  keywords[cat]\n",
    "        for w in kw:\n",
    "            if w in paper['title'].lower():\n",
    "                default_cat = cat\n",
    "        if default_cat != 'Others':\n",
    "            break\n",
    "    data_cat[default_cat].append(paper)\n",
    "for k in data_cat:\n",
    "    print(k, len(data_cat[k]))\n",
    "\n",
    "for id, pp in enumerate(data_cat['Others']):\n",
    "    print(f\"Other {id}: {pp['title']} -- {pp['info']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, pp in enumerate(data_cat['Others']):\n",
    "    print(f\"Other {id}: {pp['title']} -- {pp['info']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backdoor_md = \"\"\n",
    "\n",
    "for kk, vv in data_cat.items():\n",
    "\n",
    "    data_to_md = f'# {kk}\\n'\n",
    "    data_to_md += '|No. | Title | Venue | Year | Author |\\n'\n",
    "    data_to_md += '|----|-------|-------|------|--------|\\n'\n",
    "\n",
    "    for id, pp in enumerate(vv):\n",
    "        # print(f\"Other {id}: {pp['title']} -- {pp['info']}\")\n",
    "        # data_to_md += f\"| {id + 1} | {pp['title']} | {pp['info']} | {pp['pub_time']} | {pp['author']} |\\n\"\n",
    "        data_to_md += f\"| {id + 1} | {pp['title']} | {pp['info']} | {pp['pub_time']} | {pp['author']} |\\n\"\n",
    "\n",
    "    # print(data_to_md)\n",
    "    data_backdoor_md += data_to_md\n",
    "    \n",
    "with open('./tmp/bd_paper.md', \"w\") as fw:\n",
    "    fw.write(data_backdoor_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey\n",
      "Federated Learning (ML)\n",
      "Natural language processing (NLP)\n",
      "Automatic speech recognition (ASR)\n",
      "Watermarking\n",
      "Reinforcement Learning\n",
      "Few-Shot Learning\n",
      "Graph Learning\n",
      "3D\n",
      "Unlearning\n",
      "Physical Backdoor\n",
      "Diffusion\n",
      "Transformer\n",
      "Clean-label Backdoor\n",
      "Video\n",
      "Segmentation\n",
      "RSA\n",
      "Machine Learning (ML)\n",
      "Others\n"
     ]
    }
   ],
   "source": [
    "for kk, vv in data_cat.items():\n",
    "    print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML 0: Backdoor Attacks to Deep Neural Networks: A Survey of the Literature, Challenges, and Future Research Directions -- IEEE Access 12: 29004-29023 (2024)\n",
      "ML 1: A Comprehensive Survey on Backdoor Attacks and Their Defenses in Face Recognition Systems -- IEEE Access 12: 47433-47468 (2024)\n",
      "ML 2: Defending Against Backdoor Attacks by Quarantine Training -- IEEE Access 12: 10681-10689 (2024)\n",
      "ML 3: Imperceptible and multi-channel backdoor attack -- Appl. Intell. 54(1): 1099-1116 (2024)\n",
      "ML 4: SGBA: A stealthy scapegoat backdoor attack against deep neural networks -- Comput. Secur. 136: 103523 (2024)\n",
      "ML 5: Backdoor Attacks on Graph Neural Networks Trained with Data Augmentation -- IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 107(3): 355-358 (2024)\n",
      "ML 6: Enrollment-Stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound -- IEEE Internet Things J. 11(8): 13108-13124 (2024)\n",
      "ML 7: Effective Backdoor Attack on Graph Neural Networks in Spectral Domain -- IEEE Internet Things J. 11(7): 12102-12114 (2024)\n",
      "ML 8: NLPSweep: A comprehensive defense scheme for mitigating NLP backdoor attacks -- Inf. Sci. 661: 120176 (2024)\n",
      "ML 9: The reality of backdoored S-Boxes - An eye opener -- J. Inf. Secur. Appl. 80: 103674 (2024)\n",
      "ML 10: The Reality of Backdoored S-Boxes - An Eye Opener -- IACR Cryptol. ePrint Arch. 2023: 1073 (2023)\n",
      "ML 11: TridentShell: An enhanced covert and scalable backdoor injection attack on web applications -- J. Netw. Comput. Appl. 223: 103823 (2024)\n",
      "ML 12: A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving -- IEEE J. Sel. Areas Commun. 42(4): 948-962 (2024)\n",
      "ML 13: Detection of backdoor attacks using targeted universal adversarial perturbations for deep neural networks -- J. Syst. Softw. 207: 111859 (2024)\n",
      "ML 14: Invisible backdoor learning in regional transform domain -- Neural Comput. Appl. 36(14): 8097-8108 (2024)\n",
      "ML 15: Backdoor advertising scandals, Yingyeo culture, and cancel culture among YouTube Influencers in South Korea -- New Media Soc. 26(1): 405-425 (2024)\n",
      "ML 16: Enhanced Coalescence Backdoor Attack Against DNN Based on Pixel Gradient -- Neural Process. Lett. 56(2): 114 (2024)\n",
      "ML 17: SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers -- Pattern Recognit. Lett. 177: 103-109 (2024)\n",
      "ML 18: Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs -- IEEE Trans. Comput. Soc. Syst. 11(2): 2479-2493 (2024)\n",
      "ML 19: Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection -- IEEE Trans. Comput. Soc. Syst. 11(2): 1816-1831 (2024)\n",
      "ML 20: Incremental Learning, Incremental Backdoor Threats -- IEEE Trans. Dependable Secur. Comput. 21(2): 559-572 (2024)\n",
      "ML 21: Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network -- IEEE Trans. Inf. Forensics Secur. 19: 280-292 (2024)\n",
      "ML 22: MBA: Backdoor Attacks Against 3D Mesh Classifier -- IEEE Trans. Inf. Forensics Secur. 19: 2127-2142 (2024)\n",
      "ML 23: Imperceptible and Robust Backdoor Attack in 3D Point Cloud -- IEEE Trans. Inf. Forensics Secur. 19: 1267-1282 (2024)\n",
      "ML 24: Universal Detection of Backdoor Attacks via Density-Based Clustering and Centroids Analysis -- IEEE Trans. Inf. Forensics Secur. 19: 970-984 (2024)\n",
      "ML 25: Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers -- IEEE Trans. Inf. Forensics Secur. 19: 708-721 (2024)\n",
      "ML 26: NTD: Non-Transferability Enabled Deep Learning Backdoor Detection -- IEEE Trans. Inf. Forensics Secur. 19: 104-119 (2024)\n",
      "ML 27: On Model Outsourcing Adaptive Attacks to Deep Learning Backdoor Defenses -- IEEE Trans. Inf. Forensics Secur. 19: 2356-2369 (2024)\n",
      "ML 28: Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures -- IEEE Trans. Inf. Forensics Secur. 19: 455-468 (2024)\n",
      "ML 29: Invisible Backdoor Attack With Dynamic Triggers Against Person Re-Identification -- IEEE Trans. Inf. Forensics Secur. 19: 307-319 (2024)\n",
      "ML 30: Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger -- IEEE Trans. Ind. Informatics 20(3): 5004-5013 (2024)\n",
      "ML 31: BadCM: Invisible Backdoor Attack Against Cross-Modal Learning -- IEEE Trans. Image Process. 33: 2558-2571 (2024)\n",
      "ML 32: Critical Path-Based Backdoor Detection for Deep Neural Networks -- IEEE Trans. Neural Networks Learn. Syst. 35(3): 4032-4046 (2024)\n",
      "ML 33: Backdoor Learning: A Survey -- IEEE Trans. Neural Networks Learn. Syst. 35(1): 5-22 (2024)\n",
      "ML 34: Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction -- IEEE Trans. Netw. Sci. Eng. 11(1): 525-542 (2024)\n",
      "ML 35: Stealthy Backdoor Attack for Code Models -- IEEE Trans. Software Eng. 50(4): 721-741 (2024)\n",
      "ML 36: BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract) -- AAAI 2024: 23506-23507\n",
      "ML 37: Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models -- AAAI 2024: 21169-21178\n",
      "ML 38: Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift -- AAAI 2024: 10847-10855\n",
      "ML 39: Progressive Poisoned Data Isolation for Training-Time Backdoor Defense -- AAAI 2024: 11425-11433\n",
      "ML 40: BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning -- AAAI 2024: 11687-11694\n",
      "ML 41: Conditional Backdoor Attack via JPEG Compression -- AAAI 2024: 19823-19831\n",
      "ML 42: Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain -- AAAI 2024: 21072-21080\n",
      "ML 43: Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations -- AAAI 2024: 11944-11952\n",
      "ML 44: A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives -- AAAI 2024: 1851-1859\n",
      "ML 45: COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks -- AAAI 2024: 2436-2444\n",
      "ML 46: Temporal-Distributed Backdoor Attack against Video Based Action Recognition -- AAAI 2024: 3199-3207\n",
      "ML 47: Does Few-Shot Learning Suffer from Backdoor Attacks -- AAAI 2024: 19893-19901\n",
      "ML 48: Backdoor Attacks via Machine Unlearning -- AAAI 2024: 14115-14123\n",
      "ML 49: Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks -- AAAI 2024: 274-282\n",
      "ML 50: Chronic Poisoning: Backdoor Attack against Split Learning -- AAAI 2024: 16531-16538\n",
      "ML 51: UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation -- AAAI 2024: 21823-21831\n",
      "ML 52: DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models -- AAAI 2024: 21850-21858\n",
      "ML 53: SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly -- AAAI 2024: 7766-7774\n",
      "ML 54: Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems -- ANZCC 2024: 115-120\n",
      "ML 55: Backdoor Attacks and Generative Model Fairness: Current Trends and Future Research Directions -- COMSNETS 2024: 31-36\n",
      "ML 56: Detecting Backdoors Embedded in Ensembles -- ICEIC 2024: 1-3\n",
      "ML 57: Defense Method Challenges Against Backdoor Attacks in Neural Networks -- ICAIIC 2024: 396-400\n",
      "ML 58: Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks -- ICISSP 2024: 510-521\n",
      "ML 59: Backdoor Attack Against One-Class Sequential Anomaly Detection Models -- PAKDD (3) 2024: 262-274\n",
      "ML 60: On the Possibility of a Backdoor in the Micali-Schnorr Generator -- Public Key Cryptography (1) 2024: 352-386\n",
      "ML 61: On the Possibility of a Backdoor in the Micali-Schnorr Generator -- IACR Cryptol. ePrint Arch. 2023: 440 (2023)\n",
      "ML 62: A Closer Look at Robustness of Vision Transformers to Backdoor Attacks -- WACV 2024: 3862-3871\n",
      "ML 63: SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection -- CoRR abs/2401.00137 (2024)\n",
      "ML 64: A clean-label graph backdoor attack method in node classification task -- CoRR abs/2401.00163 (2024)\n",
      "ML 65: Is It Possible to Backdoor Face Forgery Detection with Natural Triggers -- CoRR abs/2401.00414 (2024)\n",
      "ML 66: Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control -- CoRR abs/2401.01085 (2024)\n",
      "ML 67: The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers -- CoRR abs/2401.01537 (2024)\n",
      "ML 68: Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP -- CoRR abs/2401.01911 (2024)\n",
      "ML 69: Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack -- CoRR abs/2401.02031 (2024)\n",
      "ML 70: Object-Oriented Backdoor Attack Against Image Captioning -- ICASSP 2022: 2864-2868\n",
      "ML 71: MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack -- CoRR abs/2401.02659 (2024)\n",
      "ML 72: A backdoor attack against link prediction tasks with graph neural networks -- CoRR abs/2401.02663 (2024)\n",
      "ML 73: End-to-End Anti-Backdoor Learning on Images and Time Series -- CoRR abs/2401.03215 (2024)\n",
      "ML 74: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline -- CoRR abs/2401.04136 (2024)\n",
      "ML 75: TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks -- CoRR abs/2401.05432 (2024)\n",
      "ML 76: Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks -- CoRR abs/2401.05949 (2024)\n",
      "ML 77: Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation -- CoRR abs/2401.06030 (2024)\n",
      "ML 78: Learning Backdoors for Mixed Integer Programs with Contrastive Learning -- CoRR abs/2401.10467 (2024)\n",
      "ML 79: BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models -- CoRR abs/2401.12242 (2024)\n",
      "ML 80: WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition -- CoRR abs/2401.13578 (2024)\n",
      "ML 81: BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning -- CoRR abs/2401.15002 (2024)\n",
      "ML 82: Multi-Trigger Backdoor Attacks: More Triggers, More Threats -- CoRR abs/2401.15295 (2024)\n",
      "ML 83: TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability -- CoRR abs/2401.15883 (2024)\n",
      "ML 84: Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks -- CoRR abs/2402.02034 (2024)\n",
      "ML 85: DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models -- CoRR abs/2402.02739 (2024)\n",
      "ML 86: The last Dance : Robust backdoor attack via diffusion models and bayesian approach -- CoRR abs/2402.05967 (2024)\n",
      "ML 87: Architectural Neural Backdoors from First Principles -- CoRR abs/2402.06957 (2024)\n",
      "ML 88: OrderBkd: Textual backdoor attack through repositioning -- CoRR abs/2402.07689 (2024)\n",
      "ML 89: Test-Time Backdoor Attacks on Multimodal Large Language Models -- CoRR abs/2402.08577 (2024)\n",
      "ML 90: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents -- CoRR abs/2402.11208 (2024)\n",
      "ML 91: Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection -- CoRR abs/2402.11473 (2024)\n",
      "ML 92: Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space -- CoRR abs/2402.12026 (2024)\n",
      "ML 93: Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning -- CoRR abs/2402.12168 (2024)\n",
      "ML 94: Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation -- CoRR abs/2402.13532 (2024)\n",
      "ML 95: VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models -- CoRR abs/2402.13851 (2024)\n",
      "ML 96: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment -- CoRR abs/2402.14968 (2024)\n",
      "ML 97: Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models -- CoRR abs/2402.14977 (2024)\n",
      "ML 98: Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm -- CoRR abs/2402.15653 (2024)\n",
      "ML 99: On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem -- CoRR abs/2402.16926 (2024)\n",
      "ML 100: Model X-ray: Detect Backdoored Models via Decision Boundary -- CoRR abs/2402.17465 (2024)\n",
      "ML 101: Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks -- CoRR abs/2402.18718 (2024)\n",
      "ML 102: Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models -- CoRR abs/2402.18945 (2024)\n",
      "ML 103: Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge -- CoRR abs/2402.19334 (2024)\n",
      "ML 104: WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection -- CoRR abs/2403.01472 (2024)\n",
      "ML 105: A general approach to enhance the survivability of backdoor attacks by decision path coupling -- CoRR abs/2403.02950 (2024)\n",
      "ML 106: On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder -- CoRR abs/2403.03846 (2024)\n",
      "ML 107: MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror -- CoRR abs/2403.05847 (2024)\n",
      "ML 108: AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration -- CoRR abs/2403.06430 (2024)\n",
      "ML 109: Real is not True: Backdoor Attacks Against Deepfake Detection -- CoRR abs/2403.06610 (2024)\n",
      "ML 110: Backdoor Attack with Mode Mixture Latent Modification -- CoRR abs/2403.07463 (2024)\n",
      "ML 111: Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks -- CoRR abs/2403.08208 (2024)\n",
      "ML 112: REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography -- CoRR abs/2403.09352 (2024)\n",
      "ML 113: Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency -- CoRR abs/2403.10717 (2024)\n",
      "ML 114: Impart: An Imperceptible and Effective Label-Specific Backdoor Attack -- CoRR abs/2403.13017 (2024)\n",
      "ML 115: Invisible Backdoor Attack Through Singular Value Decomposition -- CoRR abs/2403.13018 (2024)\n",
      "ML 116: BadEdit: Backdooring large language models by model editing -- CoRR abs/2403.13355 (2024)\n",
      "ML 117: Clean-image Backdoor Attacks -- CoRR abs/2403.15010 (2024)\n",
      "ML 118: An Embarrassingly Simple Defense Against Backdoor Attacks On SSL -- CoRR abs/2403.15918 (2024)\n",
      "ML 119: Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning -- CoRR abs/2403.16257 (2024)\n",
      "ML 120: Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion -- CoRR abs/2403.16365 (2024)\n",
      "ML 121: Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors -- CoRR abs/2403.16569 (2024)\n",
      "ML 122: Task-Agnostic Detector for Insertion-Based Backdoor Attacks -- CoRR abs/2403.17155 (2024)\n",
      "ML 123: LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning -- CoRR abs/2403.17188 (2024)\n",
      "ML 124: Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs -- CoRR abs/2403.18136 (2024)\n",
      "ML 125: A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks -- CoRR abs/2404.00076 (2024)\n",
      "ML 126: Privacy Backdoors: Stealing Data with Corrupted Pretrained Models -- CoRR abs/2404.00473 (2024)\n",
      "ML 127: UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models -- CoRR abs/2404.01101 (2024)\n",
      "ML 128: Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models -- CoRR abs/2404.01231 (2024)\n",
      "ML 129: Highly-Effective Backdoors for Hash Functions and Beyond -- IACR Cryptol. ePrint Arch. 2024: 536 (2024)\n",
      "ML 130: Towards a Robust Defense: A Multifaceted Approach to the Detection and Mitigation of Neural Backdoor Attacks through Feature Space Exploration and Analysis -- Old Dominion University, Norfolk, Virginia, USA, 2023\n",
      "ML 131: Instance-Agnostic and Practical Clean Label Backdoor Attack Method for Deep Learning Based Face Recognition Models -- IEEE Access 11: 144040-144050 (2023)\n",
      "ML 132: Compression-resistant backdoor attack against deep neural networks -- Appl. Intell. 53(17): 20402-20417 (2023)\n",
      "ML 133: Universal backdoor attack on deep neural networks for malware detection -- Appl. Soft Comput. 143: 110389 (2023)\n",
      "ML 134: Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces -- Sci. China Inf. Sci. 66(8) (2023)\n",
      "ML 135: Data Poisoning and Backdoor Attacks on Audio Intelligence Systems -- IEEE Commun. Mag. 61(12): 176-182 (2023)\n",
      "ML 136: DIHBA: Dynamic, invisible and high attack success rate boundary backdoor attack with low poison ratio -- Comput. Secur. 129: 103212 (2023)\n",
      "ML 137: Object-free backdoor attack and defense on semantic segmentation -- Comput. Secur. 132: 103365 (2023)\n",
      "ML 138: Towards Backdoor Attacks and Defense in Robust Machine Learning Models -- Comput. Secur. 127: 103101 (2023)\n",
      "ML 139: A New Idea for RSA Backdoors -- Cryptogr. 7(3): 45 (2023)\n",
      "ML 140: DLP: towards active defense against backdoor attacks with decoupled learning process -- Cybersecur. 6(1): 9 (2023)\n",
      "ML 141: NBA: defensive distillation for backdoor removal via neural behavior alignment -- Cybersecur. 6(1): 20 (2023)\n",
      "ML 142: A Textual Backdoor Defense Method Based on Deep Feature Classification -- Entropy 25(2): 220 (2023)\n",
      "ML 143: Backdoor Attack against Face Sketch Synthesis -- Entropy 25(7): 974 (2023)\n",
      "ML 144: A defense method against backdoor attacks on neural networks -- Expert Syst. Appl. 213(Part): 118990 (2023)\n",
      "ML 145: Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-level Backdoor Attacks -- Mach. Intell. Res. 20(2): 180-193 (2023)\n",
      "ML 146: Feature-Based Graph Backdoor Attack in the Node Classification Task -- Int. J. Intell. Syst. 2023: 1-13 (2023)\n",
      "ML 147: A lightweight backdoor defense framework based on image inpainting -- Neurocomputing 537: 22-36 (2023)\n",
      "ML 148: A Triggerless Backdoor Attack and Defense Mechanism for Intelligent Task Offloading in Multi-UAV Systems -- IEEE Internet Things J. 10(7): 5719-5732 (2023)\n",
      "ML 149: Backdoor-Resistant Public Data Integrity Verification Scheme Based on Smart Contracts -- IEEE Internet Things J. 10(16): 14269-14284 (2023)\n",
      "ML 150: Turning backdoors for efficient privacy protection against image retrieval violations -- Inf. Process. Manag. 60(5): 103471 (2023)\n",
      "ML 151: Unlabeled backdoor poisoning on trained-from-scratch semi-supervised learning -- Inf. Sci. 647: 119453 (2023)\n",
      "ML 152: Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias -- Inf. Sci. 643: 119171 (2023)\n",
      "ML 153: Detecting backdoor in deep neural networks via intentional adversarial perturbations -- Inf. Sci. 634: 564-577 (2023)\n",
      "ML 154: Multidomain active defense: Detecting multidomain backdoor poisoned samples via ALL-to-ALL decoupling training without clean datasets -- Neural Networks 168: 350-362 (2023)\n",
      "ML 155: How to backdoor split learning -- Neural Networks 168: 326-336 (2023)\n",
      "ML 156: Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey -- IEEE Open J. Comput. Soc. 4: 134-146 (2023)\n",
      "ML 157: Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses -- IEEE Trans. Pattern Anal. Mach. Intell. 45(2): 1563-1580 (2023)\n",
      "ML 158: Backdoor attacks against deep reinforcement learning based traffic signal control systems -- Peer Peer Netw. Appl. 16(1): 466-474 (2023)\n",
      "ML 159: TAT: Targeted backdoor attacks against visual object tracking -- Pattern Recognit. 142: 109629 (2023)\n",
      "ML 160: Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks -- Pattern Recognit. 139: 109512 (2023)\n",
      "ML 161: Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models -- Pattern Recognit. 144: 109844 (2023)\n",
      "ML 162: Robust Feature-Guided Generative Adversarial Network for Aerial Image Semantic Segmentation against Backdoor Attacks -- Remote. Sens. 15(10): 2580 (2023)\n",
      "ML 163: Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface -- Sensors 23(10): 4742 (2023)\n",
      "ML 164: Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface -- ASHES@CCS 2021: 63-72\n",
      "ML 165: Did You Train on My Dataset? Towards Public Dataset Protection with CleanLabel Backdoor Watermarking -- SIGKDD Explor. 25(1): 43-53 (2023)\n",
      "ML 166: Backdoor Pony: Evaluating backdoor attacks and defenses in different domains -- SoftwareX 22: 101387 (2023)\n",
      "ML 167: Stealthy Frequency-Domain Backdoor Attacks: Fourier Decomposition and Fundamental Frequency Injection -- IEEE Signal Process. Lett. 30: 1677-1681 (2023)\n",
      "ML 168: Stealthy Backdoor Attack Against Speaker Recognition Using Phase-Injection Hidden Trigger -- IEEE Signal Process. Lett. 30: 1057-1061 (2023)\n",
      "ML 169: $\\tt{PoisonedGNN}$: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems -- IEEE Trans. Computers 72(10): 2822-2834 (2023)\n",
      "ML 170: An Imperceptible Data Augmentation Based Blackbox Clean-Label Backdoor Attack on Deep Neural Networks -- IEEE Trans. Circuits Syst. I Regul. Pap. 70(12): 5011-5024 (2023)\n",
      "ML 171: FooBaR: Fault Fooling Backdoor Attack on Neural Network Training -- IEEE Trans. Dependable Secur. Comput. 20(3): 1895-1908 (2023)\n",
      "ML 172: MARNet: Backdoor Attacks Against Cooperative Multi-Agent Reinforcement Learning -- IEEE Trans. Dependable Secur. Comput. 20(5): 4188-4198 (2023)\n",
      "ML 173: Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks With RGB Filters -- IEEE Trans. Dependable Secur. Comput. 20(6): 4993-5004 (2023)\n",
      "ML 174: A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection -- IEEE Trans. Dependable Secur. Comput. 20(6): 4752-4762 (2023)\n",
      "ML 175: Can We Mitigate Backdoor Attack Using Adversarial Detection Methods? IEEE Trans -- Dependable Secur. Comput. 20(4): 2867-2881 (2023)\n",
      "ML 176: Enhancing Backdoor Attacks With Multi-Level MMD Regularization -- IEEE Trans. Dependable Secur. Comput. 20(2): 1675-1686 (2023)\n",
      "ML 177: Backdoor Attacks for Remote Sensing Data With Wavelet Transform -- IEEE Trans. Geosci. Remote. Sens. 61: 1-15 (2023)\n",
      "ML 178: Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection -- IEEE Trans. Inf. Forensics Secur. 18: 4668-4680 (2023)\n",
      "ML 179: Black-Box Dataset Ownership Verification via Backdoor Watermarking -- IEEE Trans. Inf. Forensics Secur. 18: 2318-2332 (2023)\n",
      "ML 180: B3: Backdoor Attacks against Black-box Machine Learning Models -- ACM Trans. Priv. Secur. 26(4): 43:1-43:24 (2023)\n",
      "ML 181: Bayesian Causal Bandits with Backdoor Adjustment Prior -- Trans. Mach. Learn. Res. 2023 (2023)\n",
      "ML 182: Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts -- ACM Trans. Softw. Eng. Methodol. 32(3): 61:1-61:24 (2023)\n",
      "ML 183: Hidden Backdoor Attack Against Deep Learning-Based Wireless Signal Modulation Classifiers -- IEEE Trans. Veh. Technol. 72(9): 12396-12400 (2023)\n",
      "ML 184: A stealthy and robust backdoor attack via frequency domain transform -- World Wide Web (WWW) 26(5): 2767-2783 (2023)\n",
      "ML 185: Defending Backdoor Attacks on Vision Transformer via Patch Processing -- AAAI 2023: 506-515\n",
      "ML 186: Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks -- SafeAI@AAAI 2023\n",
      "ML 187: Poisoning-Based Backdoor Attacks in Computer Vision -- AAAI 2023: 16121-16122\n",
      "ML 188: Towards Understanding How Self-training Tolerates Data Backdoor Poisoning -- SafeAI@AAAI 2023\n",
      "ML 189: Probabilistic Generalization of Backdoor Trees with Application to SAT -- AAAI 2023: 4095-4103\n",
      "ML 190: Defending against Backdoor Attacks in Natural Language Generation -- AAAI 2023: 5257-5265\n",
      "ML 191: BITE: Textual Backdoor Attacks with Iterative Trigger Injection -- ACL (1) 2023: 12951-12968\n",
      "ML 192: A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning -- ACL (1) 2023: 3508-3520\n",
      "ML 193: Defending against Insertion-based Textual Backdoor Attacks via Attribution -- ACL (Findings) 2023: 8818-8833\n",
      "ML 194: Multi-target Backdoor Attacks for Code Pre-trained Models -- ACL (1) 2023: 7236-7254\n",
      "ML 195: Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models -- ACL (Findings) 2023: 3850-3868\n",
      "ML 196: NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models -- ACL (1) 2023: 15551-15565\n",
      "ML 197: Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark -- ACL (1) 2023: 7653-7668\n",
      "ML 198: Backdooring Neural Code Search -- ACL (1) 2023: 9692-9708\n",
      "ML 199: Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias -- ACL (Findings) 2023: 2495-2517\n",
      "ML 200: Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks -- ACM TUR-C 2023: 145-147\n",
      "ML 201: PerCBA: Persistent Clean-label Backdoor Attacks on Semi-Supervised Graph Node Classification -- AISafety/SafeRL@IJCAI 2023\n",
      "ML 202: DFaP: Data Filtering and Purification Against Backdoor Attacks -- AIS&P (1) 2023: 81-97\n",
      "ML 203: SSL-ABD : An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning -- AIS&P (1) 2023: 456-467\n",
      "ML 204: CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses -- AsiaCCS 2023: 938-950\n",
      "ML 205: DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation -- AsiaCCS 2023: 731-745\n",
      "ML 206: Joint Energy-Based Model for Robust Speech Classification System Against Dirty-Label Backdoor Poisoning Attacks -- ASRU 2023: 1-8\n",
      "ML 207: SolScope: Effectively Hunting Potential Permission Backdoor Threats in Smart Contracts -- BIGCOM 2023: 88-95\n",
      "ML 208: Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning -- BMVC 2023: 172-173\n",
      "ML 209: Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors -- AISec@CCS 2023: 209-220\n",
      "ML 210: Poster: Fooling XAI with Explanation-Aware Backdoors -- CCS 2023: 3612-3614\n",
      "ML 211: Poster: Backdoor Attack on Extreme Learning Machines -- CCS 2023: 3588-3590\n",
      "ML 212: Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks -- CCS 2023: 3570-3572\n",
      "ML 213: Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information -- CCS 2023: 771-785\n",
      "ML 214: Physical Backdoor Trigger Activation of Autonomous Vehicle Using Reachability Analysis -- CDC 2023: 821-826\n",
      "ML 215: Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks -- CIKM 2023: 608-618\n",
      "ML 216: TRGE: A Backdoor Detection After Quantization -- Inscrypt (2) 2023: 394-398\n",
      "ML 217: Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks -- CISS 2023: 1-6\n",
      "ML 218: Detecting Backdoors in Collaboration Graphs of Software Repositories -- CODASPY 2023: 189-200\n",
      "ML 219: Blind Concealment from Reconstruction-based Attack Detectors for Industrial Control Systems via Backdoor Attacks -- CPSS@AsiaCCS 2023: 36-47\n",
      "ML 220: Detecting Backdoors in Pre-trained Encoders -- CVPR 2023: 16352-16362\n",
      "ML 221: Backdoor Cleansing with Unlabeled Data -- CVPR 2023: 12218-12227\n",
      "ML 222: Architectural Backdoors in Neural Networks -- CVPR 2023: 24595-24604\n",
      "ML 223: The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection -- CVPR 2023: 24585-24594\n",
      "ML 224: How to Backdoor Diffusion Models -- CVPR 2023: 4015-4024\n",
      "ML 225: Backdoor Defense via Adaptively Splitting Poisoned Dataset -- CVPR 2023: 4005-4014\n",
      "ML 226: Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs -- CVPR Workshops 2023: 2338-2345\n",
      "ML 227: Color Backdoor: A Robust Poisoning Attack in Color Space -- CVPR 2023: 8133-8142\n",
      "ML 228: Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency -- CVPR 2023: 16363-16372\n",
      "ML 229: Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks -- CVPR 2023: 20495-20503\n",
      "ML 230: Single Image Backdoor Inversion via Robust Smoothed Classifiers -- CVPR 2023: 8113-8122\n",
      "ML 231: Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning -- CVPR 2023: 12239-12249\n",
      "ML 232: MEDIC: Remove Model Backdoors via Importance Driven Cloning -- CVPR 2023: 20485-20494\n",
      "ML 233: Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger -- CVPR 2023: 12250-12259\n",
      "ML 234: You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks -- CVPR 2023: 24605-24615\n",
      "ML 235: Backdoor Defense via Deconfounded Representation Learning -- CVPR 2023: 12228-12238\n",
      "ML 236: Don't Knock! Rowhammer at the Backdoor of DNN Models -- DSN 2023: 109-122\n",
      "ML 237: XGBD: Explanation-Guided Graph Backdoor Detection -- ECAI 2023: 932-939\n",
      "ML 238: Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain -- ECAI 2023: 2954-2961\n",
      "ML 239: Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation -- EMNLP 2023: 953-967\n",
      "ML 240: Attention-Enhancing Backdoor Attacks Against BERT-based Models -- EMNLP (Findings) 2023: 10672-10690\n",
      "ML 241: Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers -- EMNLP (Findings) 2023: 12499-12527\n",
      "ML 242: Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models -- EMNLP 2023: 12303-12317\n",
      "ML 243: Backdoor Attacks Leveraging Latent Representation in Competitive Learning -- ESORICS Workshops (2) 2023: 700-718\n",
      "ML 244: Watermarking Graph Neural Networks based on Backdoor Attacks -- EuroS&P 2023: 1179-1197\n",
      "ML 245: Backdoor Mitigation in Deep Neural Networks via Strategic Retraining -- FM 2023: 635-647\n",
      "ML 246: Backdoor Attacks on Multi-Agent Reinforcement Learning-based Spectrum Management -- GLOBECOM 2023: 3361-3365\n",
      "ML 247: Backdoor Attacks Against Deep Learning-Based Massive MIMO Localization -- GLOBECOM 2023: 2796-2801\n",
      "ML 248: Black-Box Graph Backdoor Defense -- ICA3PP (5) 2023: 163-180\n",
      "ML 249: Fully Hidden Dynamic Trigger Backdoor Attacks -- ICAART (3) 2023: 81-91\n",
      "ML 250: QTROJAN: A Circuit Backdoor Against Quantum Neural Networks -- ICASSP 2023: 1-5\n",
      "ML 251: BadRes: Reveal the Backdoors Through Residual Connection -- ICASSP 2023: 1-5\n",
      "ML 252: Going in Style: Audio Backdoors Through Stylistic Transformations -- ICASSP 2023: 1-5\n",
      "ML 253: Measure and Countermeasure of the Capsulation Attack Against Backdoor-Based Deep Neural Network Watermarks -- ICASSP 2023: 1-5\n",
      "ML 254: Untargeted Backdoor Attack Against Object Detection -- ICASSP 2023: 1-5\n",
      "ML 255: Training Set Cleansing of Backdoor Poisoning by Self-Supervised Representation Learning -- ICASSP 2023: 1-5\n",
      "ML 256: BATT: Backdoor Attack with Transformation-Based Triggers -- ICASSP 2023: 1-5\n",
      "ML 257: Backdoor Defense via Suppressing Model Shortcuts -- ICASSP 2023: 1-5\n",
      "ML 258: NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning -- ICASSP 2023: 1-5\n",
      "ML 259: An Empirical Study of Backdoor Attacks on Masked Auto Encoders -- ICASSP 2023: 1-5\n",
      "ML 260: Random Location Poisoning Backdoor Attack Against Automatic Modulation Classification in Wireless Networks -- ICCC 2023: 1-6\n",
      "ML 261: Stealthy Backdoor Attack on RF Signal Classification -- ICCCN 2023: 1-10\n",
      "ML 262: Countermeasure against Backdoor Attack for Deep Learning-Based Phishing Detection -- ICCE-Taiwan 2023: 651-652\n",
      "ML 263: Invisible Encoded Backdoor attack on DNNs using Conditional GAN -- ICCE 2023: 1-5\n",
      "ML 264: PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning -- ICCV 2023: 4676-4685\n",
      "ML 265: An Embarrassingly Simple Backdoor Attack on Self-supervised Learning -- ICCV 2023: 4344-4355\n",
      "ML 266: Beating Backdoor Attack at Its Own Game -- ICCV 2023: 4597-4606\n",
      "ML 267: The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning -- ICCV 2023: 4707-4717\n",
      "ML 268: Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis -- ICCV 2023: 4561-4573\n",
      "ML 269: TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models -- ICCV 2023: 165-175\n",
      "ML 270: Computation and Data Efficient Backdoor Attacks -- ICCV 2023: 4782-4791\n",
      "ML 271: Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization -- ICCV 2023: 4443-4454\n",
      "ML 272: Backdoor Learning on Siamese Networks Using Physical Triggers: FaceNet as a Case Study -- ICDF2C (1) 2023: 279-292\n",
      "ML 273: Persistent Clean-Label Backdoor on Graph-Based Semi-supervised Cybercrime Detection -- ICDF2C (1) 2023: 264-278\n",
      "ML 274: CCBA: Code Poisoning-Based Clean-Label Covert Backdoor Attack Against DNNs -- ICDF2C (1) 2023: 179-192\n",
      "ML 275: Backdoor Attack on 3D Grey Image Segmentation -- ICDM 2023: 708-717\n",
      "ML 276: BHAC-MRI: Backdoor and Hybrid Attacks on MRI Brain Tumor Classification Using CNN -- ICIAP (2) 2023: 332-344\n",
      "ML 277: Neural Network Backdoor Attacks Fully Controlled by Composite Natural Utterance Fragments -- ICICS 2023: 451-466\n",
      "ML 278: Efficient any-Target Backdoor Attack with Pseudo Poisoned Samples -- ICIP 2023: 3319-3323\n",
      "ML 279: CSSBA: A Clean Label Sample-Specific Backdoor Attack -- ICIP 2023: 965-969\n",
      "ML 280: Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only -- ICLR 2023\n",
      "ML 281: SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency -- ICLR 2023\n",
      "ML 282: Few-shot Backdoor Attacks via Neural Tangent Kernels -- ICLR 2023\n",
      "ML 283: Distilling Cognitive Backdoor Patterns within an Image -- ICLR 2023\n",
      "ML 284: Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks -- ICLR 2023\n",
      "ML 285: The Dark Side of AutoML: Towards Architectural Backdoor Search -- ICLR 2023\n",
      "ML 286: Revisiting the Assumption of Latent Separability for Backdoor Defenses -- ICLR 2023\n",
      "ML 287: UNICORN: A Unified Backdoor Trigger Inversion Framework -- ICLR 2023\n",
      "ML 288: DBIA: Data-Free Backdoor Attack Against Transformer Networks -- ICME 2023: 2819-2824\n",
      "ML 289: Watermarks for Generative Adversarial Network Based on Steganographic Invisible Backdoor -- ICME 2023: 1211-1216\n",
      "ML 290: Rethinking Backdoor Attacks -- ICML 2023: 16216-16236\n",
      "ML 291: Reconstructive Neuron Pruning for Backdoor Defense -- ICML 2023: 19837-19854\n",
      "ML 292: Understanding Backdoor Attacks through the Adaptability Hypothesis -- ICML 2023: 37952-37976\n",
      "ML 293: UMD: Unsupervised Model Detection for X2X Backdoor Attacks -- ICML 2023: 38013-38038\n",
      "ML 294: Graph Contrastive Backdoor Attacks -- ICML 2023: 40888-40910\n",
      "ML 295: SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach -- ICONIP (12) 2023: 285-297\n",
      "ML 296: MIC: An Effective Defense Against Word-Level Textual Backdoor Attacks -- ICONIP (6) 2023: 3-18\n",
      "ML 297: X-HDNN: Explainable Hybrid DNN for Industrial Internet of Things Backdoor Attack Detection -- ICTC 2023: 1053-1057\n",
      "ML 298: Orion: Online Backdoor Sample Detection via Evolution Deviance -- IJCAI 2023: 864-874\n",
      "ML 299: Backdoor Attack on Deep Neural Networks in Perception Domain -- IJCNN 2023: 1-8\n",
      "ML 300: Rethinking the Trigger-injecting Position in Graph Backdoor Attack -- IJCNN 2023: 1-8\n",
      "ML 301: Computational Color Constancy-Based Backdoor Attacks -- ISPA 2023: 1-6\n",
      "ML 302: SDN Application Backdoor: Disrupting the Service via Poisoning the Topology -- INFOCOM 2023: 1-10\n",
      "ML 303: Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing -- INFOCOM 2023: 1-10\n",
      "ML 304: SEBD: Sensor Emulation Based Backdoor for Autopilot -- IoT 2023: 265-269\n",
      "ML 305: DUBIOUS: Detecting Unknown Backdoored Input by Observing Unusual Signatures -- MILCOM 2023: 696-702\n",
      "ML 306: Robust Sentiment Classification Based on the Backdoor Adjustment -- MLNLP 2023: 41-47\n",
      "ML 307: Physical Invisible Backdoor Based on Camera Imaging -- ACM Multimedia 2023: 7817-7825\n",
      "ML 308: PointCRT: Detecting Backdoor in 3D Point Cloud via Corruption Robustness -- ACM Multimedia 2023: 666-675\n",
      "ML 309: ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing -- ACM Multimedia 2023: 5410-5418\n",
      "ML 310: Moiré Backdoor Attack (MBA): A Novel Trigger for Pedestrian Detectors in the Physical World -- ACM Multimedia 2023: 8828-8838\n",
      "ML 311: PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification -- ACM Multimedia 2023: 9134-9142\n",
      "ML 312: Model-Contrastive Learning for Backdoor Elimination -- ACM Multimedia 2023: 8869-8880\n",
      "ML 313: Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning -- ACM Multimedia 2023: 1577-1587\n",
      "ML 314: The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems -- ACM Multimedia 2023: 7849-7858\n",
      "ML 315: MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems -- MobiCom 2023: 48:1-48:15\n",
      "ML 316: BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense -- NDSS 2023\n",
      "ML 317: Backdoor Attacks Against Dataset Distillation -- NDSS 2023\n",
      "ML 318: The \"Beatrix\" Resurrections: Robust Backdoor Detection via Gram Matrices -- NDSS 2023\n",
      "ML 319: BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning -- NeurIPS 2023\n",
      "ML 320: VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models -- NeurIPS 2023\n",
      "ML 321: BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking -- NeurIPS 2023\n",
      "ML 322: Towards Stable Backdoor Purification through Feature Shift Tuning -- NeurIPS 2023\n",
      "ML 323: Black-box Backdoor Defense via Zero-shot Image Purification -- NeurIPS 2023\n",
      "ML 324: Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots -- NeurIPS 2023\n",
      "ML 325: Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples -- NeurIPS 2023\n",
      "ML 326: Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks -- NeurIPS 2023\n",
      "ML 327: A Unified Detection Framework for Inference-Stage Backdoor Defenses -- NeurIPS 2023\n",
      "ML 328: CBD: A Certified Backdoor Detector Based on Local Dominant Probability -- NeurIPS 2023\n",
      "ML 329: Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks -- NeurIPS 2023\n",
      "ML 330: Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features -- NeurIPS 2023\n",
      "ML 331: IMTM: Invisible Multi-trigger Multimodal Backdoor Attack -- NLPCC (2) 2023: 533-545\n",
      "ML 332: Punctuation Matters! Stealthy Backdoor Attack for Language Models -- NLPCC (1) 2023: 524-536\n",
      "ML 333: Defending Against Backdoor Attacks by Layer-wise Feature Analysis -- PAKDD (2) 2023: 428-440\n",
      "ML 334: QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks -- QCE 2023: 1098-1106\n",
      "ML 335: Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems -- RepL4NLP@ACL 2023: 1-12\n",
      "ML 336: Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation -- SP 2023: 755-772\n",
      "ML 337: MagBackdoor: Beware of Your Loudspeaker as A Backdoor For Magnetic Injection Attacks -- SP 2023: 3416-3431\n",
      "ML 338: Disguising Attacks with Explanation-Aware Backdoors -- SP 2023: 664-681\n",
      "ML 339: On Feasibility of Server-side Backdoor Attacks on Split Learning -- SP (Workshops) 2023: 84-93\n",
      "ML 340: RAB: Provable Robustness Against Backdoor Attacks -- SP 2023: 1311-1328\n",
      "ML 341: Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers -- SP 2023: 719-736\n",
      "ML 342: AI-Guardian: Defeating Adversarial Attacks using Backdoors -- SP 2023: 701-718\n",
      "ML 343: \"We Must Protect the Transformers\": Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models -- SPACE 2023: 242-260\n",
      "ML 344: TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World -- SRDS 2023: 82-92\n",
      "ML 345: Training Data Leakage via Imperceptible Backdoor Attack -- SSCI 2023: 1553-1559\n",
      "ML 346: Immunizing Backdoored PRGs -- TCC (3) 2023: 153-182\n",
      "ML 347: Immunizing Backdoored PRGs -- IACR Cryptol. ePrint Arch. 2023: 1778 (2023)\n",
      "ML 348: PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis -- USENIX Security Symposium 2023: 2365-2382\n",
      "ML 349: VILLAIN: Backdoor Attacks Against Vertical Split Learning -- USENIX Security Symposium 2023: 2743-2760\n",
      "ML 350: A Data-free Backdoor Injection Approach in Neural Networks -- USENIX Security Symposium 2023: 2671-2688\n",
      "ML 351: ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms -- USENIX Security Symposium 2023: 2725-2742\n",
      "ML 352: Towards A Proactive ML Approach for Detecting Backdoor Poison Samples -- USENIX Security Symposium 2023: 1685-1702\n",
      "ML 353: Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks -- USENIX Security Symposium 2023: 2689-2706\n",
      "ML 354: Aliasing Backdoor Attacks on Pre-trained Models -- USENIX Security Symposium 2023: 2707-2724\n",
      "ML 355: TRAPDOOR: Repurposing neural network backdoors to detect dataset bias in machine learning-based genomic analysis -- VLSI-SoC 2023: 1-6\n",
      "ML 356: Unnoticeable Backdoor Attacks on Graph Neural Networks -- WWW 2023: 2263-2273\n",
      "ML 357: Training-free Lexical Backdoor Attacks on Language Models -- WWW 2023: 2198-2208\n",
      "ML 358: Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition -- CoRR abs/2301.00986 (2023)\n",
      "ML 359: Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack -- CoRR abs/2301.02615 (2023)\n",
      "ML 360: BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing -- CoRR abs/2301.10412 (2023)\n",
      "ML 361: PECAN: A Deterministic Certified Defense Against Backdoor Attacks -- CoRR abs/2301.11824 (2023)\n",
      "ML 362: Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering -- CoRR abs/2301.12318 (2023)\n",
      "ML 363: Salient Conditional Diffusion for Defending Against Backdoor Attacks -- CoRR abs/2301.13862 (2023)\n",
      "ML 364: Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks -- CoRR abs/2302.00747 (2023)\n",
      "ML 365: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification -- CoRR abs/2302.01740 (2023)\n",
      "ML 366: BackdoorBox: A Python Toolbox for Backdoor Learning -- CoRR abs/2302.01762 (2023)\n",
      "ML 367: Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder -- CoRR abs/2302.04457 (2023)\n",
      "ML 368: Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness -- CoRR abs/2302.04977 (2023)\n",
      "ML 369: Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data -- CoRR abs/2302.06279 (2023)\n",
      "ML 370: Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions -- CoRR abs/2302.06801 (2023)\n",
      "ML 371: Backdoor Attacks to Pre-trained Unified Foundation Models -- CoRR abs/2302.09360 (2023)\n",
      "ML 372: RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks -- CoRR abs/2302.09420 (2023)\n",
      "ML 373: Adversarial Machine Learning: A Systematic Survey of Backdoor Attack, Weight Attack and Adversarial Example -- CoRR abs/2302.09457 (2023)\n",
      "ML 374: Analyzing And Editing Inner Mechanisms Of Backdoored Language Models -- CoRR abs/2302.12461 (2023)\n",
      "ML 375: SATBA: An Invisible Backdoor Attack Based On Spatial Attention -- CoRR abs/2302.13056 (2023)\n",
      "ML 376: A semantic backdoor attack against Graph Convolutional Networks -- CoRR abs/2302.14353 (2023)\n",
      "ML 377: Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias -- CoRR abs/2303.01504 (2023)\n",
      "ML 378: Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking -- CoRR abs/2303.11470 (2023)\n",
      "ML 379: Influencer Backdoor Attack on Semantic Segmentation -- CoRR abs/2303.12054 (2023)\n",
      "ML 380: Do Backdoors Assist Membership Inference Attacks -- CoRR abs/2303.12589 (2023)\n",
      "ML 381: PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems -- CoRR abs/2303.14009 (2023)\n",
      "ML 382: Backdoor Attacks with Input-unique Triggers in NLP -- CoRR abs/2303.14325 (2023)\n",
      "ML 383: Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder -- CoRR abs/2303.15564 (2023)\n",
      "ML 384: A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network -- INTERSPEECH 2022: 4770-4774\n",
      "ML 385: Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning -- CoRR abs/2304.00252 (2023)\n",
      "ML 386: Evil from Within: Machine Learning Backdoors through Hardware Trojans -- CoRR abs/2304.08411 (2023)\n",
      "ML 387: Launching a Robust Backdoor Attack under Capability Constrained Scenarios -- CoRR abs/2304.10985 (2023)\n",
      "ML 388: BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT -- CoRR abs/2304.12298 (2023)\n",
      "ML 389: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger -- CoRR abs/2304.14475 (2023)\n",
      "ML 390: Backdoor Learning on Sequence to Sequence Models -- CoRR abs/2305.02424 (2023)\n",
      "ML 391: BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks -- CoRR abs/2305.03289 (2023)\n",
      "ML 392: BadCS: A Backdoor Attack Framework for Code search -- CoRR abs/2305.05503 (2023)\n",
      "ML 393: Backdoor to the Hidden Ground State: Planted Vertex Cover Example -- CoRR abs/2305.06610 (2023)\n",
      "ML 394: UOR: Universal Backdoor Attacks on Pre-trained Language Models -- CoRR abs/2305.09574 (2023)\n",
      "ML 395: Stealthy Low-frequency Backdoor Attack against Deep Neural Networks -- CoRR abs/2305.09677 (2023)\n",
      "ML 396: Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks -- CoRR abs/2305.10596 (2023)\n",
      "ML 397: Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization -- CoRR abs/2305.10701 (2023)\n",
      "ML 398: Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models -- CoRR abs/2305.14710 (2023)\n",
      "ML 399: From Shortcuts to Triggers: Backdoor Defense with Denoised PoE -- CoRR abs/2305.14910 (2023)\n",
      "ML 400: IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks -- CoRR abs/2305.16503 (2023)\n",
      "ML 401: Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study -- CoRR abs/2305.18384 (2023)\n",
      "ML 402: Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers -- CoRR abs/2306.00816 (2023)\n",
      "ML 403: Mitigating Backdoor Attack Via Prerequisite Transformation -- CoRR abs/2306.01983 (2023)\n",
      "ML 404: Backdoor Attack with Sparse and Invisible Trigger -- CoRR abs/2306.06209 (2023)\n",
      "ML 405: A Proxy-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks -- CoRR abs/2306.08313 (2023)\n",
      "ML 406: Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios -- CoRR abs/2306.08386 (2023)\n",
      "ML 407: IMPOSITION: Implicit Backdoor Attack through Scenario Injection -- CoRR abs/2306.15755 (2023)\n",
      "ML 408: Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion -- CoRR abs/2306.15875 (2023)\n",
      "ML 409: Efficient Backdoor Removal Through Natural Gradient Fine-tuning -- CoRR abs/2306.17441 (2023)\n",
      "ML 410: Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy -- CoRR abs/2307.07328 (2023)\n",
      "ML 411: Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound -- CoRR abs/2307.08208 (2023)\n",
      "ML 412: Backdoor Attack against Object Detection with Clean Annotation -- CoRR abs/2307.10487 (2023)\n",
      "ML 413: FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks -- CoRR abs/2307.11565 (2023)\n",
      "ML 414: Backdoor Attacks against Voice Recognition Systems: A Survey -- CoRR abs/2307.13643 (2023)\n",
      "ML 415: Backdoor Attacks for In-Context Learning with Language Models -- CoRR abs/2307.14692 (2023)\n",
      "ML 416: Backdoor Defense with Non-Adversarial Backdoor -- CoRR abs/2307.15539 (2023)\n",
      "ML 417: BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models -- CoRR abs/2307.16489 (2023)\n",
      "ML 418: Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection -- CoRR abs/2308.04617 (2023)\n",
      "ML 419: Test-Time Adaptation for Backdoor Defense -- CoRR abs/2308.06107 (2023)\n",
      "ML 420: Backdoor Mitigation by Correcting the Distribution of Neural Activations -- CoRR abs/2308.09850 (2023)\n",
      "ML 421: Hiding Backdoors within Event Sequence Data via Poisoning Attacks -- CoRR abs/2308.10201 (2023)\n",
      "ML 422: Backdooring Textual Inversion for Concept Censorship -- CoRR abs/2308.10718 (2023)\n",
      "ML 423: BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection -- CoRR abs/2308.12439 (2023)\n",
      "ML 424: LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors -- CoRR abs/2308.13904 (2023)\n",
      "ML 425: A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks -- CoRR abs/2308.14367 (2023)\n",
      "ML 426: Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack -- CoRR abs/2308.16684 (2023)\n",
      "ML 427: BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment -- CoRR abs/2309.01480 (2023)\n",
      "ML 428: One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning -- CoRR abs/2309.04036 (2023)\n",
      "ML 429: Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review -- CoRR abs/2309.06055 (2023)\n",
      "ML 430: Robust Backdoor Attacks on Object Detection in Real World -- CoRR abs/2309.08953 (2023)\n",
      "ML 431: Steganography for Neural Radiance Fields by Backdooring -- CoRR abs/2309.10503 (2023)\n",
      "ML 432: Horizontal Class Backdoor to Deep Learning -- CoRR abs/2310.00542 (2023)\n",
      "ML 433: GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning -- CoRR abs/2310.00626 (2023)\n",
      "ML 434: Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes -- CoRR abs/2310.02451 (2023)\n",
      "ML 435: Confidence-driven Sampling for Backdoor Attacks -- CoRR abs/2310.05263 (2023)\n",
      "ML 436: Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks -- CoRR abs/2310.05862 (2023)\n",
      "ML 437: High Dimensional Causal Inference with Variational Backdoor Adjustment -- CoRR abs/2310.06100 (2023)\n",
      "ML 438: Prompt Backdoors in Visual Prompt Learning -- CoRR abs/2310.07632 (2023)\n",
      "ML 439: Composite Backdoor Attacks Against Large Language Models -- CoRR abs/2310.07676 (2023)\n",
      "ML 440: Invisible Threats: Backdoor Attack in OCR Systems -- CoRR abs/2310.08259 (2023)\n",
      "ML 441: Defending Our Privacy With Backdoors -- CoRR abs/2310.08320 (2023)\n",
      "ML 442: Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks -- CoRR abs/2310.09744 (2023)\n",
      "ML 443: Backdoor Attack through Machine Unlearning -- CoRR abs/2310.10659 (2023)\n",
      "ML 444: Demystifying Poisoning Backdoor Attacks from a Statistical Perspective -- CoRR abs/2310.10780 (2023)\n",
      "ML 445: WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks -- CoRR abs/2310.11595 (2023)\n",
      "ML 446: PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models -- CoRR abs/2310.12439 (2023)\n",
      "ML 447: From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models -- CoRR abs/2311.02373 (2023)\n",
      "ML 448: Does Differential Privacy Prevent Backdoor Attacks in Practice -- CoRR abs/2311.06227 (2023)\n",
      "ML 449: Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration -- CoRR abs/2311.07417 (2023)\n",
      "ML 450: Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data -- CoRR abs/2311.07550 (2023)\n",
      "ML 451: Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment -- CoRR abs/2311.09433 (2023)\n",
      "ML 452: Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations -- CoRR abs/2311.09763 (2023)\n",
      "ML 453: TextGuard: Provable Defense against Backdoor Attacks on Text Classification -- CoRR abs/2311.11225 (2023)\n",
      "ML 454: Universal Jailbreak Backdoors from Poisoned Human Feedback -- CoRR abs/2311.14455 (2023)\n",
      "ML 455: Effective Backdoor Mitigation Depends on the Pre-training Objective -- CoRR abs/2311.14948 (2023)\n",
      "ML 456: BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP -- CoRR abs/2311.16194 (2023)\n",
      "ML 457: Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective -- CoRR abs/2311.16646 (2023)\n",
      "ML 458: TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4 -- CoRR abs/2311.17429 (2023)\n",
      "ML 459: Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections -- CoRR abs/2312.00027 (2023)\n",
      "ML 460: Universal Backdoor Attacks -- CoRR abs/2312.00157 (2023)\n",
      "ML 461: OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection -- CoRR abs/2312.01585 (2023)\n",
      "ML 462: Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics -- CoRR abs/2312.02673 (2023)\n",
      "ML 463: Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models -- CoRR abs/2312.03419 (2023)\n",
      "ML 464: Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger -- CoRR abs/2312.04584 (2023)\n",
      "ML 465: BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting -- CoRR abs/2312.04902 (2023)\n",
      "ML 466: Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks -- CoRR abs/2312.06230 (2023)\n",
      "ML 467: Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking -- CoRR abs/2312.07955 (2023)\n",
      "ML 468: On the Difficulty of Defending Contrastive Learning against Backdoor Attacks -- CoRR abs/2312.09057 (2023)\n",
      "ML 469: FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge -- CoRR abs/2312.09665 (2023)\n",
      "ML 470: UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks -- CoRR abs/2312.10657 (2023)\n",
      "ML 471: Manipulating Trajectory Prediction with Backdoors -- CoRR abs/2312.13863 (2023)\n",
      "ML 472: (Withdrawn) BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning -- CoRR abs/2311.12075 (2023)\n",
      "ML 473: A Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation -- IEEE Access 10: 5545-5558 (2022)\n",
      "ML 474: Backdoor Defence for Voice Print Recognition Model Based on Speech Enhancement and Weight Pruning -- IEEE Access 10: 114016-114023 (2022)\n",
      "ML 475: A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks -- Ad Hoc Networks 124: 102727 (2022)\n",
      "ML 476: Boosting the Performance of CDCL-Based SAT Solvers by Exploiting Backbones and Backdoors -- Algorithms 15(9): 302 (2022)\n",
      "ML 477: Active intellectual property protection for deep neural networks through stealthy backdoor and users' identities authentication -- Appl. Intell. 52(14): 16497-16511 (2022)\n",
      "ML 478: VulnerGAN: a backdoor attack through vulnerability amplification against machine learning-based network intrusion detection systems -- Sci. China Inf. Sci. 65(7): 1-19 (2022)\n",
      "ML 479: Backdoor smoothing: Demystifying backdoor attacks on deep neural networks -- Comput. Secur. 120: 102814 (2022)\n",
      "ML 480: The triggers that open the NLP model backdoors are hidden in the adversarial samples -- Comput. Secur. 118: 102730 (2022)\n",
      "ML 481: PTB: Robust physical backdoor attacks against deep neural networks in real world -- Comput. Secur. 118: 102726 (2022)\n",
      "ML 482: Backdoors Against Natural Language Processing: A Review -- IEEE Secur. Priv. 20(5): 50-59 (2022)\n",
      "ML 483: Experimental Study of Fault Injection Attack on Image Sensor Interface for Triggering Backdoored DNN Models -- IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 105-A(3): 336-343 (2022)\n",
      "ML 484: Multi-Model Selective Backdoor Attack with Different Trigger Positions -- IEICE Trans. Inf. Syst. 105-D(1): 170-174 (2022)\n",
      "ML 485: A multitarget backdooring attack on deep neural networks with random location trigger -- Int. J. Intell. Syst. 37(3): 2567-2583 (2022)\n",
      "ML 486: Susceptibility & defense of satellite image-trained convolutional networks to backdoor attacks -- Inf. Sci. 603: 244-261 (2022)\n",
      "ML 487: Robust backdoor injection with the capability of resisting network transfer -- Inf. Sci. 612: 594-611 (2022)\n",
      "ML 488: Backdoor-resistant identity-based proxy re-encryption for cloud-assisted wireless body area networks -- Inf. Sci. 604: 80-96 (2022)\n",
      "ML 489: BlindNet backdoor: Attack on deep neural network using blind watermark -- Multim. Tools Appl. 81(5): 6217-6234 (2022)\n",
      "ML 490: MP-BADNet+: Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs -- Peer-to-Peer Netw. Appl. 15(6): 2457-2473 (2022)\n",
      "ML 491: IBD: An Interpretable Backdoor-Detection Method via Multivariate Interactions -- Sensors 22(22): 8697 (2022)\n",
      "ML 492: Are Backdoor Mandates Ethical? - A Position Paper -- IEEE Technol. Soc. Mag. 41(4): 63-70 (2022)\n",
      "ML 493: Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks -- IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 41(8): 2611-2624 (2022)\n",
      "ML 494: Backdoor Attack on Machine Learning Based Android Malware Detectors -- IEEE Trans. Dependable Secur. Comput. 19(5): 3357-3370 (2022)\n",
      "ML 495: One-to-N & N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models -- IEEE Trans. Dependable Secur. Comput. 19(3): 1562-1578 (2022)\n",
      "ML 496: LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check -- IEEE Trans. Inf. Forensics Secur. 17: 2000-2014 (2022)\n",
      "ML 497: Stealthy Backdoors as Compression Artifacts -- IEEE Trans. Inf. Forensics Secur. 17: 1372-1387 (2022)\n",
      "ML 498: Dispersed Pixel Perturbation-Based Imperceptible Backdoor Trigger for Image Classifier Models -- IEEE Trans. Inf. Forensics Secur. 17: 3091-3106 (2022)\n",
      "ML 499: Poison Ink: Robust and Invisible Backdoor Attack -- IEEE Trans. Image Process. 31: 5691-5705 (2022)\n",
      "ML 500: Detection of Backdoors in Trained Classifiers Without Access to the Training Set -- IEEE Trans. Neural Networks Learn. Syst. 33(3): 1177-1191 (2022)\n",
      "ML 501: Model Agnostic Defence Against Backdoor Attacks in Machine Learning -- IEEE Trans. Reliab. 71(2): 880-895 (2022)\n",
      "ML 502: Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models -- IEEE Trans. Serv. Comput. 15(3): 1526-1539 (2022)\n",
      "ML 503: Tractable Abstract Argumentation via Backdoor-Treewidth -- AAAI 2022: 5608-5615\n",
      "ML 504: Backdoor Attacks on the DNN Interpretation System -- AAAI 2022: 561-570\n",
      "ML 505: Faster Algorithms for Weak Backdoors -- AAAI 2022: 3741-3748\n",
      "ML 506: Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks -- AAAI 2022: 9575-9583\n",
      "ML 507: Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework -- AAAI 2022: 3786-3795\n",
      "ML 508: Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks -- AAAI 2022: 10309-10318\n",
      "ML 509: On Probabilistic Generalization of Backdoors in Boolean Satisfiability -- AAAI 2022: 10353-10361\n",
      "ML 510: COLLIDER: A Robust Training Framework for Backdoor Data -- ACCV (6) 2022: 681-698\n",
      "ML 511: Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks -- ACSAC 2022: 267-278\n",
      "ML 512: An Approach to Generation Triggers for Parrying Backdoor in Neural Networks -- AGI 2022: 304-314\n",
      "ML 513: Dynamic Backdoors with Global Average Pooling -- AICAS 2022: 320-323\n",
      "ML 514: Sample-Specific Backdoor based Active Intellectual Property Protection for Deep Neural Networks -- AICAS 2022: 316-319\n",
      "ML 515: Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection -- BDCAT 2022: 40-47\n",
      "ML 516: Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition -- IEEE Big Data 2022: 2940-2949\n",
      "ML 517: SentMod: Hidden Backdoor Attack on Unstructured Textual Data -- BigDataSecurity/HPSC/IDS 2022: 224-231\n",
      "ML 518: An anomaly detection approach for backdoored neural networks: face recognition as a case study -- BIOSIG 2022: 80-88\n",
      "ML 519: Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain -- BMVC 2022: 259\n",
      "ML 520: Verifying Neural Networks Against Backdoor Attacks -- CAV (1) 2022: 171-192\n",
      "ML 521: Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets -- CCS 2022: 3315-3317\n",
      "ML 522: Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation -- AISec@CCS 2022: 91-102\n",
      "ML 523: Poster: Clean-label Backdoor Attack on Graph Neural Networks -- CCS 2022: 3491-3493\n",
      "ML 524: Asynchronous Evolutionary Algorithm for Finding Backdoors in Boolean Satisfiability -- CEC 2022: 1-8\n",
      "ML 525: Efficient DNN Backdoor Detection Guided by Static Weight Analysis -- Inscrypt 2022: 408-428\n",
      "ML 526: Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications -- COLING 2022: 984-993\n",
      "ML 527: Deletion-Backdoors for Argumentation Frameworks with Collective Attacks -- SAFA@COMMA 2022: 98-110\n",
      "ML 528: Learning Pseudo-Backdoors for Mixed Integer Programs -- CPAIOR 2022: 91-102\n",
      "ML 529: Learning Pseudo-Backdoors for Mixed Integer Programs -- SOCS 2021: 170-172\n",
      "ML 530: FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis -- CVPR 2022: 20844-20853\n",
      "ML 531: Few-shot Backdoor Defense Using Shapley Estimation -- CVPR 2022: 13348-13357\n",
      "ML 532: Complex Backdoor Detection by Symmetric Feature Differencing -- CVPR 2022: 14983-14993\n",
      "ML 533: Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks -- CVPR 2022: 13337-13347\n",
      "ML 534: Backdoor Attacks on Self-Supervised Learning -- CVPR 2022: 13327-13336\n",
      "ML 535: Better Trigger Inversion Optimization in Backdoor Scanning -- CVPR 2022: 13358-13368\n",
      "ML 536: Dual-Key Multimodal Backdoors for Visual Question Answering -- CVPR 2022: 15354-15364\n",
      "ML 537: DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints -- CVPR 2022: 15192-15201\n",
      "ML 538: Combining Defences Against Data-Poisoning Based Backdoor Attacks on Neural Networks -- DBSec 2022: 28-47\n",
      "ML 539: TextBack: Watermarking Text Classifiers using Backdooring -- DSD 2022: 340-347\n",
      "ML 540: BadDet: Backdoor Attacks on Object Detection -- ECCV Workshops (1) 2022: 396-412\n",
      "ML 541: RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN -- ECCV (4) 2022: 708-724\n",
      "ML 542: An Invisible Black-Box Backdoor Attack Through Frequency Domain -- ECCV (13) 2022: 396-413\n",
      "ML 543: Data-Free Backdoor Removal Based on Channel Lipschitzness -- ECCV (5) 2022: 175-191\n",
      "ML 544: Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks -- EMNLP 2022: 11215-11221\n",
      "ML 545: Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks -- EMNLP (Findings) 2022: 668-683\n",
      "ML 546: WeDef: Weakly Supervised Backdoor Defense for Text Classification -- EMNLP 2022: 11614-11626\n",
      "ML 547: Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models -- EMNLP (Findings) 2022: 355-372\n",
      "ML 548: SAT Backdoors: Depth Beats Size -- ESA 2022: 46:1-46:18\n",
      "ML 549: Kallima: A Clean-Label Framework for Textual Backdoor Attacks -- ESORICS (1) 2022: 447-466\n",
      "ML 550: The Devil Is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models -- ESORICS (3) 2022: 776-783\n",
      "ML 551: TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors -- EuroS&P 2022: 684-702\n",
      "ML 552: Dynamic Backdoor Attacks Against Machine Learning Models -- EuroS&P 2022: 703-718\n",
      "ML 553: A Pragmatic Label-Specific Backdoor Attack -- FCS 2022: 149-162\n",
      "ML 554: Planting Undetectable Backdoors in Machine Learning Models : [Extended Abstract] -- FOCS 2022: 931-942\n",
      "ML 555: A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning -- GLOBECOM 2022: 2710-2715\n",
      "ML 556: Backdoor Sets on Nowhere Dense SAT -- ICALP 2022: 91:1-91:20\n",
      "ML 557: Stealthy Backdoor Attack with Adversarial Training -- ICASSP 2022: 2969-2973\n",
      "ML 558: Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks -- ICASSP 2022: 3333-3337\n",
      "ML 559: Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks -- ICASSP 2022: 96-100\n",
      "ML 560: When Does Backdoor Attack Succeed in Image Reconstruction? A Study of Heuristics vs -- Bi-Level Solution. ICASSP 2022: 4398-4402\n",
      "ML 561: Detecting Backdoor Attacks against Point Cloud Classifiers -- ICASSP 2022: 3159-3163\n",
      "ML 562: Towards Backdoor Attack on Deep Learning based Time Series Classification -- ICDE 2022: 1274-1287\n",
      "ML 563: Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models -- ICDIS 2022: 61-66\n",
      "ML 564: Backdoor Poisoning of Encrypted Traffic Classifiers -- ICDM (Workshops) 2022: 577-585\n",
      "ML 565: Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack -- ICIAP (2) 2022: 176-185\n",
      "ML 566: CRAB: Certified Patch Robustness Against Poisoning-Based Backdoor Attacks -- ICIP 2022: 2486-2490\n",
      "ML 567: Few-Shot Backdoor Attacks on Visual Object Tracking -- ICLR 2022\n",
      "ML 568: Poisoning and Backdooring Contrastive Learning -- ICLR 2022\n",
      "ML 569: BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models -- ICLR 2022\n",
      "ML 570: AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis -- ICLR 2022\n",
      "ML 571: Backdoor Defense via Decoupling the Training Process -- ICLR 2022\n",
      "ML 572: Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios -- ICLR 2022\n",
      "ML 573: Adversarial Unlearning of Backdoors via Implicit Hypergradient -- ICLR 2022\n",
      "ML 574: How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data -- ICLR 2022\n",
      "ML 575: Unlabeled Backdoor Poisoning in Semi-Supervised Learning -- ICME 2022: 1-6\n",
      "ML 576: Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense -- ICML 2022: 19879-19892\n",
      "ML 577: A Generic Enhancer for Backdoor Attacks on Deep Neural Networks -- ICONIP (7) 2022: 296-307\n",
      "ML 578: Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers -- ICONIP (3) 2022: 216-227\n",
      "ML 579: UltraBD: Backdoor Attack against Automatic Speaker Verification Systems via Adversarial Ultrasound -- ICPADS 2022: 193-200\n",
      "ML 580: Image Watermarking Backdoor Attacks in CNN-Based Classification Tasks -- ICPR Workshops (4) 2022: 3-16\n",
      "ML 581: Backdoor Attacks against Deep Neural Networks by Personalized Audio Steganography -- ICPR 2022: 68-74\n",
      "ML 582: Backdoors in Neural Models of Source Code -- ICPR 2022: 2892-2899\n",
      "ML 583: Detecting Backdoor Attacks on Deep Neural Networks Based on Model Parameters Analysis -- ICTAI 2022: 630-637\n",
      "ML 584: PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning -- IJCAI 2022: 680-686\n",
      "ML 585: Membership Inference via Backdooring -- IJCAI 2022: 3832-3838\n",
      "ML 586: Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation -- IJCAI 2022: 1481-1487\n",
      "ML 587: Data-Efficient Backdoor Attacks -- IJCAI 2022: 3992-3998\n",
      "ML 588: Imperceptible Backdoor Attack: From Input Space to Feature Representation -- IJCAI 2022: 1736-1742\n",
      "ML 589: Latent Space-Based Backdoor Attacks Against Deep Neural Networks -- IJCNN 2022: 1-10\n",
      "ML 590: ACTSS: Input Detection Defense against Backdoor Attacks via Activation Subset Scanning -- IJCNN 2022: 1-8\n",
      "ML 591: Backdoor Defense with Machine Unlearning -- INFOCOM 2022: 280-289\n",
      "ML 592: TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers -- INFOCOM 2022: 1429-1438\n",
      "ML 593: An Improved Method for Making CNN Immune to Backdoor Attack by Activating Clustering -- ISCSIC 2022: 1-6\n",
      "ML 594: Practical Backdoor Attack Against Speaker Recognition System -- ISPEC 2022: 468-484\n",
      "ML 595: Energy-Based Learning for Preventing Backdoor Attack -- KSEM (3) 2022: 706-721\n",
      "ML 596: I Know Your Triggers: Defending Against Textual Backdoor Attacks with Benign Backdoor Augmentation -- MILCOM 2022: 442-449\n",
      "ML 597: Natural Backdoor Attacks on Speech Recognition Models -- ML4CS (1) 2022: 597-610\n",
      "ML 598: Opportunistic Backdoor Attacks: Exploring Human-imperceptible Vulnerabilities on Speech Recognition Systems -- ACM Multimedia 2022: 2390-2398\n",
      "ML 599: Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving -- ACM Multimedia 2022: 2957-2968\n",
      "ML 600: BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label -- ACM Multimedia 2022: 678-686\n",
      "ML 601: Backdoor Attacks on Crowd Counting -- ACM Multimedia 2022: 5351-5360\n",
      "ML 602: Purifier: Plug-and-play Backdoor Mitigation for Pre-trained Models Via Anomaly Activation Suppression -- ACM Multimedia 2022: 4291-4299\n",
      "ML 603: Audio-domain position-independent backdoor attack via unnoticeable triggers -- MobiCom 2022: 583-595\n",
      "ML 604: Triggerless Backdoor Attack for NLP Tasks with Clean Labels -- NAACL-HLT 2022: 2942-2952\n",
      "ML 605: ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks -- NDSS 2022\n",
      "ML 606: Handcrafted Backdoors in Deep Neural Networks -- NeurIPS 2022\n",
      "ML 607: Provable Defense against Backdoor Policies in Reinforcement Learning -- NeurIPS 2022\n",
      "ML 608: BadPrompt: Backdoor Attacks on Continuous Prompts -- NeurIPS 2022\n",
      "ML 609: Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets -- NeurIPS 2022\n",
      "ML 610: One-shot Neural Backdoor Erasing via Adversarial Weight Masking -- NeurIPS 2022\n",
      "ML 611: Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples -- NeurIPS 2022\n",
      "ML 612: A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks -- NeurIPS 2022\n",
      "ML 613: Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class -- NeurIPS 2022\n",
      "ML 614: Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection -- NeurIPS 2022\n",
      "ML 615: Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch -- NeurIPS 2022\n",
      "ML 616: Training with More Confidence: Mitigating Injected and Natural Backdoors During Training -- NeurIPS 2022\n",
      "ML 617: Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork -- NeurIPS 2022\n",
      "ML 618: Finding Naturally Occurring Physical Backdoors in Image Datasets -- NeurIPS 2022\n",
      "ML 619: BackdoorBench: A Comprehensive Benchmark of Backdoor Learning -- NeurIPS 2022\n",
      "ML 620: Pre-activation Distributions Expose Backdoor Neurons -- NeurIPS 2022\n",
      "ML 621: Moderate-fitting as a Natural Backdoor Defender for Pre-trained Language Models -- NeurIPS 2022\n",
      "ML 622: How to Backdoor (Classic) McEliece and How to Guard Against Backdoors -- PQCrypto 2022: 24-44\n",
      "ML 623: A Survey on Backdoor Attack and Defense in Natural Language Processing -- QRS 2022: 809-820\n",
      "ML 624: Transferable Graph Backdoor Attack -- RAID 2022: 321-332\n",
      "ML 625: Un-Fair Trojan: Targeted Backdoor Attacks Against Model Fairness -- SDS 2022: 1-9\n",
      "ML 626: Towards Backdoor Attacks against LiDAR Object Detection in Autonomous Driving -- SenSys 2022: 533-547\n",
      "ML 627: Energy-Based Learning for Polluted Outlier Detection in Backdoor -- SmartCloud 2022: 47-52\n",
      "ML 628: Inconspicuous Data Augmentation Based Backdoor Attack on Deep Neural Networks -- SOCC 2022: 1-6\n",
      "ML 629: Patch-Based Backdoors Detection and Mitigation with Feature Masking -- SocialSec 2022: 229-246\n",
      "ML 630: BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning -- SP 2022: 2043-2059\n",
      "ML 631: Piccolo: Exposing Complex Backdoors in NLP Transformer Models -- SP 2022: 2025-2042\n",
      "ML 632: Big Brother Is Watching You: A Closer Look at Backdoor Construction -- SPACE 2022: 81-96\n",
      "ML 633: Big Brother Is Watching You: A Closer Look At Backdoor Construction -- IACR Cryptol. ePrint Arch. 2022: 953 (2022)\n",
      "ML 634: A General Backdoor Attack to Graph Neural Networks Based on Explanation Method -- TrustCom 2022: 759-768\n",
      "ML 635: Clean-label Backdoor Attack on Machine Learning-based Malware Detection Models and Countermeasures -- TrustCom 2022: 1235-1242\n",
      "ML 636: A Novel Backdoor Attack Adapted to Transfer Learning -- SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta 2022: 1730-1735\n",
      "ML 637: Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation -- USENIX Security Symposium 2022: 3611-3628\n",
      "ML 638: Low-Poisoning Rate Invisible Backdoor Attack Based on Important Neurons -- WASA (2) 2022: 375-383\n",
      "ML 639: Extracting a Minimal Trigger for an Efficient Backdoor Poisoning Attack Using the Activation Values of a Deep Neural Network -- WDC@AsiaCCS 2022: 3-6\n",
      "ML 640: Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers -- WiseML@WiSec 2022: 57-62\n",
      "ML 641: Deep Learning Backdoors -- Security and Artificial Intelligence 2022: 313-334\n",
      "ML 642: Rethink Stealthy Backdoor Attacks in Natural Language Processing -- CoRR abs/2201.02993 (2022)\n",
      "ML 643: Neighboring Backdoor Attacks on Graph Convolutional Network -- CoRR abs/2201.06202 (2022)\n",
      "ML 644: Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World -- CoRR abs/2201.08619 (2022)\n",
      "ML 645: Hiding Behind Backdoors: Self-Obfuscation Against Generative Models -- CoRR abs/2201.09774 (2022)\n",
      "ML 646: Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire -- CoRR abs/2201.12211 (2022)\n",
      "ML 647: Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks -- CoRR abs/2201.13164 (2022)\n",
      "ML 648: Backdoor Detection in Reinforcement Learning -- CoRR abs/2202.03609 (2022)\n",
      "ML 649: False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger -- CoRR abs/2202.04479 (2022)\n",
      "ML 650: Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples -- CoRR abs/2202.06312 (2022)\n",
      "ML 651: Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches -- CoRR abs/2202.10320 (2022)\n",
      "ML 652: On the Effectiveness of Adversarial Training against Backdoor Attacks -- CoRR abs/2202.10627 (2022)\n",
      "ML 653: Label-Smoothed Backdoor Attack -- CoRR abs/2202.11203 (2022)\n",
      "ML 654: Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild -- CoRR abs/2203.00858 (2022)\n",
      "ML 655: Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks -- CoRR abs/2203.03692 (2022)\n",
      "ML 656: PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks -- CoRR abs/2203.09289 (2022)\n",
      "ML 657: Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning -- CoRR abs/2203.15506 (2022)\n",
      "ML 658: An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks -- CoRR abs/2204.04329 (2022)\n",
      "ML 659: Backdoor Attack against NLP models with Robustness-Aware Perturbation defense -- CoRR abs/2204.05758 (2022)\n",
      "ML 660: Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures -- CoRR abs/2204.06273 (2022)\n",
      "ML 661: Planting Undetectable Backdoors in Machine Learning Models -- CoRR abs/2204.06974 (2022)\n",
      "ML 662: Backdooring Explainable Machine Learning -- CoRR abs/2204.09498 (2022)\n",
      "ML 663: Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering -- CoRR abs/2204.12848 (2022)\n",
      "ML 664: Model-Contrastive Learning for Backdoor Defense -- CoRR abs/2205.04411 (2022)\n",
      "ML 665: Universal Post-Training Backdoor Detection -- CoRR abs/2205.06900 (2022)\n",
      "ML 666: Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution -- CoRR abs/2205.09167 (2022)\n",
      "ML 667: Textual Backdoor Attacks with Iterative Trigger Injection -- CoRR abs/2205.12700 (2022)\n",
      "ML 668: Circumventing Backdoor Defenses That Are Based on Latent Separability -- CoRR abs/2205.13613 (2022)\n",
      "ML 669: Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations -- CoRR abs/2205.13616 (2022)\n",
      "ML 670: Defending Against Stealthy Backdoor Attacks -- CoRR abs/2205.14246 (2022)\n",
      "ML 671: CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences -- CoRR abs/2206.00145 (2022)\n",
      "ML 672: Contributor-Aware Defenses Against Adversarial Backdoor Attacks -- CoRR abs/2206.03583 (2022)\n",
      "ML 673: Can Backdoor Attacks Survive Time-Varying Models -- CoRR abs/2206.04677 (2022)\n",
      "ML 674: Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers -- CoRR abs/2206.04881 (2022)\n",
      "ML 675: Backdoor Attacks on Vision Transformers -- CoRR abs/2206.08477 (2022)\n",
      "ML 676: DECK: Model Hardening for Defending Pervasive Backdoors -- CoRR abs/2206.09272 (2022)\n",
      "ML 677: Natural Backdoor Datasets -- CoRR abs/2206.10673 (2022)\n",
      "ML 678: Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain -- CoRR abs/2207.04209 (2022)\n",
      "ML 679: FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair -- CoRR abs/2207.12863 (2022)\n",
      "ML 680: Backdoor Watermarking Deep Learning Classification Models With Deep Fidelity -- CoRR abs/2208.00563 (2022)\n",
      "ML 681: Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons -- CoRR abs/2208.06537 (2022)\n",
      "ML 682: Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer -- CoRR abs/2208.06592 (2022)\n",
      "ML 683: Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers -- CoRR abs/2208.14127 (2022)\n",
      "ML 684: MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World -- CoRR abs/2209.02339 (2022)\n",
      "ML 685: Defending Against Backdoor Attack on Graph Nerual Network by Explainability -- CoRR abs/2209.02902 (2022)\n",
      "ML 686: Adaptive Perturbation Generation for Multiple Backdoors Detection -- CoRR abs/2209.05244 (2022)\n",
      "ML 687: Black-box Ownership Verification for Dataset Protection via Backdoor Watermarking -- CoRR abs/2209.06015 (2022)\n",
      "ML 688: Augmentation Backdoors -- CoRR abs/2209.15139 (2022)\n",
      "ML 689: ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks -- CoRR abs/2210.00108 (2022)\n",
      "ML 690: Backdoor Attacks in the Supply Chain of Masked Image Modeling -- CoRR abs/2210.01632 (2022)\n",
      "ML 691: Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets -- CoRR abs/2210.04688 (2022)\n",
      "ML 692: Understanding Impacts of Task Similarity on Backdoor Attack and Detection -- CoRR abs/2210.06509 (2022)\n",
      "ML 693: Watermarking Pre-trained Language Models with Backdooring -- CoRR abs/2210.07543 (2022)\n",
      "ML 694: Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning -- CoRR abs/2210.11082 (2022)\n",
      "ML 695: Detecting Backdoors in Deep Text Classifiers -- CoRR abs/2210.11264 (2022)\n",
      "ML 696: M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models -- CoRR abs/2211.01875 (2022)\n",
      "ML 697: Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models -- CoRR abs/2211.02408 (2022)\n",
      "ML 698: Physics-Constrained Backdoor Attacks on Power System Fault Localization -- CoRR abs/2211.04445 (2022)\n",
      "ML 699: Backdoor Attacks on Time Series: A Generative Approach -- CoRR abs/2211.07915 (2022)\n",
      "ML 700: CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning -- CoRR abs/2211.08229 (2022)\n",
      "ML 701: PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking -- CoRR abs/2211.08697 (2022)\n",
      "ML 702: Backdoor Attacks on Multiagent Collaborative Systems -- CoRR abs/2211.11455 (2022)\n",
      "ML 703: Backdoor Vulnerabilities in Normally Trained Deep Learning Models -- CoRR abs/2211.15929 (2022)\n",
      "ML 704: Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape -- CoRR abs/2211.16192 (2022)\n",
      "ML 705: Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning -- CoRR abs/2212.02582 (2022)\n",
      "ML 706: Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models -- CoRR abs/2212.04687 (2022)\n",
      "ML 707: Fine-Tuning Is All You Need to Mitigate Backdoor Attacks -- CoRR abs/2212.09067 (2022)\n",
      "ML 708: Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation -- CoRR abs/2212.09979 (2022)\n",
      "ML 709: VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion -- CoRR abs/2212.10103 (2022)\n",
      "ML 710: How to backdoor LWE-like cryptosystems -- IACR Cryptol. ePrint Arch. 2022: 1381 (2022)\n",
      "ML 711: How to Backdoor (Classical) McEliece and How to Guard Against Backdoors -- IACR Cryptol. ePrint Arch. 2022: 362 (2022)\n",
      "ML 712: Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs -- IACR Cryptol. ePrint Arch. 2022: 1681 (2022)\n",
      "ML 713: Use Procedural Noise to Achieve Backdoor Attack -- IEEE Access 9: 127204-127216 (2021)\n",
      "ML 714: Neural network laundering: Removing black-box backdoor watermarks from deep neural networks -- Comput. Secur. 106: 102277 (2021)\n",
      "ML 715: BDDR: An Effective Defense Against Textual Backdoor Attacks -- Comput. Secur. 110: 102433 (2021)\n",
      "ML 716: Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing -- Comput. Secur. 106: 102280 (2021)\n",
      "ML 717: Internet of Things backdoors: Resource management issues, security challenges, and detection methods -- Trans. Emerg. Telecommun. Technol. 32(2) (2021)\n",
      "ML 718: Study of scale-free structures in feed-forward neural networks against backdoor attacks -- ICT Express 7(2): 265-268 (2021)\n",
      "ML 719: Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification -- Neurocomputing 452: 253-262 (2021)\n",
      "ML 720: Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment -- IEEE J. Sel. Areas Commun. 39(8): 2617-2631 (2021)\n",
      "ML 721: Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set -- Neural Comput. 33(5): 1329-1371 (2021)\n",
      "ML 722: Stability-Based Analysis and Defense against Backdoor Attacks on Edge Computing Services -- IEEE Netw. 35(1): 163-169 (2021)\n",
      "ML 723: Existence versus exploitation: the opacity of backdoors and backbones -- Prog. Artif. Intell. 10(3): 297-308 (2021)\n",
      "ML 724: Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems -- Peer-to-Peer Netw. Appl. 14(3): 1458-1474 (2021)\n",
      "ML 725: A Master Key backdoor for universal impersonation attack against DNN-based face verification -- Pattern Recognit. Lett. 144: 61-67 (2021)\n",
      "ML 726: PBDT: Python Backdoor Detection Model Based on Combined Features -- Secur. Commun. Networks 2021: 9923234:1-9923234:13 (2021)\n",
      "ML 727: Training Data Poisoning in ML-CAD: Backdooring DL-Based Lithographic Hotspot Detectors -- IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 40(6): 1244-1257 (2021)\n",
      "ML 728: Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors Against Backdooring Attacks -- IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 40(10): 2077-2089 (2021)\n",
      "ML 729: Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization -- IEEE Trans. Dependable Secur. Comput. 18(5): 2088-2105 (2021)\n",
      "ML 730: Text Backdoor Detection Using an Interpretable RNN Abstract Model -- IEEE Trans. Inf. Forensics Secur. 16: 4117-4132 (2021)\n",
      "ML 731: Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems -- IEEE Trans. Inf. Forensics Secur. 16: 4772-4787 (2021)\n",
      "ML 732: Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures -- IEEE Trans. Inf. Forensics Secur. 16: 4827-4842 (2021)\n",
      "ML 733: Embedding asymmetric backdoors into the RSA key generator -- J. Comput. Virol. Hacking Tech. 17(1): 37-46 (2021)\n",
      "ML 734: Backdoor Decomposable Monotone Circuits and Propagation Complete Encodings -- AAAI 2021: 3832-3840\n",
      "ML 735: DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation -- AAAI 2021: 10585-10593\n",
      "ML 736: Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger -- ACL/IJCNLP (1) 2021: 443-453\n",
      "ML 737: Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution -- ACL/IJCNLP (1) 2021: 4873-4883\n",
      "ML 738: Rethinking Stealthiness of Backdoor Attack against NLP Models -- ACL/IJCNLP (1) 2021: 5543-5557\n",
      "ML 739: MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks -- ACM TUR-C 2021: 104-109\n",
      "ML 740: BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements -- ACSAC 2021: 554-569\n",
      "ML 741: DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation -- AsiaCCS 2021: 363-377\n",
      "ML 742: A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples -- IEEE BigData 2021: 834-846\n",
      "ML 743: Covid-19 digital Contact-tracing: a doorway to well-being or a backdoor to security vulnerabilities -- IEEE BigData 2021: 4297-4302\n",
      "ML 744: Countermeasures Against Backdoor Attacks Towards Malware Detectors -- CANS 2021: 295-314\n",
      "ML 745: Hidden Backdoors in Human-Centric Language Models -- CCS 2021: 3123-3140\n",
      "ML 746: Backdoor Pre-trained Models Can Transfer to All -- CCS 2021: 3141-3158\n",
      "ML 747: A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems -- CDC 2021: 4394-4399\n",
      "ML 748: A physically realizable backdoor attack on 3D point cloud deep learning: work-in-progress -- CODES+ISSS 2021: 27-28\n",
      "ML 749: Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger -- CollaborateCom (2) 2021: 276-296\n",
      "ML 750: Reasoning Short Cuts in Infinite Domain Constraint Satisfaction: Algorithms and Lower Bounds for Backdoors -- CP 2021: 32:1-32:20\n",
      "ML 751: Backdoor Attacks Against Deep Learning Systems in the Physical World -- CVPR 2021: 6206-6215\n",
      "ML 752: Protecting Deep Cerebrospinal Fluid Cell Image Processing Models with Backdoor and Semi-Distillation -- DICTA 2021: 1-7\n",
      "ML 753: A Random Multi-target Backdooring Attack on Deep Neural Networks -- DMBD (2) 2021: 45-52\n",
      "ML 754: Backdoor Filter: Mitigating Visible Backdoor Triggers in Dataset -- DTPI 2021: 102-105\n",
      "ML 755: BFClass: A Backdoor-free Text Classification Framework -- EMNLP (Findings) 2021: 444-453\n",
      "ML 756: Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning -- EMNLP (1) 2021: 3023-3032\n",
      "ML 757: ONION: A Simple and Effective Defense Against Textual Backdoor Attacks -- EMNLP (1) 2021: 9558-9566\n",
      "ML 758: Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer -- EMNLP (1) 2021: 4569-4580\n",
      "ML 759: RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models -- EMNLP (1) 2021: 8365-8381\n",
      "ML 760: Generative strategy based backdoor attacks to 3D point clouds: work-in-progress -- EMSOFT 2021: 23-24\n",
      "ML 761: Pixdoor: A Pixel-space Backdoor Attack on Deep Learning Models -- EUSIPCO 2021: 681-685\n",
      "ML 762: Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack -- GLOBECOM 2021: 1-6\n",
      "ML 763: Why is Your Trojan NOT Responding? A Quantitative Analysis of Failures in Backdoor Attacks of Neural Networks -- ICA3PP (3) 2021: 754-771\n",
      "ML 764: Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff -- ICASSP 2021: 3855-3859\n",
      "ML 765: L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access to the Training Set -- ICASSP 2021: 3745-3749\n",
      "ML 766: Backdoor Attack Against Speaker Verification -- ICASSP 2021: 2560-2564\n",
      "ML 767: LIRA: Learnable, Imperceptible and Robust Backdoor Attacks -- ICCV 2021: 11946-11956\n",
      "ML 768: Black-box Detection of Backdoor Attacks with Limited Information and Data -- ICCV 2021: 16462-16471\n",
      "ML 769: PointBA: Towards Backdoor Attacks in 3D Point Cloud -- ICCV 2021: 16472-16481\n",
      "ML 770: Invisible Backdoor Attack with Sample-Specific Triggers -- ICCV 2021: 16443-16452\n",
      "ML 771: A Backdoor Attack against 3D Point Cloud Classifiers -- ICCV 2021: 7577-7587\n",
      "ML 772: Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective -- ICCV 2021: 16453-16461\n",
      "ML 773: CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks -- ICCV 2021: 16433-16442\n",
      "ML 774: Backdoor Investigation and Incident Response: From Zero to Profit -- ICDF2C 2021: 229-247\n",
      "ML 775: Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks -- ICIP 2021: 3023-3027\n",
      "ML 776: Simtrojan: Stealthy Backdoor Attack -- ICIP 2021: 819-823\n",
      "ML 777: Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks -- ICLR 2021\n",
      "ML 778: WaNet - Imperceptible Warping-based Backdoor Attack -- ICLR 2021\n",
      "ML 779: Defense against backdoor attacks via robust covariance estimation -- ICML 2021: 4129-4139\n",
      "ML 780: Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks -- ICML 2021: 9389-9398\n",
      "ML 781: Backdoor Scanning for Deep Neural Networks through K-Arm Optimization -- ICML 2021: 9525-9536\n",
      "ML 782: ROWBACK: RObust Watermarking for neural networks using BACKdoors -- ICMLA 2021: 1728-1735\n",
      "ML 783: DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection -- ICSE 2021: 263-274\n",
      "ML 784: On the Robustness of Backdoor-based Watermarking in Deep Neural Networks -- IH&MMSec 2021: 177-188\n",
      "ML 785: Backdoor DNFs -- IJCAI 2021: 1403-1409\n",
      "ML 786: BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning -- IJCAI 2021: 3699-3705\n",
      "ML 787: Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks -- INFOCOM 2021: 1-10\n",
      "ML 788: AdvDoor: adversarial backdoor attack of deep learning system -- ISSTA 2021: 127-138\n",
      "ML 789: TridentShell: a Covert and Scalable Backdoor Injection Attack on Web Applications -- ISC 2021: 177-194\n",
      "ML 790: On-line Functional Testing of Memristor-mapped Deep Neural Networks using Backdoored Checksums -- ITC 2021: 83-92\n",
      "ML 791: The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game -- IUI 2021: 423-433\n",
      "ML 792: What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors -- KDD 2021: 1027-1035\n",
      "ML 793: Recursive Backdoors for SAT -- MFCS 2021: 73:1-73:18\n",
      "ML 794: Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation -- ACM Multimedia 2021: 826-834\n",
      "ML 795: Backdoor Attack with Imperceptible Input and Latent Modification -- NeurIPS 2021: 18944-18957\n",
      "ML 796: Anti-Backdoor Learning: Training Clean Models on Poisoned Data -- NeurIPS 2021: 14900-14912\n",
      "ML 797: Excess Capacity and Backdoor Poisoning -- NeurIPS 2021: 20373-20384\n",
      "ML 798: Adversarial Neuron Pruning Purifies Backdoored Deep Models -- NeurIPS 2021: 16913-16925\n",
      "ML 799: Identifying and blocking the backdoors in Linux -- RTA-CSIT 2021: 193-197\n",
      "ML 800: Backdoor Attacks to Graph Neural Networks -- SACMAT 2021: 15-26\n",
      "ML 801: A Textual Clean-Label Backdoor Attack Strategy against Spam Detection -- SIN 2021: 1-8\n",
      "ML 802: Robust Backdoor Attacks against Deep Neural Networks in Real Physical World -- TrustCom 2021: 620-626\n",
      "ML 803: A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks -- UbiSec 2021: 1-12\n",
      "ML 804: Blind Backdoors in Deep Learning Models -- USENIX Security Symposium 2021: 1505-1521\n",
      "ML 805: Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers -- USENIX Security Symposium 2021: 1487-1504\n",
      "ML 806: Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection -- USENIX Security Symposium 2021: 1541-1558\n",
      "ML 807: Graph Backdoor -- USENIX Security Symposium 2021: 1523-1540\n",
      "ML 808: Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper -- WiseML@WiSec 2021: 37-42\n",
      "ML 809: Explainability-based Backdoor Attacks Against Graph Neural Networks -- WiseML@WiSec 2021: 31-36\n",
      "ML 810: Explainability Matters: Backdoor Attacks on Medical Imaging -- CoRR abs/2101.00008 (2021)\n",
      "ML 811: Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks -- CoRR abs/2101.06969 (2021)\n",
      "ML 812: On Provable Backdoor Defense in Collaborative Learning -- CoRR abs/2101.08177 (2021)\n",
      "ML 813: What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors -- CoRR abs/2102.13624 (2021)\n",
      "ML 814: DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations -- CoRR abs/2103.02079 (2021)\n",
      "ML 815: Hidden Backdoor Attack against Semantic Segmentation Models -- CoRR abs/2103.04038 (2021)\n",
      "ML 816: EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry -- CoRR abs/2103.08820 (2021)\n",
      "ML 817: TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation -- CoRR abs/2103.10274 (2021)\n",
      "ML 818: RABA: A Robust Avatar Backdoor Attack on Deep Neural Network -- CoRR abs/2104.01026 (2021)\n",
      "ML 819: Backdoor Attack in the Physical World -- CoRR abs/2104.02361 (2021)\n",
      "ML 820: SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics -- CoRR abs/2104.11315 (2021)\n",
      "ML 821: Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds -- CoRR abs/2105.04839 (2021)\n",
      "ML 822: Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions -- CoRR abs/2106.07214 (2021)\n",
      "ML 823: Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks -- CoRR abs/2106.07925 (2021)\n",
      "ML 824: Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting -- CoRR abs/2107.07240 (2021)\n",
      "ML 825: Can You Hear It? Backdoor Attacks via Ultrasonic Triggers -- CoRR abs/2107.14569 (2021)\n",
      "ML 826: The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks -- CoRR abs/2108.01644 (2021)\n",
      "ML 827: Quantization Backdoors to Deep Learning Models -- CoRR abs/2108.09187 (2021)\n",
      "ML 828: TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis -- CoRR abs/2108.10132 (2021)\n",
      "ML 829: Backdoor Attacks on Network Certification via Data Poisoning -- CoRR abs/2108.11299 (2021)\n",
      "ML 830: Backdoor Attack and Defense for Deep Regression -- CoRR abs/2109.02381 (2021)\n",
      "ML 831: Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain -- CoRR abs/2109.05507 (2021)\n",
      "ML 832: Clean-label Backdoor Attack against Deep Hashing based Retrieval -- CoRR abs/2109.08868 (2021)\n",
      "ML 833: Widen The Backdoor To Let More Attackers In -- CoRR abs/2110.04571 (2021)\n",
      "ML 834: An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware -- CoRR abs/2110.07683 (2021)\n",
      "ML 835: A Statistical Difference Reduction Method for Escaping Backdoor Detection -- CoRR abs/2111.05077 (2021)\n",
      "ML 836: An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences -- CoRR abs/2111.08429 (2021)\n",
      "ML 837: Backdoor Attack through Frequency Domain -- CoRR abs/2111.10991 (2021)\n",
      "ML 838: NTD: Non-Transferability Enabled Backdoor Detection -- CoRR abs/2111.11157 (2021)\n",
      "ML 839: DBIA: Data-free Backdoor Injection Attack against Transformer Networks -- CoRR abs/2111.11870 (2021)\n",
      "ML 840: A General Framework for Defending Against Backdoor Attacks via Influence Graph -- CoRR abs/2111.14309 (2021)\n",
      "ML 841: How to Backdoor a Cipher -- IACR Cryptol. ePrint Arch. 2021: 442 (2021)\n",
      "ML 842: Factoring Primes to Factor Moduli: Backdooring and Distributed Generation of Semiprimes -- IACR Cryptol. ePrint Arch. 2021: 1610 (2021)\n",
      "ML 843: (Withdrawn) Spinning Sequence-to-Sequence Models with Meta-Backdoors -- CoRR abs/2107.10443 (2021)\n",
      "ML 844: (Withdrawn) CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing -- CoRR abs/2112.13064 (2021)\n",
      "ML 845: Cryptographic Primitives that Resist Backdooring and Subversion -- Technical University of Darmstadt, Germany, 2020\n",
      "ML 846: Detecting Backdoor Attacks via Class Difference in Deep Neural Networks -- IEEE Access 8: 191049-191056 (2020)\n",
      "ML 847: Backdoor Suppression in Neural Networks using Input Fuzzing and Majority Voting -- IEEE Des. Test 37(2): 103-110 (2020)\n",
      "ML 848: Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition -- EURASIP J. Inf. Secur. 2020: 12 (2020)\n",
      "ML 849: Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks -- IEICE Trans. Inf. Syst. 103-D(4): 883-887 (2020)\n",
      "ML 850: Backdoors into Two Occurrences -- J. Satisf. Boolean Model. Comput. 12(1): 1-15 (2020)\n",
      "ML 851: Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments -- IEEE Netw. 34(5): 141-147 (2020)\n",
      "ML 852: Hidden Trigger Backdoor Attacks -- AAAI 2020: 11957-11965\n",
      "ML 853: Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems -- ACM TUR-C 2020: 231-235\n",
      "ML 854: Optimizing Deep Learning Based Intrusion Detection Systems Defense Against White-Box and Backdoor Adversarial Attacks Through a Genetic Algorithm -- AIPR 2020: 1-8\n",
      "ML 855: Differentiable Causal Backdoor Discovery -- AISTATS 2020: 3970-3979\n",
      "ML 856: Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features -- CCS 2020: 113-131\n",
      "ML 857: Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks -- AISec@CCS 2020: 117-127\n",
      "ML 858: Can Adversarial Weight Perturbations Inject Neural Backdoors -- CIKM 2020: 2029-2032\n",
      "ML 859: Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation -- CODASPY 2020: 97-108\n",
      "ML 860: The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers -- CRYPTO (3) 2020: 249-278\n",
      "ML 861: The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers -- IACR Cryptol. ePrint Arch. 2020: 986 (2020)\n",
      "ML 862: Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs -- CVPR 2020: 298-307\n",
      "ML 863: Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers -- CVPR Workshops 2020: 3422-3431\n",
      "ML 864: Clean-Label Backdoor Attacks on Video Recognition Models -- CVPR 2020: 14431-14440\n",
      "ML 865: TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning -- DAC 2020: 1-6\n",
      "ML 866: One-Pixel Signature: Characterizing CNN Models for Backdoor Detection -- ECCV (27) 2020: 326-341\n",
      "ML 867: Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks -- ECCV (10) 2020: 182-199\n",
      "ML 868: Interpretability Derived Backdoor Attacks Detection in Deep Neural Networks: Work-in-Progress -- EMSOFT 2020: 13-14\n",
      "ML 869: Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating -- EuroS&P 2020: 184-197\n",
      "ML 870: Bypassing Backdoor Detection Algorithms in Deep Learning -- EuroS&P 2020: 175-183\n",
      "ML 871: Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification -- ICASSP 2020: 3827-3831\n",
      "ML 872: Backdooring Convolutional Neural Networks via Targeted Weight Perturbations -- IJCB 2020: 1-9\n",
      "ML 873: Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks -- ICDM 2020: 162-171\n",
      "ML 874: TargetNet Backdoor: Attack on Deep Neural Network with Use of Different Triggers -- ICIIT 2020: 140-145\n",
      "ML 875: A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks -- ICISS 2020: 69-80\n",
      "ML 876: Backdooring Deep Learning Architectures: Threats and (some) Opportunities -- ICISSP 2020: 15-16\n",
      "ML 877: Robust anomaly detection and backdoor attack detection via differential privacy -- ICLR 2020\n",
      "ML 878: Segmentation Based Backdoor Attack Detection -- ICMLC 2020: 298-302\n",
      "ML 879: Removing Backdoor-Based Watermarks in Neural Networks with Limited Data -- ICPR 2020: 10149-10156\n",
      "ML 880: FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network -- ICSIM 2020: 53-57\n",
      "ML 881: Application of complex systems in neural networks against Backdoor attacks -- ICTC 2020: 57-59\n",
      "ML 882: Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks -- IJCNN 2020: 1-8\n",
      "ML 883: Defending Deep Learning Based Anomaly Detection Systems Against White-Box Adversarial Examples and Backdoor Attacks -- ISTAS 2020: 294-301\n",
      "ML 884: A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model -- MLHC 2020: 376-396\n",
      "ML 885: Revealing Perceptible Backdoors in DNNs, Without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic -- MLSP 2020: 1-6\n",
      "ML 886: GangSweep: Sweep out Neural Backdoors by GAN -- ACM Multimedia 2020: 3173-3181\n",
      "ML 887: Input-Aware Dynamic Backdoor Attack -- NeurIPS 2020\n",
      "ML 888: On the Trade-off between Adversarial and Backdoor Robustness -- NeurIPS 2020\n",
      "ML 889: Scalable Backdoor Detection in Neural Networks -- ECML/PKDD (2) 2020: 289-304\n",
      "ML 890: Escaping Backdoor Attack Detection of Deep Learning -- SEC 2020: 431-445\n",
      "ML 891: Backdooring and Poisoning Neural Networks with Image-Scaling Attacks -- SP (Workshops) 2020: 41-47\n",
      "ML 892: Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity -- TCC (3) 2020: 241-273\n",
      "ML 893: Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity -- IACR Cryptol. ePrint Arch. 2020: 1199 (2020)\n",
      "ML 894: Detecting acoustic backdoor transmission of inaudible messages using deep learning -- WiseML@WiSec 2020: 80-85\n",
      "ML 895: NNoculation: Broad Spectrum and Targeted Treatment of Backdoored DNNs -- CoRR abs/2002.08313 (2020)\n",
      "ML 896: On Certifying Robustness against Backdoor Attacks via Randomized Smoothing -- CoRR abs/2002.11750 (2020)\n",
      "ML 897: Defending against Backdoor Attack on Deep Neural Networks -- CoRR abs/2002.12162 (2020)\n",
      "ML 898: Exposing Backdoors in Robust Machine Learning Models -- CoRR abs/2003.00865 (2020)\n",
      "ML 899: Exploring Backdoor Poisoning Attacks Against Malware Classifiers -- CoRR abs/2003.01031 (2020)\n",
      "ML 900: Watch your back: Backdoor Attacks in Deep Reinforcement Learning-based Autonomous Vehicle Control Systems -- CoRR abs/2003.07859 (2020)\n",
      "ML 901: Rethinking the Trigger of Backdoor Attack -- CoRR abs/2004.04692 (2020)\n",
      "ML 902: BadNL: Backdoor Attacks Against NLP Models -- CoRR abs/2006.01043 (2020)\n",
      "ML 903: A new measure for overfitting and its implications for backdooring of deep learning -- CoRR abs/2006.06721 (2020)\n",
      "ML 904: FaceHack: Triggering backdoored facial recognition systems using facial characteristics -- CoRR abs/2006.11623 (2020)\n",
      "ML 905: Backdoor Attacks on Facial Recognition in the Physical World -- CoRR abs/2006.14580 (2020)\n",
      "ML 906: Natural Backdoor Attack on Text Data -- CoRR abs/2006.16176 (2020)\n",
      "ML 907: Backdoor attacks and defenses in feature-partitioned collaborative learning -- CoRR abs/2007.03608 (2020)\n",
      "ML 908: Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review -- CoRR abs/2007.10760 (2020)\n",
      "ML 909: Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection -- CoRR abs/2007.14748 (2020)\n",
      "ML 910: Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks -- CoRR abs/2008.00123 (2020)\n",
      "ML 911: Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems -- CoRR abs/2009.06996 (2020)\n",
      "ML 912: What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors -- CoRR abs/2009.10639 (2020)\n",
      "ML 913: BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models -- CoRR abs/2010.03007 (2020)\n",
      "ML 914: Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks -- CoRR abs/2010.03282 (2020)\n",
      "ML 915: Open-sourced Dataset Protection via Backdoor Watermarking -- CoRR abs/2010.05821 (2020)\n",
      "ML 916: Poisoned classifiers are not only backdoored, they are fundamentally broken -- CoRR abs/2010.09080 (2020)\n",
      "ML 917: On Evaluating Neural Network Backdoor Defenses -- CoRR abs/2010.12186 (2020)\n",
      "ML 918: EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks -- CoRR abs/2011.00101 (2020)\n",
      "ML 919: Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection -- CoRR abs/2011.02526 (2020)\n",
      "ML 920: Effect of backdoor attacks over the complexity of the latent space distribution -- CoRR abs/2012.01931 (2020)\n",
      "ML 921: Backdoor Attack with Sample-Specific Triggers -- CoRR abs/2012.03816 (2020)\n",
      "ML 922: HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios -- CoRR abs/2012.07474 (2020)\n",
      "ML 923: TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask) -- CoRR abs/2012.09302 (2020)\n",
      "ML 924: A Backdoor Attack Against LSTM-Based Text Classification Systems -- IEEE Access 7: 138872-138878 (2019)\n",
      "ML 925: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks -- IEEE Access 7: 47230-47244 (2019)\n",
      "ML 926: Backdoors to planning -- Artif. Intell. 269: 49-75 (2019)\n",
      "ML 927: Backdoors to Planning -- AAAI 2014: 2300-2307\n",
      "ML 928: Backdoors for Linear Temporal Logic -- Algorithmica 81(2): 476-496 (2019)\n",
      "ML 929: Backdoors for Linear Temporal Logic -- IPEC 2016: 23:1-23:17\n",
      "ML 930: Cryptography with Disposable Backdoors -- Cryptogr. 3(3): 22 (2019)\n",
      "ML 931: Testing the Human Backdoor: Organizational Response to a Phishing Campaign -- J. Univers. Comput. Sci. 25(11): 1458-1477 (2019)\n",
      "ML 932: Interbank Networks and Backdoor Bailouts: Benefiting from Other Banks' Government Guarantees -- Manag. Sci. 65(8): 3673-3693 (2019)\n",
      "ML 933: Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering -- SafeAI@AAAI 2019\n",
      "ML 934: Latent Backdoor Attacks on Deep Neural Networks -- CCS 2019: 2041-2055\n",
      "ML 935: Backdoor Attacks in Neural Networks - A Systematic Evaluation on Multiple Traffic Sign Datasets -- CD-MAKE 2019: 285-300\n",
      "ML 936: Walling up Backdoors in Intrusion Detection Systems -- Big-DAMA@CoNEXT 2019: 8-13\n",
      "ML 937: I Want to Break Square-free: The 4p - 1 Factorization Method and Its RSA Backdoor Viability -- ICETE (2) 2019: 25-36\n",
      "ML 938: A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning -- ICIP 2019: 101-105\n",
      "ML 939: How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts -- ITCS 2019: 42:1-42:20\n",
      "ML 940: How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts -- IACR Cryptol. ePrint Arch. 2018: 212 (2018)\n",
      "ML 941: Is Backside the New Backdoor in Modern SoCs?: Invited Paper -- ITC 2019: 1-10\n",
      "ML 942: Towards Leveraging Backdoors in Qualitative Constraint Networks -- KI 2019: 308-315\n",
      "ML 943: A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense -- MLSP 2019: 1-6\n",
      "ML 944: Luminance-based video backdoor attack against anti-spoofing rebroadcast detection -- MMSP 2019: 1-6\n",
      "ML 945: Defending Neural Backdoors via Generative Distribution Modeling -- NeurIPS 2019: 14004-14013\n",
      "ML 946: On Embedding Backdoor in Malware Detectors Using Machine Learning -- PST 2019: 1-5\n",
      "ML 947: Existence Versus Exploitation: The Opacity of Backdoors and Backbones Under a Weak Assumption -- SOFSEM 2019: 247-259\n",
      "ML 948: True2F: Backdoor-Resistant Authentication Tokens -- IEEE Symposium on Security and Privacy 2019: 398-416\n",
      "ML 949: Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks -- IEEE Symposium on Security and Privacy 2019: 707-723\n",
      "ML 950: Design of intentional backdoors in sequential models -- CoRR abs/1902.09972 (2019)\n",
      "ML 951: BSEA-1 - A Stream Cipher Backdooring Technique -- CoRR abs/1903.11063 (2019)\n",
      "ML 952: Adversarial Audio: A New Information Hiding Method and Backdoor for DNN-based Speech Recognition Models -- CoRR abs/1904.03829 (2019)\n",
      "ML 953: Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks -- CoRR abs/1905.10447 (2019)\n",
      "ML 954: On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks -- CoRR abs/1906.07745 (2019)\n",
      "ML 955: TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems -- CoRR abs/1908.01763 (2019)\n",
      "ML 956: Invisible Backdoor Attacks Against Deep Neural Networks -- CoRR abs/1909.02742 (2019)\n",
      "ML 957: The Tale of Evil Twins: Adversarial Inputs versus Backdoored Models -- CoRR abs/1911.01559 (2019)\n",
      "ML 958: NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations -- CoRR abs/1911.07399 (2019)\n",
      "ML 959: Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic -- CoRR abs/1911.07970 (2019)\n",
      "ML 960: Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks -- CoRR abs/1911.08040 (2019)\n",
      "ML 961: Label-Consistent Backdoor Attacks -- CoRR abs/1912.02771 (2019)\n",
      "ML 962: Backdoor detection systems for embedded devices -- University of Birmingham, UK, 2018\n",
      "ML 963: Technical perspective: Backdoor engineering -- Commun. ACM 61(11): 147 (2018)\n",
      "ML 964: On Cryptographic Attacks Using Backdoors for SAT -- AAAI 2018: 6641-6648\n",
      "ML 965: Real-time Detection of Passive Backdoor Behaviors on Android System -- CNS 2018: 1-9\n",
      "ML 966: Learning-Sensitive Backdoors with Restarts -- CP 2018: 453-469\n",
      "ML 967: From Backdoor Key to Backdoor Completability: Improving a Known Measure of Hardness for the Satisfiable CSP -- CPAIOR 2018: 198-214\n",
      "ML 968: Combiners for Backdoored Random Oracles -- CRYPTO (2) 2018: 272-302\n",
      "ML 969: Combiners for Backdoored Random Oracles -- IACR Cryptol. ePrint Arch. 2018: 770 (2018)\n",
      "ML 970: Backdoored Hash Functions: Immunizing HMAC and HKDF -- CSF 2018: 105-118\n",
      "ML 971: Backdoored Hash Functions: Immunizing HMAC and HKDF -- IACR Cryptol. ePrint Arch. 2018: 362 (2018)\n",
      "ML 972: Remote Desktop Backdoor Implementation with Reverse TCP Payload Using Open Source Tools for Instructional Use -- EIT 2018: 249-254\n",
      "ML 973: Backdoor Attacks on Neural Network Operations -- GlobalSIP 2018: 1154-1158\n",
      "ML 974: UFO - Hidden Backdoor Discovery and Security Verification in IoT Device Firmware -- ISSRE Workshops 2018: 18-23\n",
      "ML 975: Spectral Signatures in Backdoor Attacks -- NeurIPS 2018: 8011-8021\n",
      "ML 976: Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks -- RAID 2018: 273-294\n",
      "ML 977: Backdoors: Definition, Deniability and Detection -- RAID 2018: 92-113\n",
      "ML 978: ALIAS: A Modular Tool for Finding Backdoors for SAT -- SAT 2018: 419-427\n",
      "ML 979: Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring -- USENIX Security Symposium 2018: 1615-1631\n",
      "ML 980: Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings -- CoRR abs/1811.09435 (2018)\n",
      "ML 981: How a simple bug in ML compiler could be exploited for backdoors -- CoRR abs/1811.10851 (2018)\n",
      "ML 982: Cryptography with Dispensable Backdoors -- IACR Cryptol. ePrint Arch. 2018: 352 (2018)\n",
      "ML 983: On the Existence of Non-Linear Invariants and Algebraic Polynomial Constructive Approach to Backdoors in Block Ciphers -- IACR Cryptol. ePrint Arch. 2018: 807 (2018)\n",
      "ML 984: Backdoors into heterogeneous classes of SAT and CSP -- J. Comput. Syst. Sci. 85: 38-56 (2017)\n",
      "ML 985: Backdoors into Heterogeneous Classes of SAT and CSP -- AAAI 2014: 2652-2658\n",
      "ML 986: Open Sesame! Design and Implementation of Backdoor to Secretly Unlock Android Devices -- J. Internet Serv. Inf. Secur. 7(4): 35-44 (2017)\n",
      "ML 987: BackDoor: Sounds that a microphone can record, but that humans can't hear -- GetMobile Mob. Comput. Commun. 21(4): 25-29 (2017)\n",
      "ML 988: Backdoor attacks against learning systems -- CNS 2017: 1-9\n",
      "ML 989: Stringer: Measuring the Importance of Static Data Comparisons to Detect Backdoors and Undocumented Functionality -- ESORICS (2) 2017: 513-531\n",
      "ML 990: Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher -- ICISSP 2017: 622-631\n",
      "ML 991: Low-cost detection of backdoor malware -- ICITST 2017: 197-198\n",
      "ML 992: Backdoor Trees for Answer Set Programming -- ASPOCP@LPNMR 2017\n",
      "ML 993: BackDoor: Making Microphones Hear Inaudible Sounds -- MobiSys 2017: 2-14\n",
      "ML 994: Indiscreet Logs: Diffie-Hellman Backdoors in TLS -- NDSS 2017\n",
      "ML 995: Backdoor Treewidth for SAT -- SAT 2017: 20-37\n",
      "ML 996: Combining Treewidth and Backdoors for CSP -- STACS 2017: 36:1-36:17\n",
      "ML 997: Backdoor Sets for CSP -- The Constraint Satisfaction Problem 2017: 137-157\n",
      "ML 998: The Opacity of Backbones and Backdoors Under a Weak Assumption -- CoRR abs/1706.04582 (2017)\n",
      "ML 999: Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning -- CoRR abs/1712.05526 (2017)\n",
      "ML 1000: Backdoors to q-Horn -- Algorithmica 74(1): 540-557 (2016)\n",
      "ML 1001: Backdoors to q-Horn -- STACS 2013: 67-79\n",
      "ML 1002: No backdoor required or expected -- Commun. ACM 59(6): 8-9 (2016)\n",
      "ML 1003: Detecting Stealthy Backdoors and Port Knocking Sequences through Flow Analysis -- Prax. Inf.verarb. Kommun. 38(3-4): 97-104 (2016)\n",
      "ML 1004: Backdoors to Tractable Valued CSP -- CP 2016: 233-250\n",
      "ML 1005: Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results -- CRYPTO (1) 2016: 403-432\n",
      "ML 1006: Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results -- IACR Cryptol. ePrint Arch. 2016: 577 (2016)\n",
      "ML 1007: Controlled Randomness - A Defense Against Backdoors in Cryptographic Devices -- Mycrypt 2016: 215-232\n",
      "ML 1008: Strong Backdoors for Default Logic -- SAT 2016: 45-59\n",
      "ML 1009: Backdoors to SAT -- Encyclopedia of Algorithms 2016: 167-170\n",
      "ML 1010: Strong Backdoors for Linear Temporal Logic -- CoRR abs/1602.04934 (2016)\n",
      "ML 1011: A Formal Treatment of Backdoored Pseudorandom Generators -- IACR Cryptol. ePrint Arch. 2016: 306 (2016)\n",
      "ML 1012: A Formal Treatment of Backdoored Pseudorandom Generators -- EUROCRYPT (1) 2015: 101-126\n",
      "ML 1013: Indiscreet Logs: Persistent Diffie-Hellman Backdoors in TLS -- IACR Cryptol. ePrint Arch. 2016: 999 (2016)\n",
      "ML 1014: DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors -- IACR Cryptol. ePrint Arch. 2016: 245 (2016)\n",
      "ML 1015: How to Backdoor Diffie-Hellman -- IACR Cryptol. ePrint Arch. 2016: 644 (2016)\n",
      "ML 1016: Backdoors to tractable answer set programming -- Artif. Intell. 220: 64-103 (2015)\n",
      "ML 1017: Backdoors to Normality for Disjunctive Logic Programs -- ACM Trans. Comput. Log. 17(1): 7 (2015)\n",
      "ML 1018: Backdoors to Normality for Disjunctive Logic Programs -- AAAI 2013: 320-327\n",
      "ML 1019: Variable-Deletion Backdoors to Planning -- AAAI 2015: 3305-3312\n",
      "ML 1020: Internet-facing PLCs as a network backdoor -- CNS 2015: 524-532\n",
      "ML 1021: Netzbasierte Erkennung von mittels Port Knocking verstecksten Dienstern und Backdoors -- DFN-Forum Kommunikationstechnologien 2015: 57-67\n",
      "ML 1022: Integrated Sensor: A Backdoor for Hardware Trojan Insertions -- DSD 2015: 415-422\n",
      "ML 1023: Covert remote syscall communication at kernel level: A SPOOKY backdoor -- MALWARE 2015: 74-81\n",
      "ML 1024: Devil in a box: Installing backdoors in electronic door locks -- PST 2015: 139-144\n",
      "ML 1025: Solving d-SAT via Backdoors to Small Treewidth -- SODA 2015: 630-641\n",
      "ML 1026: Tradeoffs in the complexity of backdoors to satisfiability: dynamic sub-solvers and learning during search -- Ann. Math. Artif. Intell. 70(4): 399-431 (2014)\n",
      "ML 1027: Backdoor -- Datenschutz und Datensicherheit 38(2): 119 (2014)\n",
      "ML 1028: On Backdoors to Tractable Constraint Languages -- CP 2014: 224-239\n",
      "ML 1029: Answer Set Solver Backdoors -- JELIA 2014: 674-683\n",
      "ML 1030: Backdoor Branching -- INFORMS J. Comput. 25(4): 693-700 (2013)\n",
      "ML 1031: Backdoor Branching -- IPCO 2011: 183-191\n",
      "ML 1032: Backdoors to the Tractability of Answer Set Programming -- Theory Pract. Log. Program. 13(4-5-Online-Supplement) (2013)\n",
      "ML 1033: Backdoors to Tractability of Answer-Set Programming -- AAAI 2013: 1662-1663\n",
      "ML 1034: Implementation and implications of a stealth hard-drive backdoor -- ACSAC 2013: 279-288\n",
      "ML 1035: Towards reducing the attack surface of software backdoors -- CCS 2013: 851-862\n",
      "ML 1036: Preventing Backdoors in Server Applications with a Separated Software Architecture - (Short Paper) -- DIMVA 2013: 197-206\n",
      "ML 1037: Strong Backdoors to Bounded Treewidth SAT -- FOCS 2013: 489-498\n",
      "ML 1038: Crowdsourcing Backdoor Identification for Combinatorial Optimization -- IJCAI 2013: 2840-2847\n",
      "ML 1039: Backdoors to Abduction -- IJCAI 2013: 1046-1052\n",
      "ML 1040: Upper and Lower Bounds for Weak Backdoor Set Detection -- SAT 2013: 394-402\n",
      "ML 1041: Vulnerability-Based Backdoors: Threats from Two-step Trojans -- SERE 2013: 169-177\n",
      "ML 1042: A generalized backdoor criterion -- CoRR abs/1307.5636 (2013)\n",
      "ML 1043: Backdoors to Satisfaction -- The Multivariate Algorithmic Revolution and Beyond 2012: 287-317\n",
      "ML 1044: Breakthrough Silicon Scanning Discovers Backdoor in Military Chip -- CHES 2012: 23-40\n",
      "ML 1045: Backdoors to Acyclic SAT -- ICALP (1) 2012: 363-374\n",
      "ML 1046: Detecting Stealthy Backdoors with Association Rule Mining -- Networking (2) 2012: 161-171\n",
      "ML 1047: Strong Backdoors to Nested Satisfiability -- SAT 2012: 72-85\n",
      "ML 1048: A Framework to Eliminate Backdoors from Response-Computable Authentication -- IEEE Symposium on Security and Privacy 2012: 3-17\n",
      "ML 1049: Finding Small Backdoors in SAT Instances -- Canadian AI 2011: 269-280\n",
      "ML 1050: Trusting the open latent IC backdoors -- STC@CCS 2011: 1-2\n",
      "ML 1051: TorusDesktop: pointing via the backdoor is sometimes shorter -- CHI 2011: 829-838\n",
      "ML 1052: Backdoors to Tractable Answer-Set Programming -- IJCAI 2011: 863-868\n",
      "ML 1053: Silencing Hardware Backdoors -- IEEE Symposium on Security and Privacy 2011: 49-63\n",
      "ML 1054: Static detection of application backdoors - Detecting both malicious software behavior and malicious indicators from the static analysis of executable code -- Datenschutz und Datensicherheit 34(3): 149-155 (2010)\n",
      "ML 1055: Simple Backdoors on RSA Modulus by Using RSA Vulnerability -- IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 92-A(9): 2326-2332 (2009)\n",
      "ML 1056: Backdoor Sets of Quantified Boolean Formulas -- J. Autom. Reason. 42(1): 77-97 (2009)\n",
      "ML 1057: Backdoor Sets of Quantified Boolean Formulas -- SAT 2007: 230-243\n",
      "ML 1058: Matched Formulas and Backdoor Sets -- J. Satisf. Boolean Model. Comput. 6(1-3): 1-12 (2009)\n",
      "ML 1059: Matched Formulas and Backdoor Sets -- SAT 2007: 94-99\n",
      "ML 1060: CPU bugs, CPU backdoors and consequences on security -- J. Comput. Virol. 5(2): 91-104 (2009)\n",
      "ML 1061: CPU Bugs, CPU Backdoors and Consequences on Security -- ESORICS 2008: 580-599\n",
      "ML 1062: A chipset level network backdoor: bypassing host-based firewall & IDS -- AsiaCCS 2009: 125-134\n",
      "ML 1063: Backdoors to Combinatorial Optimization: Feasibility and Optimality -- CPAIOR 2009: 56-70\n",
      "ML 1064: A study on intrusion protection techniques against Linux kernel backdoor -- ICHIT 2009: 86-90\n",
      "ML 1065: Backdoors in the Context of Learning -- SAT 2009: 73-79\n",
      "ML 1066: Backdoor Trees -- AAAI 2008: 363-368\n",
      "ML 1067: A New Empirical Study of Weak Backdoors -- CP 2008: 618-623\n",
      "ML 1068: Tradeoffs in Backdoors: Inconsistency Detection, Dynamic Simplification, and Preprocessing -- ISAIM 2008\n",
      "ML 1069: Computation of Renameable Horn Backdoors -- SAT 2008: 154-160\n",
      "ML 1070: A New Bound for an NP-Hard Subclass of 3-SAT Using Backdoors -- SAT 2008: 161-167\n",
      "ML 1071: Detecting and Guarding against Kernel Backdoors through Packet Flow Differentials -- IEICE Trans. Commun. 90-B(10): 2638-2645 (2007)\n",
      "ML 1072: A Timing-Resistant Elliptic Curve Backdoor in RSA -- Inscrypt 2007: 427-441\n",
      "ML 1073: Tradeoffs in the Complexity of Backdoor Detection -- CP 2007: 256-270\n",
      "ML 1074: From Horn Strong Backdoor Sets to Ordered Strong Backdoor Sets -- MICAI 2007: 105-117\n",
      "ML 1075: COTS and other electronic voting backdoors -- Commun. ACM 49(11): 112 (2006)\n",
      "ML 1076: Computing Horn Strong Backdoor Sets Thanks to Local Search -- ICTAI 2006: 139-143\n",
      "ML 1077: An Elliptic Curve Backdoor Algorithm for RSASSA -- Information Hiding 2006: 355-374\n",
      "ML 1078: Backdoor Sets for DLL Subsolvers -- J. Autom. Reason. 35(1-3): 73-88 (2005)\n",
      "ML 1079: Backbones and Backdoors in Satisfiability -- AAAI 2005: 1368-1373\n",
      "ML 1080: A Space Efficient Backdoor in RSA and Its Applications -- Selected Areas in Cryptography 2005: 128-143\n",
      "ML 1081: A self-checking signature scheme for checking backdoor security attacks in Internet -- J. High Speed Networks 13(4): 309-317 (2004)\n",
      "ML 1082: The Backdoor Key: A Path to Understanding Problem Hardness -- AAAI 2004: 124-130\n",
      "ML 1083: Backdoor Creativity: Collaborative Creativity in Technology Supported Teams -- COOP 2004: 99-114\n",
      "ML 1084: Remote Repair of Operating System State Using Backdoors -- ICAC 2004: 256-263\n",
      "ML 1085: Detecting Backdoor Sets with Respect to Horn and Binary Clauses -- SAT 2004\n",
      "ML 1086: Backdoor Attacks on Black-Box Ciphers Exploiting Low-Entropy Plaintexts -- ACISP 2003: 297-311\n",
      "ML 1087: Simple Backdoors for RSA Key Generation -- CT-RSA 2003: 403-416\n",
      "ML 1088: Automatic Backdoor Analysis with Network Intrusion Detection System and Integrated Service Checker -- IAW 2003: 122-126\n",
      "ML 1089: Backdoors To Typical Case Complexity -- IJCAI 2003: 1173-1178\n",
      "ML 1090: Simple backdoors to RSA key generation -- IACR Cryptol. ePrint Arch. 2002: 183 (2002)\n",
      "ML 1091: Detecting Backdoors -- USENIX Security Symposium 2000\n"
     ]
    }
   ],
   "source": [
    "for id, pp in enumerate(data_cat['ml']):\n",
    "    print(f\"ML {id}: {pp['title']} -- {pp['info']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
