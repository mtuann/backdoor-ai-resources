# Table of Contents
1. [Backdoor Learning Papers](#backdoor-learning-papers)
2. [Other topics](#other-topics)
3. [Backdoor Learning Papers with Code](#backdoor-learning-papers-with-code)


## Backdoor Learning Papers
This GitHub repository contains an updated list of Federated Learning papers as of **July 27, 2025**. 

- The resources are collected from various sources, including arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, NIPS, IEEE, ACM, Springer, ScienceDirect, Wiley, Nature, Science, and other top AI/ML conferences and journals.
- For a better reading experience, visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---
# Other Topics
Explore additional research papers on the following topics:
- For **Large Language Models** papers, please visit the [**LLM Repository**](https://github.com/mtuann/llm-updated-papers).
- For **Backdoor Learning** papers, please visit the [**Backdoor Learning Repository**](https://github.com/mtuann/backdoor-ai-resources).
- For **Federated Learning** papers, please visit the [**Federated Learning Repository**](https://github.com/mtuann/federated-learning-updated-papers).
- For **Machine Unlearning** papers, please visit the [**Machine Unlearning Repository**](https://github.com/mtuann/machine-unlearning-papers).

---

For contributions, inquiries, or suggestions, feel free to reach out via [email](mailto:tuannm0312@gmail.com).

---

If you find this application helpful and would like to support its development, you can buy me a coffee using one of the following methods:
- **Techcombank (Vietnam):** 5877 5555 55 (Nguyen Thi Lan Phuong)
- **PayPal or Credit/Debit Card:** [https://ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)

---

## Backdoor Learning Papers with Code
Due to GitHub repository limitations, this section includes only those papers that provide accompanying code, sorted by publish date. For access to the full list of papers, please visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---


|No.|Title|Authors|Publish Date|Venue|Code|URL|
|---|---|---|---|---|---|---|
|1|BackFed: An Efficient & Standardized Benchmark Suite for Backdoor
  Attacks in Federated Learning|Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/thinh-dao/BackFed.|http://arxiv.org/abs/2507.04903v1|
|2|PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning|Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/azure-123/PNAct.|http://arxiv.org/abs/2507.00485v1|
|3|CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset
  Separation|Binyan Xu, Fan Yang, Xilin Dai, Di Tang, Kehuan Zhang|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/binyxu/CGD.|http://arxiv.org/abs/2507.05113v1|
|4|TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor
  Attacks on Deep Reinforcement Learning|Songze Li, Mingxuan Zhang, Kang Wei, Shouling Ji|<li><span style="color:#FF5733;">2025-06-01</span></li>|arXiv|https://github.com/S3IC-Lab/TooBadRL.|https://doi.org/10.48550/arXiv.2506.09562|
|5|Dynamic Attention Analysis for Backdoor Detection in Text-to-Image
  Diffusion Models|Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen|<li><span style="color:#FF5733;">2025-04-01</span></li>|arXiv|https://github.com/Robin-WZQ/DAA.|https://doi.org/10.48550/arXiv.2504.20518|
|6|Propaganda via AI? A Study on Semantic Backdoors in Large Language
  Models|Nay Myat Min, Long H. Pham, Yige Li, Jun Sun|<li><span style="color:#FF5733;">2025-04-01</span></li>|arXiv|https://github.com/NayMyatMin/RAVEN.|https://doi.org/10.48550/arXiv.2504.12344|
|7|CBW: Towards Dataset Ownership Verification for Speaker Verification via
  Clustering-based Backdoor Watermarking|Yiming Li, Kaiying Yan, Shuo Shao, Tongqing Zhai, Shu-Tao Xia, Zhan Qin, Dacheng Tao|<li><span style="color:#FF5733;">2025-03-01</span></li>|arXiv|https://github.com/Radiant0726/CBW|https://doi.org/10.48550/arXiv.2503.05794|
|8|Detecting Backdoor Attacks in Federated Learning via Direction Alignment
  Inspection|Jiahao Xu, Zikai Zhang, Rui Hu|<li><span style="color:#FF5733;">2025-03-01</span></li>|CVPR|https://github.com/JiiahaoXU/AlignIns.|https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection_CVPR_2025_paper.html|
|9|DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on
  LLM-based Agent|Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, Sen Su|<li><span style="color:#FF5733;">2025-02-18</span></li>|arXiv|https://github.com/whfeLingYu/DemonAgent.|https://doi.org/10.48550/arXiv.2502.12575|
|10|BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion
  Model|Weilin Lin, Nanjun Zhou, Yanyun Wang, Jianze Li, Hui Xiong, Li Liu|<li><span style="color:#FF5733;">2025-02-17</span></li>|arXiv|https://github.com/linweiii/BackdoorDM.|https://doi.org/10.48550/arXiv.2502.11798|
|11|BoT: Breaking Long Thought Processes of o1-like Large Language Models
  through Backdoor Attack|Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, Baoyuan Wu|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/zihao-ai/BoT|https://doi.org/10.48550/arXiv.2502.12202|
|12|Revisiting the Auxiliary Data in Backdoor Purification|Shaokui Wei, Shanchao Yang, Jiayin Liu, Hongyuan Zha|<li><span style="color:#FF5733;">2025-02-10</span></li>|arXiv|https://github.com/shawkui/BackdoorBenchER.|https://doi.org/10.48550/arXiv.2502.07231|
|13|BadRefSR: Backdoor Attacks Against Reference-based Image Super
  Resolution|Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, Jiaming He|<li><span style="color:#FF5733;">2025-02-01</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/xuefusiji/BadRefSR.|https://doi.org/10.1109/icassp49660.2025.10889523|
|14|Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in
  Multilingual LLMs|Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh|<li><span style="color:#FF5733;">2025-02-01</span></li>|arXiv|https://github.com/himanshubeniwal/X-BAT.|https://doi.org/10.48550/arXiv.2502.16901|
|15|Detecting Backdoor Samples in Contrastive Language Image Pretraining|Hanxun Huang, Sarah Monazam Erfani, Yige Li, Xingjun Ma, James Bailey|<li><span style="color:#FF5733;">2025-02-01</span></li>|arXiv|https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples|https://openreview.net/forum?id=KmQEsIfhr9|
|16|Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on
  Diffusion Models|Yu Pan, Jiahao Chen, Bingrong Dai, Lin Wang, Yi Du, Jiao Liu|<li><span style="color:#FF5733;">2025-02-01</span></li>|arXiv|https://github.com/paoche11/Gungnir.|https://doi.org/10.48550/arXiv.2502.20650|
|17|UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep
  Reinforcement Learning|Oubo Ma, Linkang Du, Yang Dai, Chunyi Zhou, Qingming Li, Yuwen Pu, Shouling Ji|<li><span style="color:#FF5733;">2025-01-26</span></li>|arXiv|https://github.com/maoubo/UNIDOOR.|https://doi.org/10.48550/arXiv.2501.15529|
|18|Backdoor Token Unlearning: Exposing and Defending Backdoors in
  Pretrained Language Models|Peihai Jiang, Xixiang Lyu, Yige Li, Jing Ma|<li><span style="color:#FF5733;">2025-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/XDJPH/BTU.|https://doi.org/10.1609/aaai.v39i23.34605|
|19|Circumventing Backdoor Space via Weight Symmetry|Jie Peng, Hongwei Yang, Jing Zhao, Hengji Dong, Hui He, Weizhe Zhang, Haoyu He|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/JiePeng104/TSC.|https://doi.org/10.48550/arXiv.2506.07467|
|20|Claim-Guided Textual Backdoor Attack for Practical Applications|Minkyoo Song, Hanna Kim, Jaehan Kim, Youngjin Jin, Seungwon Shin|<li><span style="color:#FF5733;">2025-01-01</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/PaperCGBA/CGBA.|https://doi.org/10.18653/v1/2025.findings-naacl.64|
|21|Energy Backdoor Attack to Deep Neural Networks|Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier DÃ©forges, Kassem Kallas|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/hbrachemi/energy_backdoor.|https://doi.org/10.1109/icassp49660.2025.10888330|
|22|Exploring Backdoor Vulnerabilities of Chat Models|Wenkai Yang, Yunzhuo Hao, Yankai Lin|<li><span style="color:#FF5733;">2025-01-01</span></li>|COLING|https://github.com/hychaochao/Chat-Models-Backdoor-Attacking|https://aclanthology.org/2025.coling-main.62/|
|23|FLARE: Towards Universal Dataset Purification against Backdoor Attacks|Linshan Hou, Wei Luo, Zhongyun Hua, Songhua Chen, Leo Yu Zhang, Yiming Li|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/THUYimingLi/BackdoorBox|https://doi.org/10.1109/TIFS.2025.3581719|
|24|Invisible Backdoor Attack against Self-supervised Learning|Hanrong Zhang, Zhenting Wang, Boheng Li, Fulin Lin, Tingxu Han, Mingyu Jin, Chenlu Zhan, Mengnan Du, Hongwei Wang, Shiqi...|<li><span style="color:#FF5733;">2025-01-01</span></li>|CVPR|https://github.com/Zhang-Henry/INACTIVE.|https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Invisible_Backdoor_Attack_against_Self-supervised_Learning_CVPR_2025_paper.html|
|25|Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking|Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/aiiu-lab/BackdoorImageEditing|https://doi.org/10.48550/arXiv.2506.04879|
|26|REFINE: Inversion-Free Backdoor Defense via Model Reprogramming|Yukun Chen, Shuo Shao, Enhao Huang, Yiming Li, Pin-Yu Chen, Zhan Qin, Kui Ren|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/THUYimingLi/BackdoorBox|https://openreview.net/forum?id=4IYdCws9fc|
|27|The Ripple Effect: On Unforeseen Complications of Backdoor Attacks|Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, Yang Zhang|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/zhangrui4041/Backdoor_Complications.|https://doi.org/10.48550/arXiv.2505.11586|
|28|Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model|Jie Zhang, Zhongqi Wang, Shiguang Shan, Xilin Chen|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/Robin-WZQ/IBA.|https://doi.org/10.48550/arXiv.2503.17724|
|29|UFID: A Unified Framework for Black-box Input-level Backdoor Detection on Diffusion Models|Zihan Guan, Mengxuan Hu, Sheng Li, Anil Kumar S. Vullikanti|<li><span style="color:#FF5733;">2025-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/GuanZihan/official_UFID.|https://doi.org/10.1609/aaai.v39i26.34941|
|30|Vertical Federated Unlearning via Backdoor Certification|Mengde Han, Tianqing Zhu, Lefeng Zhang, Huan Huo, Wanlei Zhou|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Services Computing|https://github.com/mengde-han/VFL-unlearn.|https://doi.org/10.48550/arXiv.2412.11476|
|31|Cert-SSB: Toward Certified Sample-Specific Backdoor Defense|Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbing Li, Yiming Li|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/NcepuQiaoTing/Cert-SSB.|https://doi.org/10.48550/arXiv.2504.21730|
|32|Gracefully Filtering Backdoor Samples for Generative Large Language
  Models without Retraining|Zongru Wu, Pengzhou Cheng, Lingyong Fang, Zhuosheng Zhang, Gongshen Liu|<li><span style="color:#FF5733;">2024-12-03</span></li>|COLING|https://github.com/ZrW00/GraceFul.|https://aclanthology.org/2025.coling-main.220/|
|33|Fisher Information guided Purification against Backdoor Attacks|Nazmul Karim, Abdullah Al Arafat, Adnan Siraj Rakin, Zhishan Guo, Nazanin Rahnavard|<li><span style="color:#FF5733;">2024-12-02</span></li>|OpenAlex|https://github.com/nazmul-karim170/FIP-Fisher-Backdoor-Removal|https://doi.org/10.48550/arXiv.2409.00863|
|34|BadMerging: Backdoor Attacks Against Model Merging|Jinghuai Zhang, Jianfeng Chi, Zheng Li, Kunlin Cai, Yang Zhang, Yuan Tian|<li><span style="color:#FF5733;">2024-12-02</span></li>|OpenAlex|https://github.com/jzhang538/BadMerging.|https://doi.org/10.48550/arXiv.2408.07362|
|35|Backdoor Attacks against No-Reference Image Quality Assessment Models
  via a Scalable Trigger|Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, YapâPeng Tan, Alex C. Kot|<li><span style="color:#FF5733;">2024-12-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/yuyi-sd/BAIQA.|https://doi.org/10.48550/arXiv.2412.07277|
|36|Invisible Textual Backdoor Attacks based on Dual-Trigger|Yang Hou, Qiuling Yue, Lujia Chai, Guozhao Liao, Wenbao Han, Wei Ou|<li><span style="color:#FF5733;">2024-12-01</span></li>|arXiv|https://github.com/HoyaAm/Double-Landmines.|http://arxiv.org/abs/2412.17531v3|
|37|Perturb and Recover: Fine-tuning for Effective Backdoor Removal from
  CLIP|Naman Deep Singh, Francesco Croce, Matthias Hein|<li><span style="color:#FF5733;">2024-12-01</span></li>|arXiv|https://github.com/nmndeep/PerturbAndRecover.|https://doi.org/10.48550/arXiv.2412.00727|
|38|T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models|Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen|<li><span style="color:#FF5733;">2024-11-26</span></li>|Lecture notes in computer science|https://github.com/Robin-WZQ/T2IShield.|https://doi.org/10.1007/978-3-031-73013-9_7|
|39|BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for
  Backdoor Defense Evaluation|Haiyang Yu, Tian Xie, Jiaping Gui, Pengyang Wang, Pengzhou Cheng, Ping Yi, Yue Wu|<li><span style="color:#FF5733;">2024-11-17</span></li>|OpenAlex|https://github.com/SJTUHaiyangYu/BackdoorMBTI.|https://doi.org/10.48550/arXiv.2411.11006|
|40|Identify Backdoored Model in Federated Learning via Individual
  Unlearning|Jiahao Xu, Zikai Zhang, Rui Hu|<li><span style="color:#FF5733;">2024-11-01</span></li>|arXiv|https://github.com/JiiahaoXU/MASA|https://doi.org/10.1109/WACV61041.2025.00773|
|41|Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation
  Attack against EaaS Watermark|Zekun Fei, Biao Yi, Jianing Geng, Ruiqi He, Lihai Nie, Zheli Liu|<li><span style="color:#FF5733;">2024-11-01</span></li>|arXiv|https://github.com/Zk4-ps/EaaS-Embedding-Watermark.|http://arxiv.org/abs/2411.09359v2|
|42|UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening|Siyuan Cheng, Guangyu Shen, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Hanxi Guo, Shiqing Ma, Xiangyu Zhang|<li><span style="color:#FF5733;">2024-10-31</span></li>|Lecture notes in computer science|https://github.com/Megum1/UNIT.|https://doi.org/10.1007/978-3-031-73033-7_15|
|43|Mitigating the Backdoor Effect for Multi-Task Model Merging via
  Safety-Aware Subspace|Jinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, Fei Wu|<li><span style="color:#FF5733;">2024-10-16</span></li>|arXiv|https://github.com/Yangjinluan/DAM.|https://openreview.net/forum?id=dqMqAaw7Sq|
|44|Adversarially Guided Stateful Defense Against Backdoor Attacks in
  Federated Deep Learning|Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay K. Jha|<li><span style="color:#FF5733;">2024-10-01</span></li>|OpenAlex|https://github.com/hassanalikhatim/AGSD.|https://doi.org/10.1109/ACSAC63791.2024.00070|
|45|Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and
  Defenses in LLM-based Agents|Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang|<li><span style="color:#FF5733;">2024-10-01</span></li>|arXiv|https://github.com/agiresearch/ASB.|http://arxiv.org/abs/2410.02644v4|
|46|Expose Before You Defend: Unifying and Enhancing Backdoor Defenses via
  Exposed Models|Yige Li, Hanxun Huang, Jiaming Zhang, Xingjun Ma, Yu-Gang Jiang|<li><span style="color:#FF5733;">2024-10-01</span></li>|arXiv|https://github.com/bboylyg/Expose-Before-You-Defend.|https://doi.org/10.48550/arXiv.2410.19427|
|47|Event Trojan: Asynchronous Event-Based Backdoor Attacks|Ruofei Wang, Qing Guo, Haoliang Li, Renjie Wan|<li><span style="color:#FF5733;">2024-09-28</span></li>|Lecture notes in computer science|https://github.com/rfww/EventTrojan.|https://doi.org/10.1007/978-3-031-72667-5_18|
|48|Mask-Based Invisible Backdoor Attacks on Object Detection|Shin Jeong Jin|<li><span style="color:#FF5733;">2024-09-27</span></li>|2022 IEEE International Conference on Image Processing (ICIP)|https://github.com/jeongjin0/invisible-backdoor-object-detection|https://doi.org/10.36227/techrxiv.171440796.64142276/v1|
|49|Obliviate: Neutralizing Task-agnostic Backdoors within the
  Parameter-efficient Fine-tuning Paradigm|Jaehan Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin|<li><span style="color:#FF5733;">2024-09-21</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/obliviateARR/Obliviate.|https://doi.org/10.18653/v1/2025.findings-naacl.71|
|50|TERD: A Unified Framework for Safeguarding Diffusion Models Against
  Backdoors|Yichuan Mo, Hui Huang, Mingjie Li, Ang Li, Yisen Wang|<li><span style="color:#FF5733;">2024-09-08</span></li>|International Conference on Machine Learning 2024|https://github.com/PKU-ML/TERD.|https://openreview.net/forum?id=lpHjmPvxW1|
|51|Exploiting the Vulnerability of Large Language Models via Defense-Aware
  Architectural Backdoor|Abdullah Arafat Miah, Yu Bi|<li><span style="color:#FF5733;">2024-09-03</span></li>|arXiv|https://github.com/SiSL-URI/Arch_Backdoor_LLM.|https://doi.org/10.48550/arXiv.2409.01952|
|52|NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack
  Through White Gaussian Noise|Abdullah Arafat Miah, Kaan Icer, Resit Sendag, Yu Bi|<li><span style="color:#FF5733;">2024-09-03</span></li>|arXiv|https://github.com/SiSL-URI/NoiseAttack|https://doi.org/10.48550/arXiv.2409.02251|
|53|Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual
  Perturbations Against Backdoor Attacks|Oscar Chew, Po-Yi Lu, Jayden Lin, Hsuan-Tien Lin|<li><span style="color:#FF5733;">2024-08-28</span></li>|arXiv|https://github.com/oscarchew/t2i-backdoor-defense.|https://doi.org/10.48550/arXiv.2408.15721|
|54|VFLIP: A Backdoor Defense for Vertical Federated Learning via
  Identification and Purification|Yungi Cho, Woorim Han, Miseon Yu, Younghan Lee, Ho Bae, Yunheung Paek|<li><span style="color:#FF5733;">2024-08-28</span></li>|Lecture notes in computer science|https://github.com/blingcho/VFLIP-esorics24|https://doi.org/10.1007/978-3-031-70903-6_15|
|55|MakeupAttack: Feature Space Black-box Backdoor Attack on Face
  Recognition via Makeup Transfer|Ming Sun, Lihua Jing, Zixuan Zhu, Rui Wang|<li><span style="color:#FF5733;">2024-08-22</span></li>|Frontiers in artificial intelligence and applications|https://github.com/AaronSun2000/MakeupAttack.|https://doi.org/10.48550/arXiv.2408.12312|
|56|BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses
  on Large Language Models|Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun|<li><span style="color:#FF5733;">2024-08-01</span></li>|arXiv|https://github.com/bboylyg/BackdoorLLM.|https://doi.org/10.48550/arXiv.2408.12798|
|57|Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion
  Models|Hao Jiang, Jin Xiao, Xiaoguang Hu, Chen Tianyou, Zhao Jiajia|<li><span style="color:#FF5733;">2024-07-30</span></li>|arXiv|https://github.com/shymuel/diff-cleanse.|https://doi.org/10.48550/arXiv.2407.21316|
|58|BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor
  Learning|Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen|<li><span style="color:#FF5733;">2024-07-29</span></li>|International Journal of Computer Vision|[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/backdoorbench-a-comprehensive-benchmark-of/code)|http://papers.nips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html|
|59|Flatness-aware Sequential Learning Generates Resilient Backdoors|Hoang N. Pham, The-Anh Ta, Anh Tran, Khoa D. Doan|<li><span style="color:#FF5733;">2024-07-19</span></li>|Lecture notes in computer science|https://github.com/mail-research/SBL-resilient-backdoors|https://doi.org/10.1007/978-3-031-73021-4_6|
|60|Provable Robustness of (Graph) Neural Networks Against Data Poisoning
  and Backdoor Attacks|Lukas Gosch, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Stephan GÃ¼nnemann|<li><span style="color:#FF5733;">2024-07-15</span></li>|Trans. Mach. Learn. Res.|https://github.com/saper0/qpcert|https://openreview.net/forum?id=jIAPLDdGVx|
|61|Distributed Backdoor Attacks on Federated Graph Learning and Certified
  Defenses|Yuxin Yang, Qiang Li, Jinyuan Jia, Yuan Hong, Binghui Wang|<li><span style="color:#FF5733;">2024-07-11</span></li>|OpenAlex|https://github.com/Yuxin104/Opt-GDBA.|https://doi.org/10.48550/arXiv.2407.08935|
|62|Future Events as Backdoor Triggers: Investigating Temporal
  Vulnerabilities in LLMs|Sara Price, Arjun Panickssery, Samuel R. Bowman, Asa Cooper Stickland|<li><span style="color:#FF5733;">2024-07-04</span></li>|arXiv|https://github.com/sbp354/Future_triggered_backdoors|https://doi.org/10.48550/arXiv.2407.04108|
|63|ShadowCode: Towards (Automatic) External Prompt Injection Attack against
  Code LLMs|Yuchen Yang, Yiming Li, Hongwei Yao, Bingrun Yang, Yiling He, Tianwei Zhang, Dacheng Tao, Zhan Qin|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/LianPing-cyber/ShadowCodeEPI.|http://arxiv.org/abs/2407.09164v6|
|64|Venomancer: Towards Imperceptible and Target-on-Demand Backdoor Attacks
  in Federated Learning|Son Nguyen, Thinh Viet Nguyen, Khoa D. Doan, KokâSeng Wong|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/nguyenhongson1902/Venomancer.|https://doi.org/10.48550/arXiv.2407.03144|
|65|Towards Clean-Label Backdoor Attacks in the Physical World|Thinh Dao, Cuong Phan Minh Le, Khoa D. Doan, KokâSeng Wong|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/21thinh/Clean-Label-Physical-Backdoor-Attacks.|https://doi.org/10.48550/arXiv.2407.19203|
|66|IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields|Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/jiang-wenxiang/IPA-NeRF.|http://arxiv.org/abs/2407.11921v2|
|67|Backdoor Graph Condensation|Jiahao Wu, Ning Lu, Zeiyu Dai, Kun Wang, Wenqi Fan, Shengcai Liu, Qing Li, Ke Tang|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/JiahaoWuGit/BGC.|https://doi.org/10.48550/arXiv.2407.11025|
|68|Defending Against Repetitive-based Backdoor Attacks on Semi-supervised
  Learning through Lens of Rate-Distortion-Perception Trade-off|Cheng-Yi Lee, Ching-Chia Kao, Cheng-Han Yeh, Chun-Shien Lu, Chia-Mu Yu, Chu-Song Chen|<li><span style="color:#FF5733;">2024-07-01</span></li>|2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/chengyi-chris/UPure|https://doi.org/10.1109/WACV61041.2025.00630|
|69|A Whole-Process Certifiably Robust Aggregation Method Against Backdoor
  Attacks in Federated Learning|Anqi Zhou, Yezheng Liu, Yidong Chai, Hongyi Zhu, Xinyue Ge, Yuanchun Jiang, Meng Wang|<li><span style="color:#FF5733;">2024-06-30</span></li>|arXiv|https://github.com/brick-brick/WPCRAM.|https://doi.org/10.48550/arXiv.2407.00719|
|70|Lotus: Evasive and Resilient Backdoor Attacks through Sub-Partitioning|Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, ...|<li><span style="color:#FF5733;">2024-06-16</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/Megum1/LOTUS.|https://doi.org/10.1109/cvpr52733.2024.02342|
|71|BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents|Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian|<li><span style="color:#FF5733;">2024-06-05</span></li>|OpenAlex|https://github.com/DPamK/BadAgent|https://doi.org/10.18653/v1/2024.acl-long.530|
|72|Invisible Backdoor Attacks on Diffusion Models|Sen Li, Junchi Ma, Minhao Cheng|<li><span style="color:#FF5733;">2024-06-02</span></li>|arXiv|https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion.|https://doi.org/10.48550/arXiv.2406.00816|
|73|Let the Noise Speak: Harnessing Noise for a Unified Defense Against
  Adversarial and Backdoor Attacks|Md Hasan Shahriar, Ning Wang, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou|<li><span style="color:#FF5733;">2024-06-01</span></li>|arXiv|https://github.com/shahriar0651/NoiSec.|http://arxiv.org/abs/2406.13073v2|
|74|BAN: Detecting Backdoors Activated by Adversarial Neuron Noise|Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Shujian Yu, Stjepan Picek|<li><span style="color:#FF5733;">2024-05-30</span></li>|NeurIPS|https://github.com/xiaoyunxxy/ban|http://papers.nips.cc/paper_files/paper/2024/hash/cfaccbd9b5e62562779351ebcb140c94-Abstract-Conference.html|
|75|Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor|Shaokui Wei, Hongyuan Zha, Baoyuan Wu|<li><span style="color:#FF5733;">2024-05-25</span></li>|NeurIPS|https://github.com/shawkui/Proactive_Defensive_Backdoor.|http://papers.nips.cc/paper_files/paper/2024/hash/9374af323abb65ce551168d44b09ad5f-Abstract-Conference.html|
|76|Towards Imperceptible Backdoor Attack in Self-supervised Learning|Hanrong Zhang, Zhenting Wang, Tingxu Han, Mingyu Jin, Chenlu Zhan, Mengnan Du, Hongwei Wang, Shiqing Ma|<li><span style="color:#FF5733;">2024-05-23</span></li>|arXiv|https://github.com/Zhang-Henry/IMPERATIVE.|https://doi.org/10.48550/arXiv.2405.14672|
|77|IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling
  Consistency|Linshan Hou, Ruili Feng, Zhongyun Hua, Wei Luo, Leo Yu Zhang, Yiming Li|<li><span style="color:#FF5733;">2024-05-15</span></li>|ICML|https://github.com/THUYimingLi/BackdoorBox|https://openreview.net/forum?id=YCzbfs2few|
|78|EmInspector: Combating Backdoor Attacks in Federated Self-Supervised
  Learning Through Embedding Inspection|Yuwen Qian, Shuchi Wu, Kang Wei, Ming Ding, Di Xiao, Tao Xiang, Chuan Ma, Song Guo|<li><span style="color:#FF5733;">2024-05-01</span></li>|arXiv|https://github.com/ShuchiWu/EmInspector.|https://doi.org/10.48550/arXiv.2405.13080|
|79|Nearest is Not Dearest: Towards Practical Defense against
  Quantization-conditioned Backdoor Attacks|Boheng Li, Yishuo Cai, Haowei Li, Feng Xue, Zhifeng Li, Yiming Li|<li><span style="color:#FF5733;">2024-05-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/AntigoneRandy/QuantBackdoor_EFRAP.|https://doi.org/10.1109/CVPR52733.2024.02315|
|80|Not All Prompts Are Secure: A Switchable Backdoor Attack Against
  Pre-trained Vision Transformers|Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, Shu-tao Xia|<li><span style="color:#FF5733;">2024-05-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/20000yshust/SWARM.|https://doi.org/10.1109/CVPR52733.2024.02306|
|81|Beyond Traditional Threats: A Persistent Backdoor Attack on Federated
  Learning|Tao Liu, Yuhang Zhang, Feng Zhu, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang|<li><span style="color:#FF5733;">2024-04-26</span></li>||https://github.com/PhD-TaoLiu/FCBA.|https://doi.org/10.1609/aaai.v38i19.30131|
|82|Privacy Backdoors: Stealing Data with Corrupted Pretrained Models|Shanglun Feng, Florian TramÃ¨r|<li><span style="color:#FF5733;">2024-03-30</span></li>|ICML|https://github.com/ShanglunFengatETHZ/PrivacyBackdoor|https://openreview.net/forum?id=7yixJXmzb8|
|83|BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning|Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang|<li><span style="color:#FF5733;">2024-03-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/7777777cc/code.|https://doi.org/10.1609/aaai.v38i10.29052|
|84|COMBAT: Alternated Training for Near-Perfect Clean-Label Backdoor Attacks|Tran Ngoc Huynh, Dang Minh Nguyen, Tung Pham, Anh Tuan Tran|<li><span style="color:#FF5733;">2024-03-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/VinAIResearch/COMBAT.|https://openreview.net/pdf/c182fdd518fe8ec0aeafeb8d1b2b55bb8e46a463.pdf|
|85|Generating Potent Poisons and Backdoors from Scratch with Guided
  Diffusion|Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellapp...|<li><span style="color:#FF5733;">2024-03-24</span></li>|arXiv|https://github.com/hsouri/GDP|https://doi.org/10.48550/arXiv.2403.16365|
|86|Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain|Linkun Fan, Fazhi He, Tongzhen Si, Wei Tang, Bing Li|<li><span style="color:#FF5733;">2024-03-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/f-lk/IBAPC.|https://doi.org/10.1609/aaai.v38i19.30099|
|87|Progressive Poisoned Data Isolation for Training-Time Backdoor Defense|Yiming Chen, Haiwei Wu, Jiantao Zhou|<li><span style="color:#FF5733;">2024-03-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/RorschachChen/PIPD.git.|https://doi.org/10.1609/aaai.v38i10.29023|
|88|An Embarrassingly Simple Defense Against Backdoor Attacks On SSL|Aryan Satpathy, Nilaksh Nilaksh, Dhruva Rajwade|<li><span style="color:#FF5733;">2024-03-23</span></li>|arXiv|https://github.com/Aryan-Satpathy/Backdoor.|https://doi.org/10.48550/arXiv.2403.15918|
|89|PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models|Hongwei Yao, Jian Lou, Zhan Qin|<li><span style="color:#FF5733;">2024-03-18</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/grasses/PoisonPrompt|https://doi.org/10.1109/icassp48485.2024.10446267|
|90|Invisible Black-Box Backdoor Attack against Deep Cross-Modal Hashing Retrieval|Tianshi Wang, Fengling Li, Lei Zhu, Jingjing Li, Zheng Zhang, Heng Tao Shen|<li><span style="color:#FF5733;">2024-03-02</span></li>|ACM transactions on office information systems|https://github.com/tswang0116/IB3A.|https://doi.org/10.1145/3650205|
|91|Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized
  Scaled Prediction Consistency|Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu|<li><span style="color:#FF5733;">2024-03-01</span></li>|ICLR|https://github.com/OPTML-Group/BackdoorMSPC.|https://openreview.net/forum?id=1OfAO2mes1|
|92|BapFL: You can Backdoor Personalized Federated Learning|Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao|<li><span style="color:#FF5733;">2024-02-23</span></li>|ACM Transactions on Knowledge Discovery from Data|https://github.com/BapFL/code|https://doi.org/10.1145/3649316|
|93|Acquiring Clean Language Models from Backdoor Poisoned Datasets by
  Downscaling Frequency Space|Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu|<li><span style="color:#FF5733;">2024-02-19</span></li>|OpenReview|https://github.com/ZrW00/MuScleLoRA.|https://openreview.net/pdf/9eb71f0c75e3630c53671cf8b0175e95463726f1.pdf|
|94|Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery
  Detection|Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao|<li><span style="color:#FF5733;">2024-02-18</span></li>|ICLR|https://github.com/JWLiang007/PFF|https://openreview.net/forum?id=8iTpB4RNvP|
|95|Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
  Agents|Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun|<li><span style="color:#FF5733;">2024-02-17</span></li>|NeurIPS|https://github.com/lancopku/agent-backdoor-attacks|http://papers.nips.cc/paper_files/paper/2024/hash/b6e9d6f4f3428cd5f3f9e9bbae2cab10-Abstract-Conference.html|
|96|OrderBkd: Textual backdoor attack through repositioning|Irina Alekseevskaia, Konstantin Arkhipenko|<li><span style="color:#FF5733;">2024-02-12</span></li>|OpenAlex|https://github.com/alekseevskaia/OrderBkd.|https://doi.org/10.1109/ispras60948.2023.10508175|
|97|SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via
  Syntactic Transfer|Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Zhuosheng Zhang, Gongshen Liu|<li><span style="color:#FF5733;">2024-02-01</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/Zhou-CyberSecurity-AI/SynGhost.|https://doi.org/10.18653/v1/2025.findings-naacl.196|
|98|TransTroj: Transferable Backdoor Attacks to Pre-trained Models via
  Embedding Indistinguishability|Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang|<li><span style="color:#FF5733;">2024-01-28</span></li>|arXiv|https://github.com/haowang-cqu/TransTroj|https://doi.org/10.48550/arXiv.2401.15883|
|99|A Closer Look at Robustness of Vision Transformers to Backdoor Attacks|Akshayvarun Subramanya, Soroush Abbasi Koohpayegani, Aniruddha Saha, Ajinkya Tejankar, Hamed Pirsiavash|<li><span style="color:#FF5733;">2024-01-03</span></li>|2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/UCDvision/backdoor_transformer.git|https://doi.org/10.1109/wacv57701.2024.00383|
|100|BackTime: Backdoor Attacks on Multivariate Time Series Forecasting|Xiaola Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, Hanghang Tong|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/xiaolin-cs/BackTime|http://papers.nips.cc/paper_files/paper/2024/hash/ed3cd2520148b577039adfade82a5566-Abstract-Conference.html|
|101|Backdoor Contrastive Learning via Bi-level Trigger Optimization|Weiyu Sun, Xinyu Zhang, Hao Lu, Ying-Cong Chen, Ting Wang, Jinghui Chen, Lu Lin|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/SWY666/SSL-backdoor-BLTO.|https://openreview.net/forum?id=oxjeePpgSP|
|102|BadCM: Invisible Backdoor Attack Against Cross-Modal Learning|Zheng Zhang, Xu Yuan, Lei Zhu, Jingkuan Song, Liqiang Nie|<li><span style="color:#FF5733;">2024-01-01</span></li>||https://github.com/xandery-geek/BadCM.|https://doi.org/10.48550/arXiv.2410.02182|
|103|Defending Against Backdoor Attacks by Quarantine Training|Chengxu Yu, Yulai Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Access|https://github.com/Chengx-Yu/Quarantine-Training.|https://doi.org/10.1109/access.2024.3354385|
|104|How to Backdoor Consistency Models?|Chengen Wang, Murat Kantarcioglu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/chengenw/backdoorCM|https://doi.org/10.1007/978-981-96-8295-9_23|
|105|How to Craft Backdoors with Unlabeled Data Alone?|Yifei Wang, Wenhan Ma, Stefanie Jegelka, Yisen Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/PKU-ML/nlb.|https://doi.org/10.48550/arXiv.2404.06694|
|106|Model Supply Chain Poisoning: Backdooring Pre-trained Models via
  Embedding Indistinguishability|Hao Wang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang, Tao Xiang|<li><span style="color:#FF5733;">2024-01-01</span></li>|WWW|https://github.com/haowang-cqu/TransTroj|https://doi.org/10.1145/3696410.3714624|
|107|PBP: Post-training Backdoor Purification for Malware Classifiers|Dung Thuy Nguyen, Ngoc N. Tran, Taylor T. Johnson, Kevin Leach|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/judydnguyen/pbp-backdoor-purification-official|https://www.ndss-symposium.org/ndss-paper/pbp-post-training-backdoor-purification-for-malware-classifiers/|
|108|PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection|Wei Li, Pin-Yu Chen, Sijia Liu, Ren Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/WL-619/PSBD.|https://openaccess.thecvf.com/content/CVPR2025/html/Li_PSBD_Prediction_Shift_Uncertainty_Unlocks_Backdoor_Detection_CVPR_2025_paper.html|
|109|Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor
  Attacks|Yige Li, Jiabo He, Hanxun Huang, Jun Sun, Xingjun Ma, Yu-Gang Jiang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/bboylyg/Multi-Trigger-Backdoor-Attacks|http://arxiv.org/abs/2401.15295v3|
|110|Toward Stealthy Backdoor Attacks Against Speech Recognition via Elements of Sound|Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/HanboCai/BadSpeech_SoE.|https://doi.org/10.1109/tifs.2024.3404885|
|111|Adversarial Feature Map Pruning for Backdoor|Dong Huang, Qingwen Bu|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/retsuh-bqw/FMP.|https://openreview.net/forum?id=IOEEDkla96|
|112|OCGEC: One-class Graph Embedding Classification for DNN Backdoor
  Detection|Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi|<li><span style="color:#FF5733;">2023-12-01</span></li>|2022 International Joint Conference on Neural Networks (IJCNN)|https://github.com/jhy549/OCGEC.|https://doi.org/10.1109/ijcnn60899.2024.10650468|
|113|UltraClean: A Simple Framework to Train Robust Neural Networks against
  Backdoor Attacks|Bingyin Zhao, Yingjie Lao|<li><span style="color:#FF5733;">2023-12-01</span></li>|arXiv|https://github.com/bxz9200/UltraClean.|https://doi.org/10.48550/arXiv.2312.10657|
|114|Activation Gradient based Poisoned Sample Detection Against Backdoor
  Attacks|Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu|<li><span style="color:#FF5733;">2023-12-01</span></li>|ICLR|https://github.com/SCLBD/bdzoo2|https://openreview.net/forum?id=VNMJfBBUd5|
|115|Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking|Shengsheng Qian, Dizhan Xue, Yifei Wang, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu|<li><span style="color:#FF5733;">2023-12-01</span></li>|arXiv|https://github.com/LivXue/PoisonCAM.|https://doi.org/10.48550/arXiv.2312.07955|
|116|A Practical Clean-Label Backdoor Attack with Limited Information in Vertical Federated Learning|Peng Chen, Jirui Yang, Junxiong Lin, Zhihui Lu, Qiang Duan, Hongfeng Chai|<li><span style="color:#FF5733;">2023-12-01</span></li>|2021 IEEE International Conference on Data Mining (ICDM)|https://github.com/13thDayOLunarMay/TECB-attack|https://doi.org/10.1109/icdm58522.2023.00013|
|117|TextGuard: Provable Defense against Backdoor Attacks on Text
  Classification|Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song|<li><span style="color:#FF5733;">2023-11-01</span></li>|OpenAlex|https://github.com/AI-secure/TextGuard.|https://www.ndss-symposium.org/ndss-paper/textguard-provable-defense-against-backdoor-attacks-on-text-classification/|
|118|ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing|Yulin Jin, Xiaoyu Zhang, Jian Lou, Xiaofeng Chen|<li><span style="color:#FF5733;">2023-10-26</span></li>|ACM Multimedia|https://github.com/Backdoor-defense/ACQ|https://doi.org/10.1145/3581783.3612410|
|119|Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks|Zihan Guan, Lichao Sun, Mengnan Du, Ninghao Liu|<li><span style="color:#FF5733;">2023-10-21</span></li>|OpenAlex|https://github.com/GuanZihan/Deep-Backdoor-Attack.|https://doi.org/10.1145/3583780.3614784|
|120|An Embarrassingly Simple Backdoor Attack on Self-supervised Learning|Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji, Yuan Yao, Ting Wang|<li><span style="color:#FF5733;">2023-10-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/meet-cjli/CTRL|https://doi.org/10.1109/iccv51070.2023.00403|
|121|Computation and Data Efficient Backdoor Attacks|Yutong Wu, Xingshuo Han, Han Qiu, Tianwei Zhang|<li><span style="color:#FF5733;">2023-10-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/WU-YU-TONG/computational_efficient_backdoor|https://doi.org/10.1109/iccv51070.2023.00443|
|122|FLTracer: Accurate Poisoning Attack Provenance in Federated Learning|Xinyu Zhang, Qingyu Liu, Zhongjie Ba, Yuan Hong, Tianhang Zheng, Feng Lin, Li Lu, Kui Ren|<li><span style="color:#FF5733;">2023-10-01</span></li>|arXiv|https://github.com/Eyr3/FLTracer|http://arxiv.org/abs/2310.13424v1|
|123|XGBD: Explanation-Guided Graph Backdoor Detection|Zihan Guan, Mengnan Du, Ninghao Liu|<li><span style="color:#FF5733;">2023-09-28</span></li>|Frontiers in artificial intelligence and applications|https://github.com/GuanZihan/GNN_backdoor_detection.|https://doi.org/10.48550/arXiv.2308.04406|
|124|Resisting Backdoor Attacks in Federated Learning via Bidirectional
  Elections and Individual Perspective|Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng|<li><span style="color:#FF5733;">2023-09-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zhenqincn/Snowball|https://doi.org/10.48550/arXiv.2309.16456|
|125|TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal
  Backdoored Models|Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha|<li><span style="color:#FF5733;">2023-08-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/SRI-CSL/TIJO.|https://doi.org/10.1109/iccv51070.2023.00022|
|126|Towards Stealthy Backdoor Attacks against Speech Recognition via
  Elements of Sound|Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li|<li><span style="color:#FF5733;">2023-07-01</span></li>|arXiv|https://github.com/HanboCai/BadSpeech_SoE|https://doi.org/10.48550/arXiv.2307.08208|
|127|Detecting Backdoors in Pre-trained Encoders|Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang|<li><span style="color:#FF5733;">2023-06-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/GiantSeaweed/DECREE.|https://doi.org/10.1109/cvpr52729.2023.01569|
|128|VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion
  Models|Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho|<li><span style="color:#FF5733;">2023-06-01</span></li>|NeurIPS|https://github.com/IBM/villandiffusion|http://papers.nips.cc/paper_files/paper/2023/hash/6b055b95d689b1f704d8f92191cdb788-Abstract-Conference.html|
|129|Efficient Backdoor Attacks for Deep Neural Networks in Real-world
  Scenarios|Ziqiang Li, Hong Sun, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li|<li><span style="color:#FF5733;">2023-06-01</span></li>|arXiv|https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios|https://openreview.net/forum?id=vRyp2dhEQp|
|130|Single Image Backdoor Inversion via Robust Smoothed Classifiers|Mingjie Sun, J. Zico Kolter|<li><span style="color:#FF5733;">2023-06-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/locuslab/smoothinv.|https://doi.org/10.1109/cvpr52729.2023.00784|
|131|DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via
  Restricted Adversarial Distillation|Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao|<li><span style="color:#FF5733;">2023-06-01</span></li>|OpenAlex|https://github.com/yanzhicong/DHBE|https://doi.org/10.48550/arXiv.2306.08009|
|132|Backdoor Cleansing with Unlabeled Data|Lu Pang, Tong Sun, Haibin Ling, Chao Chen|<li><span style="color:#FF5733;">2023-06-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/luluppang/BCU.|https://openreview.nethttps://arxiv.org/pdf/2211.12044|
|133|Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated
  Graph Neural Network|Fan Liu, Siqi Lai, Yansong Ning, Hao Liu|<li><span style="color:#FF5733;">2023-06-01</span></li>|arXiv|https://github.com/usail-hkust/BkdFedGCN.|https://doi.org/10.48550/arXiv.2306.10351|
|134|Backdoor Defense via Deconfounded Representation Learning|Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu|<li><span style="color:#FF5733;">2023-06-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/zaixizhang/CBD.|https://doi.org/10.1109/cvpr52729.2023.01177|
|135|An Empirical Study of Backdoor Attacks on Masked Auto Encoders|Shuli Zhuang, Pengfei Xia, Bin Li|<li><span style="color:#FF5733;">2023-05-05</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/zhuangshuli/MAE-Backdoor.|https://doi.org/10.1109/icassp49357.2023.10095201|
|136|Going in Style: Audio Backdoors Through Stylistic Transformations|Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti|<li><span style="color:#FF5733;">2023-05-05</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/skoffas/going-in-style.|https://doi.org/10.1109/icassp49357.2023.10096332|
|137|Text-to-Image Diffusion Models can be Easily Backdoored through
  Multimodal Data Poisoning|Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su|<li><span style="color:#FF5733;">2023-05-01</span></li>|ACM Multimedia|https://github.com/sf-zhai/BadT2I.|https://doi.org/10.48550/arXiv.2305.04175|
|138|Training-free Lexical Backdoor Attacks on Language Models|Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen|<li><span style="color:#FF5733;">2023-04-26</span></li>|Proceedings of the ACM Web Conference 2022|https://github.com/Jinxhy/TFLexAttack.|https://doi.org/10.48550/arXiv.2302.04116|
|139|Defending Against Patch-based Backdoor Attacks on Self-Supervised
  Learning|Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan|<li><span style="color:#FF5733;">2023-04-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/UCDvision/PatchSearch|https://doi.org/10.1109/cvpr52729.2023.01178|
|140|Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware
  Minimization|Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu|<li><span style="color:#FF5733;">2023-04-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/SCLBD/BackdoorBench.|https://doi.org/10.1109/iccv51070.2023.00412|
|141|Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection|Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, Jinyin Chen|<li><span style="color:#FF5733;">2023-03-30</span></li>|IEEE Transactions on Computational Social Systems|https://github.com/Seaocn/Link-Backdoor.|https://doi.org/10.1109/tcss.2023.3260833|
|142|AdaptGuard: Defending Against Universal Attacks for Model Adaptation|Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan|<li><span style="color:#FF5733;">2023-03-01</span></li>|arXiv|https://github.com/TomSheng21/AdaptGuard.|http://arxiv.org/abs/2303.10594v2|
|143|Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based
  Artificial Bias|Shangxi Wu, Qiuyang He, Fangzhao Wu, Jitao Sang, Yaowei Wang, Changsheng Xu|<li><span style="color:#FF5733;">2023-03-01</span></li>|IEEE Transactions on Circuits and Systems for Video Technology|https://github.com/KirinNg/DBA.|https://doi.org/10.1109/tcsvt.2025.3548657|
|144|CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive
  Learning|Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, Kai-Wei Chang|<li><span style="color:#FF5733;">2023-03-01</span></li>|RTML Workshop 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/cleanclip-mitigating-data-poisoning-attacks/code)|https://openreview.net/pdf/6a86afb6f0e0ce8a38d619097336004f6f0b6a73.pdf|
|145|Detecting Backdoors During the Inference Stage Based on Corruption
  Robustness Consistency|Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao|<li><span style="color:#FF5733;">2023-03-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/CGCL-codes/TeCo|https://doi.org/10.1109/cvpr52729.2023.01570|
|146|Mask and Restore: Blind Backdoor Defense at Test Time with Masked
  Autoencoder|Tong Sun, Lu Pang, Chao Chen, Haibin Ling|<li><span style="color:#FF5733;">2023-03-01</span></li>|arXiv|https://github.com/tsun/BDMAE.|https://doi.org/10.48550/arXiv.2303.15564|
|147|TrojText: Test-time Invisible Textual Trojan Insertion|Qian Lou, Yepeng Liu, Bo Feng|<li><span style="color:#FF5733;">2023-02-02</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/trojtext-test-time-invisible-textual-trojan/code)|https://openreview.net/pdf/090c1fa0cc728fa6eb032fe3c74b8b5125be7e94.pdf|
|148|Towards Robust Model Watermark via Reducing Parametric Vulnerability|Guanhao Gan, Yiming Li, Dongxian Wu, Shu-Tao Xia|<li><span style="color:#FF5733;">2023-02-02</span></li>|Submitted to ICLR 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/towards-robust-model-watermark-via-reducing/code)|https://openreview.net/pdf/c3ea0d03202ba1e2fbf2a003a936364bb447ce98.pdf|
|149|ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep
  Learning Paradigms|Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia|<li><span style="color:#FF5733;">2023-02-01</span></li>|USENIX Security Symposium|https://github.com/ruoxi-jia-group/ASSET.|https://www.usenix.org/conference/usenixsecurity23/presentation/pan|
|150|Backdoor Attacks on Time Series: A Generative Approach|Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey|<li><span style="color:#FF5733;">2023-02-01</span></li>|SaTML 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/backdoor-attacks-on-time-series-a-generative/code)|https://openreview.net/pdf/b15b1e53dab0744f34198d60d727ddab895c8074.pdf|
|151|Backdoor Learning for NLP: Recent Advances, Challenges, and Future
  Research Directions|Marwan Omar|<li><span style="color:#FF5733;">2023-02-01</span></li>|arXiv|https://github.com/marwanomar1/Backdoor-Learning-for-NLP.|https://doi.org/10.48550/arXiv.2302.06801|
|152|Revisiting Personalized Federated Learning: Robustness Against Backdoor
  Attacks|Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng|<li><span style="color:#FF5733;">2023-02-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/alibaba/FederatedScope|https://openreview.nethttps://arxiv.org/pdf/2302.01677.pdf|
|153|SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via
  Analyzing Scaled Prediction Consistency|Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu|<li><span style="color:#FF5733;">2023-02-01</span></li>|ICLR 2023 poster|https://github.com/JunfengGo/SCALE-UP.|https://openreview.net/pdf/341ae2d07a7459242b24bb6e6ff7e2aec7a756e1.pdf|
|154|SafeNet: The Unreasonable Effectiveness of Ensembles in Private Collaborative Learning|Harsh Chaudhari, Matthew Jagielski, Alina Oprea|<li><span style="color:#FF5733;">2023-01-01</span></li>|SaTML 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2205.09986/code)|https://openreview.net/pdf/512a5c7a02310e8ac2b28531e9e0c6518ad1c4e6.pdf|
|155|NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models|Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/RU-System-Software-and-Security/Notable.|https://doi.org/10.18653/v1/2023.acl-long.867|
|156|Reconstructive Neuron Pruning for Backdoor Defense|Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|ICML|https://github.com/bboylyg/RNP|https://proceedings.mlr.press/v202/li23v.html|
|157|Removing Backdoors in Pre-trained Models by Regularized Continual Pre-training|Biru Zhu, Ganqu Cui, Yangyi Chen, Yujia Qin, Lifan Yuan, Chong Fu, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu|<li><span style="color:#FF5733;">2023-01-01</span></li>|Transactions of the Association for Computational Linguistics|https://github.com/thunlp/RECIPE.|https://openreview.net/pdf/b90599e4935794e4f111f07737fb0e5a485048f3.pdf|
|158|RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks|Marwan Omar|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/marwanomar1/Backdoor-Learning-for-NLP|https://doi.org/10.48550/arXiv.2302.09420|
|159|Learning to Backdoor Federated Learning|Henger Li, Wu Chen, Senchun Zhu, Zizhan Zheng|<li><span style="color:#FF5733;">2023-01-01</span></li>|ICLR 2023 BANDS Spotlight|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/learning-to-backdoor-federated-learning/code)|https://openreview.net/pdf/b3222725885adf97fe0f200feabe9bbd3df94344.pdf|
|160|The &quot;Beatrix&quot; Resurrections: Robust Backdoor Detection via Gram Matrices|Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/wanlunsec/Beatrix|https://www.ndss-symposium.org/ndss-paper/the-beatrix-resurrections-robust-backdoor-detection-via-gram-matrices/|
|161|Towards Stable Backdoor Purification through Feature Shift Tuning|Rui Min, Zeyu Qin, Li Shen, Minhao Cheng|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/AISafety-HKUST/stable_backdoor_purification.|http://papers.nips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html|
|162|Universal Backdoor Attacks|Benjamin Schneider, Nils Lukas, Florian Kerschbaum|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.|https://openreview.net/forum?id=3QkzYBSWqL|
|163|You Can Backdoor Personalized Federated Learning|Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao|<li><span style="color:#FF5733;">2023-01-01</span></li>|ACM Trans. Knowl. Discov. Data 2024|https://github.com/BapFL/code.|https://doi.org/10.48550/arXiv.2307.15971|
|164|UNICORN: A Unified Backdoor Trigger Inversion Framework|Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma|<li><span style="color:#FF5733;">2023-01-01</span></li>|ICLR 2023 notable top 25%|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/unicorn-a-unified-backdoor-trigger-inversion/code)|https://openreview.net/pdf/edd35173abda536a0bd486d49c34c8ce04e56652.pdf|
|165|From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models|Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/OPTML-Group/BiBadDiff.|https://doi.org/10.48550/arXiv.2311.02373|
|166|FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks|Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/retsuh-bqw/FMP.|https://doi.org/10.48550/arXiv.2307.11565|
|167|Distilling Cognitive Backdoor Patterns within an Image|Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey|<li><span style="color:#FF5733;">2023-01-01</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/distilling-cognitive-backdoor-patterns-within/code)|https://openreview.net/pdf/9582391717db932771feaf1c877a1ff5a58478f5.pdf|
|168|Black-box Backdoor Defense via Zero-shot Image Purification|Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, Ninghao Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/sycny/ZIP.|http://papers.nips.cc/paper_files/paper/2023/hash/b36554b97da741b1c48c9de05c73993e-Abstract-Conference.html|
|169|Beating Backdoor Attack at Its Own Game|Min Liu, Alberto L. Sangiovanni-Vincentelli, Xiangyu Yue|<li><span style="color:#FF5733;">2023-01-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/damianliumin/non-adversarial_backdoor.|https://doi.org/10.1109/ICCV51070.2023.00426|
|170|BackdoorBox: A Python Toolbox for Backdoor Learning|Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia|<li><span style="color:#FF5733;">2023-01-01</span></li>|ICLR 2023 BANDS Spotlight|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/backdoorbox-a-python-toolbox-for-backdoor/code)|https://openreview.net/pdf/81b89920b0128744bafa5c1943ac1ed8b0a871c7.pdf|
|171|Backdoor Defense via Adaptively Splitting Poisoned Dataset|Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia|<li><span style="color:#FF5733;">2023-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/KuofengGao/ASD.|https://doi.org/10.1109/CVPR52729.2023.00390|
|172|Backdoor Attacks for Remote Sensing Data With Wavelet Transform|Nikolaus DrÃ¤ger, Yonghao Xu, Pedram Ghamisi|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Trans. Geos. Remote Sens., vol. 61, pp. 1-15, 2023|https://github.com/ndraeger/waba.|https://doi.org/10.1109/tgrs.2023.3289307|
|173|Backdoor Attack with Sparse and Invisible Trigger|Yinghua Gao, Yiming Li, Xueluan Gong, Zhifeng Li, Shu-Tao Xia, Qian Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/YinghuaGao/SIBA|https://doi.org/10.48550/arXiv.2306.06209|
|174|Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment|Haoran Wang, Kai Shu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/wang2226/Backdoor-Activation-Attack|https://doi.org/10.48550/arXiv.2311.09433|
|175|BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models|J Kerekes Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/JJ-Vice/BAGM|https://doi.org/10.1109/tifs.2024.3386058|
|176|Rethinking Backdoor Data Poisoning Attacks in the Context of
  Semi-Supervised Learning|Marissa Catherine Connor, Vincent Emanuele|<li><span style="color:#FF5733;">2022-12-01</span></li>|Submitted to ICLR 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/rethinking-backdoor-data-poisoning-attacks-in/code)|https://openreview.net/pdf/1e8864ea33570efedf181847e8e700fc3a7e8855.pdf|
|177|BagFlip: A Certified Defense Against Data Poisoning|Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni|<li><span style="color:#FF5733;">2022-11-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/bagflip-a-certified-defense-against-data/code)|https://openreview.net/pdf/960044f35c0a2651737dc6ac8644ffd315d6a2dc.pdf|
|178|CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive
  Learning|Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2022-11-01</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/corruptencoder-data-poisoning-based-backdoor/code)|https://openreview.net/pdf/a71769013eb8042087131d5a81891020c7af2964.pdf|
|179|Rethinking the Reverse-engineering of Trojan Triggers|Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, Shiqing Ma|<li><span style="color:#FF5733;">2022-11-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/rethinking-the-reverse-engineering-of-trojan/code)|https://openreview.net/pdf/e8ad6cc8620c4cec22babbe51c8f36d680dcd00c.pdf|
|180|Rickrolling the Artist: Injecting Backdoors into Text Encoders for
  Text-to-Image Synthesis|Lukas Struppek, Dominik Hintersdorf, Kristian Kersting|<li><span style="color:#FF5733;">2022-11-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/LukasStruppek/Rickrolling-the-Artist.|https://doi.org/10.1109/iccv51070.2023.00423|
|181|Towards Out-of-Distribution Sequential Event Prediction: A Causal Treatment|Chenxiao Yang, Qitian Wu, Qingsong Wen, Zhiqiang Zhou, Liang Sun, Junchi Yan|<li><span style="color:#FF5733;">2022-11-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/towards-out-of-distribution-sequential-event/code)|https://openreview.net/pdf/b5224a60869a26365b6e70239acbac055a762b08.pdf|
|182|Opportunistic Backdoor Attacks: Exploring Human-imperceptible Vulnerabilities on Speech Recognition Systems|Qiang Liu, Tongqing Zhou, Zhiping Cai, Yonghao Tang|<li><span style="color:#FF5733;">2022-10-10</span></li>|Proceedings of the 30th ACM International Conference on Multimedia|https://github.com/lqsunshine/DABA.|https://openreview.nethttps://dl.acm.org/doi/abs/10.1145/3503161.3548261|
|183|Expose Backdoors on the Way: A Feature-Based Efficient Defense against
  Textual Backdoor Attacks|Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun|<li><span style="color:#FF5733;">2022-10-01</span></li>|OpenAlex|https://github.com/lancopku/DAN.|https://doi.org/10.18653/v1/2022.findings-emnlp.47|
|184|FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated
  Learning|Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, ...|<li><span style="color:#FF5733;">2022-10-01</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/flip-a-provable-defense-framework-for/code)|https://openreview.net/pdf/6731b5784520aedd43f4da6cb01e5587b66819be.pdf|
|185|Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks
  via Motifs|Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, Guohan Huang|<li><span style="color:#FF5733;">2022-10-01</span></li>|IEEE Transactions on Computational Social Systems|https://github.com/Seaocn/Motif-Backdoor|https://doi.org/10.1109/tcss.2023.3267094|
|186|Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor
  Attacks in Federated Learning|Yuxin Wen, Jonas Geiping, Liam H Fowl, Hossein Souri, Rama Chellappa, Micah Goldblum, Tom Goldstein|<li><span style="color:#FF5733;">2022-10-01</span></li>|Submitted to ICLR 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/thinking-two-moves-ahead-anticipating-other/code)|https://openreview.net/pdf/f0d55f776bc33c4bd42632e2a4a381cc8a49356b.pdf|
|187|Trap and Replace: Defending Backdoor Attacks by Trapping Them into an
  Easy-to-Replace Subnetwork|Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang|<li><span style="color:#FF5733;">2022-10-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/trap-and-replace-defending-backdoor-attacks/code)|http://papers.nips.cc/paper_files/paper/2022/hash/ea06e6e9e80f1c3d382317fff67041ac-Abstract-Conference.html|
|188|Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset
  Copyright Protection|Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li|<li><span style="color:#FF5733;">2022-10-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/untargeted-backdoor-watermark-towards/code)|http://papers.nips.cc/paper_files/paper/2022/hash/55bfedfd31489e5ae83c9ce8eec7b0e1-Abstract-Conference.html|
|189|TransCAB: Transferable Clean-Annotation Backdoor to Object Detection
  with Natural Trigger in Real-World|Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, âªSurya Nepalâ¬, Derek Abbott|<li><span style="color:#FF5733;">2022-09-01</span></li>|OpenAlex|https://github.com/inconstance/T-shirt-natural-backdoor-dataset|https://doi.org/10.1109/SRDS60354.2023.00018|
|190|Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for
  Classification Models|Guang Hua, Andrew Beng Jin Teoh|<li><span style="color:#FF5733;">2022-08-01</span></li>|Pattern Recognition, Vol. 144, Dec. 2023|https://github.com/ghua-ac/dnn_watermark.|https://doi.org/10.1016/j.patcog.2023.109844|
|191|Friendly Noise against Adversarial Noise: A Powerful Defense against
  Data Poisoning Attacks|Tian Yu Liu, Yu Yang, Baharan Mirzasoleiman|<li><span style="color:#FF5733;">2022-08-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/friendly-noise-against-adversarial-noise-a/code)|https://openreview.net/pdf/d929e1c412e3fecf6a4fb8991f306a09330510c6.pdf|
|192|RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact
  DNN|Huy P. Phan, Cong Shi, Yi Xie, Tianfang Zhang, Zhuohang Li, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen, Bo Yuan|<li><span style="color:#FF5733;">2022-08-01</span></li>|European Conference on Computer Vision (ECCV 2022)|https://github.com/huyvnphan/ECCV2022-RIBAC|https://doi.org/10.1007/978-3-031-19772-7_41|
|193|Data-Efficient Backdoor Attacks|Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li|<li><span style="color:#FF5733;">2022-07-01</span></li>|Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence|https://github.com/xpf/Data-Efficient-Backdoor-Attacks.|https://openreview.nethttps://www.ijcai.org/proceedings/2022/0554.pdf|
|194|A Unified Evaluation of Textual Backdoor Learning: Frameworks and
  Benchmarks|Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun|<li><span style="color:#FF5733;">2022-06-01</span></li>|NeurIPS 2022 Datasets and Benchmarks |[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/a-unified-evaluation-of-textual-backdoor/code)|http://papers.nips.cc/paper_files/paper/2022/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_and_Benchmarks.html|
|195|Backdoor Attacks on Self-Supervised Learning|Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash|<li><span style="color:#FF5733;">2022-06-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/UMBCvision/SSL-Backdoor|https://doi.org/10.1109/cvpr52688.2022.01298|
|196|Imperceptible Backdoor Attack: From Input Space to Feature
  Representation|Nan Zhong, Zhenxing Qian, Xinpeng Zhang|<li><span style="color:#FF5733;">2022-05-01</span></li>|Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence|https://github.com/Ekko-zn/IJCAI2022-Backdoor.|https://doi.org/10.48550/arXiv.2205.03190|
|197|Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free|Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, Zhangyang Wang|<li><span style="color:#FF5733;">2022-05-01</span></li>|arXiv|https://github.com/VITA-Group/Backdoor-LTH.|http://arxiv.org/abs/2205.11819v1|
|198|Enhancing Backdoor Attacks With Multi-Level MMD Regularization|Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li|<li><span style="color:#FF5733;">2022-03-28</span></li>|IEEE Transactions on Dependable and Secure Computing|https://github.com/xpf/Multi-Level-MMD-Regularization.|https://doi.org/10.1109/tdsc.2022.3161477|
|199|Partial Identification with Noisy Covariates: A Robust Optimization Approach|Wenshuo Guo, Mingzhang Yin, Yixin Wang, Michael Jordan|<li><span style="color:#FF5733;">2022-02-10</span></li>|CLeaR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/partial-identification-with-noisy-covariates/code)|https://openreview.net/pdf/e1406a39171680783aba296614172a44fd6bdbf7.pdf|
|200|Under-confidence Backdoors Are Resilient and Stealthy Backdoors|Minlong Peng, Zidi Xiong, Quang H. Nguyen, Mingming Sun, Khoa D. Doan, Ping Li|<li><span style="color:#FF5733;">2022-02-01</span></li>|arXiv|https://github.com/v-mipeng/LabelSmoothedAttack.git|http://arxiv.org/abs/2202.11203v2|
|201|Training with More Confidence: Mitigating Injected and Natural Backdoors
  During Training|Zhenting Wang, Hailun Ding, Juan Zhai, Shiqing Ma|<li><span style="color:#FF5733;">2022-02-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.06382/code)|http://papers.nips.cc/paper_files/paper/2022/hash/ec0c9ca85b4ea49c7ebfb503cf55f2ae-Abstract-Conference.html|
|202|Trigger Hunting with a Topological Prior for Trojan Detection|Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, Chao Chen|<li><span style="color:#FF5733;">2022-01-29</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 7 code implementations](https://www.catalyzex.com/paper/trigger-hunting-with-a-topological-prior-for/code)|https://openreview.net/pdf/4db1d42d467c296c5ec7fa3f38e37dcb5c140e84.pdf|
|203|Few-shot Backdoor Attacks via Neural Tangent Kernels|Jonathan Hayase, Sewoong Oh|<li><span style="color:#FF5733;">2022-01-01</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/few-shot-backdoor-attacks-via-neural-tangent/code)|https://openreview.net/pdf/fbf6611dad17d0a7975a0a139013d45d767f9c59.pdf|
|204|Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks|Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun|<li><span style="color:#FF5733;">2022-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/thunlp/StyleAttack.|https://openreview.net/pdf/09ec283781ceeabec1fbbbfda26653cf25e8db09.pdf|
|205|Stealthy Backdoors as Compression Artifacts|Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans|<li><span style="color:#FF5733;">2022-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/yulongtzzz/Stealthy-Backdoors-as-Compression-Artifacts|https://doi.org/10.1109/tifs.2022.3160359|
|206|Provable Defense against Backdoor Policies in Reinforcement Learning|Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Xiaojin Zhu|<li><span style="color:#FF5733;">2022-01-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/provable-defense-against-backdoor-policies-in/code)|http://papers.nips.cc/paper_files/paper/2022/hash/5e67e6a814526079ad8505bf6d926fb6-Abstract-Conference.html|
|207|Post-Training Detection of Backdoor Attacks for Two-Class and
  Multi-Attack Scenarios|Zhen Xiang, David J. Miller, George Kesidis|<li><span style="color:#FF5733;">2022-01-01</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/post-training-detection-of-backdoor-attacks/code)|https://openreview.net/pdf/ab4bf90af1442414ba5fa816448b5b73d44ecb92.pdf|
|208|Model-Contrastive Learning for Backdoor Elimination|Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen|<li><span style="color:#FF5733;">2022-01-01</span></li>|ACM Multimedia|https://github.com/WeCanShow/MCL.|https://doi.org/10.48550/arXiv.2205.04411|
|209|Label-Smoothed Backdoor Attack|Minlong Peng, Zidi Xiong, Mingming Sun, Ping Li|<li><span style="color:#FF5733;">2022-01-01</span></li>|arXiv|https://github.com/v-mipeng/LabelSmoothedAttack.git|https://arxiv.org/abs/2202.11203|
|210|An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks|Xinqiao Zhang, Huili Chen, Ke Huang, Farinaz Koushanfar|<li><span style="color:#FF5733;">2022-01-01</span></li>|arXiv|https://github.com/xinqiaozhang/adatrojan|https://doi.org/10.48550/arXiv.2204.04329|
|211|How to Backdoor Diffusion Models?|Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho|<li><span style="color:#FF5733;">2022-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/how-to-backdoor-diffusion-models/code)|https://openreview.net/pdf/1dc679066d3bc93c7cd365d2948a2a48e4d2ff3a.pdf|
|212|Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation|Tianrui Qin, Xianghuan He, Xitong Gao, Yiren Zhao, Kejiang Ye, Cheng-Zhong Xu|<li><span style="color:#FF5733;">2022-01-01</span></li>|Submitted to ICLR 2023|https://github.com/lafeat/flareon.|https://openreview.net/pdf/8f5e6d6b8c53b5115dfb5e4950961efed881feaa.pdf|
|213|Imperceptible and Robust Backdoor Attack in 3D Point Cloud|Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia|<li><span style="color:#FF5733;">2022-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/KuofengGao/IRBA|https://doi.org/10.48550/arXiv.2208.08052|
|214|Few-Shot Backdoor Attacks on Visual Object Tracking|Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia|<li><span style="color:#FF5733;">2022-01-01</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/few-shot-backdoor-attacks-on-visual-object/code)|https://openreview.net/pdf/132d1b18d6c8d837cebbdb801781870f713295cb.pdf|
|215|Backdoor Attacks on Vision Transformers|Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash|<li><span style="color:#FF5733;">2022-01-01</span></li>|arXiv|https://github.com/UCDvision/backdoor_transformer.git|https://doi.org/10.48550/arXiv.2206.08477|
|216|Data-Free Backdoor Removal Based on Channel Lipschitzness|Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu|<li><span style="color:#FF5733;">2022-01-01</span></li>|Lecture notes in computer science|https://github.com/rkteddy/channel-Lipschitzness-based-pruning.|https://doi.org/10.1007/978-3-031-20065-6_11|
|217|Augmentation Backdoors|Joseph Rance, Yiren Zhao, Ilia Shumailov, Robert D. Mullins|<li><span style="color:#FF5733;">2022-01-01</span></li>|ICLR 2023 BANDS Spotlight|[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/augmentation-backdoors/code)|https://openreview.net/pdf/59a474155bd99e72fd1d60447640fe322d4f340d.pdf|
|218|Backdoor Attacks in the Supply Chain of Masked Image Modeling|Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang|<li><span style="color:#FF5733;">2022-01-01</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/backdoor-attacks-in-the-supply-chain-of/code)|https://openreview.net/pdf/0c5ec0b08ce9e3512fdc3d80cd06802dbb8ef089.pdf|
|219|Architectural Backdoors in Neural Networks|Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert D. Mullins, Nicolas Papernot|<li><span style="color:#FF5733;">2022-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/architectural-backdoors-in-neural-networks/code)|https://openreview.net/pdf/c202e3f7b58579019c2ae7534b94815d06eda13d.pdf|
|220|Backdoor Defense via Decoupling the Training Process|Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren|<li><span style="color:#FF5733;">2022-01-01</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 9 code implementations](https://www.catalyzex.com/paper/backdoor-defense-via-decoupling-the-training/code)|https://openreview.net/pdf/825a2fee50fe494bcf13085113d2a7565af192b6.pdf|
|221|BadPrompt: Backdoor Attacks on Continuous Prompts|Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan|<li><span style="color:#FF5733;">2022-01-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/badprompt-backdoor-attacks-on-continuous/code)|http://papers.nips.cc/paper_files/paper/2022/hash/f0722b58f02d7793acf7d328928f933a-Abstract-Conference.html|
|222|Black-box Dataset Ownership Verification via Backdoor Watermarking|Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia|<li><span style="color:#FF5733;">2022-01-01</span></li>|IEEE Transactions on Information Forensics and Security|https://github.com/THUYimingLi/DVBW.|https://doi.org/10.1109/TIFS.2023.3265535|
|223|Identifying a Training-Set Attack's Target Using Renormalized Influence
  Estimation|Zayd Hammoudeh, Daniel Lowd|<li><span style="color:#FF5733;">2022-01-01</span></li>|arXiv|https://github.com/ZaydH/target_identification.|http://arxiv.org/abs/2201.10055v2|
|224|FIBA: Frequency-Injection based Backdoor Attack in Medical Image
  Analysis|Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao|<li><span style="color:#FF5733;">2021-12-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/HazardFY/FIBA.|https://doi.org/10.1109/cvpr52688.2022.02021|
|225|Manipulating SGD with Data Ordering Attacks|Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu, Ross Anderson|<li><span style="color:#FF5733;">2021-11-10</span></li>|NeurIPS 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/manipulating-sgd-with-data-ordering-attacks/code)|https://openreview.net/pdf/38b5087efcece7a26b421cd3cd7e0a2a30c8096e.pdf|
|226|Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes|Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, Tudor Dumitras|<li><span style="color:#FF5733;">2021-11-10</span></li>|NeurIPS 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/qu-anti-zation-exploiting-quantization/code)|https://openreview.net/pdf/d99b499610c11e58db2b8e2b8b421fbd7ec493a8.pdf|
|227|A Kernel Test for Causal Association via Noise Contrastive Backdoor
  Adjustment|Robert Hu, Dino Sejdinovic, Robin J. Evans|<li><span style="color:#FF5733;">2021-11-01</span></li>|J. Mach. Learn. Res.|https://github.com/MrHuff/kgformula|https://jmlr.org/papers/v25/21-1409.html|
|228|Anomaly Localization in Model Gradients Under Backdoor Attacks Against
  Federated Learning|Zeki Bilgin|<li><span style="color:#FF5733;">2021-11-01</span></li>|OpenAlex|https://github.com/ArcelikAcikKaynak/Federated_Learning.git|https://dblp.uni-trier.de/db/journals/corr/corr2111.html#abs-2111-14683|
|229|Adversarial Neuron Pruning Purifies Backdoored Deep Models|Dongxian Wu, Yisen Wang|<li><span style="color:#FF5733;">2021-10-27</span></li>|NeurIPS 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/adversarial-neuron-pruning-purifies/code)|https://proceedings.neurips.cc/paper/2021/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html|
|230|Anti-Backdoor Learning: Training Clean Models on Poisoned Data|Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma|<li><span style="color:#FF5733;">2021-10-21</span></li>|NeurIPS 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/anti-backdoor-learning-training-clean-models/code)|https://proceedings.neurips.cc/paper/2021/hash/7d38b1e9bd793d3f45e0e212a729a93c-Abstract.html|
|231|Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text
  Style Transfer|Fanchao Qi, YangâYi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun|<li><span style="color:#FF5733;">2021-10-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/thunlp/StyleAttack.|https://doi.org/10.18653/v1/2021.emnlp-main.374|
|232|RAP: Robustness-Aware Perturbations for Defending against Backdoor
  Attacks on NLP Models|Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun|<li><span style="color:#FF5733;">2021-10-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/lancopku/RAP.|https://doi.org/10.18653/v1/2021.emnlp-main.659|
|233|AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value
  Analysis|Junfeng Guo, Ang Li, Cong Liu|<li><span style="color:#FF5733;">2021-10-01</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/aeva-black-box-backdoor-detection-using/code)|https://openreview.net/pdf/b8ad85b4ddd615a5abac4d7c1d5713fc92b9f0e9.pdf|
|234|Invisible Backdoor Attack with Sample-Specific Triggers|Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu|<li><span style="color:#FF5733;">2021-10-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/yuezunli/ISSBA.|https://doi.org/10.1109/iccv48922.2021.01615|
|235|A Backdoor Attack against 3D Point Cloud Classifiers|Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis|<li><span style="color:#FF5733;">2021-10-01</span></li>|2021 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/zhenxianglance/PCBA.|https://doi.org/10.1109/iccv48922.2021.00750|
|236|Backdoor Attack on Hash-based Image Retrieval via Clean-label Data
  Poisoning|Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia|<li><span style="color:#FF5733;">2021-09-01</span></li>|BMVC|https://github.com/KuofengGao/CIBA.|http://proceedings.bmvc2023.org/172/|
|237|BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning|Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song|<li><span style="color:#FF5733;">2021-08-01</span></li>|OpenAlex|https://github.com/wanglun1996/multi_agent_rl_backdoor_videos.|https://doi.org/10.24963/ijcai.2021/509|
|238|BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised
  Learning|Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2021-08-01</span></li>|2022 IEEE Symposium on Security and Privacy (SP)|https://github.com/jjy1994/BadEncoder.|https://openreview.nethttps://arxiv.org/pdf/2108.00352.pdf|
|239|Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks
  Trained from Scratch|Hossein Souri, Liam H Fowl, Rama Chellappa, Micah Goldblum, Tom Goldstein|<li><span style="color:#FF5733;">2021-06-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/sleeper-agent-scalable-hidden-trigger/code)|http://papers.nips.cc/paper_files/paper/2022/hash/79eec295a3cd5785e18c61383e7c996b-Abstract-Conference.html|
|240|Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word
  Substitution|Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun|<li><span style="color:#FF5733;">2021-06-01</span></li>|ACL/IJCNLP|https://github.com/thunlp/BkdAtk-LWS.|https://doi.org/10.18653/v1/2021.acl-long.377|
|241|Incompatibility Clustering as a Defense Against Backdoor Poisoning
  Attacks|Charles Jin, Melinda Sun, Martin C. Rinard|<li><span style="color:#FF5733;">2021-05-01</span></li>|ICLR 2023 poster|https://github.com/charlesjin/compatibility_clustering|https://openreview.net/pdf/e27bb4c7787b3770053151428e69c5ab0f279dd2.pdf|
|242|SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics|Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh|<li><span style="color:#FF5733;">2021-04-22</span></li>|arXiv|https://github.com/SewoongLab/spectre-defense|http://export.arxiv.org/pdf/2104.11315|
|243|Targeted Attack against Deep Neural Networks via Flipping Limited Weight
  Bits|Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, Shu-Tao Xia|<li><span style="color:#FF5733;">2021-02-01</span></li>|ICLR 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/targeted-attack-against-deep-neural-networks/code)|https://openreview.net/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf|
|244|Neural Attention Distillation: Erasing Backdoor Triggers from Deep
  Neural Networks|Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma|<li><span style="color:#FF5733;">2021-01-14</span></li>|ICLR 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 8 code implementations](https://www.catalyzex.com/paper/neural-attention-distillation-erasing/code)|https://openreview.net/pdf/42f5786a622e8cdc4ce43d79d5d83ebe8e4feeeb.pdf|
|245|Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching|Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein|<li><span style="color:#FF5733;">2021-01-13</span></li>|ICLR 2021 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/witches-brew-industrial-scale-data-poisoning/code)|https://openreview.net/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf|
|246|WaNet -- Imperceptible Warping-based Backdoor Attack|Tuan Anh Nguyen, Anh Tuan Tran|<li><span style="color:#FF5733;">2021-01-01</span></li>|International Conference on Learning Representations|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/wanet-imperceptible-warping-based-backdoor/code)|https://openreview.net/pdf/db3277f5b47619abfe13880772b864960e98f643.pdf|
|247|Use Procedural Noise to Achieve Backdoor Attack|Xuan Chen, Yuena Ma, Shiwei Lu|<li><span style="color:#FF5733;">2021-01-01</span></li>|IEEE Access|https://github.com/928082786/pnoiseattack.|https://doi.org/10.1109/access.2021.3110239|
|248|Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks|Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun|<li><span style="color:#FF5733;">2021-01-01</span></li>|ICML 2021 Workshop AML Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/red-alarm-for-pre-trained-models-universal/code)|https://openreview.net/pdf/1cc11ab778ba03f41a45f941b3a3e42ccb867cc6.pdf|
|249|Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger|Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun|<li><span style="color:#FF5733;">2021-01-01</span></li>|ACL/IJCNLP|https://github.com/thunlp/HiddenKiller.|https://doi.org/10.18653/v1/2021.acl-long.37|
|250|ONION: A Simple and Effective Defense Against Textual Backdoor Attacks|Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun|<li><span style="color:#FF5733;">2021-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/thunlp/ONION.|https://doi.org/10.18653/v1/2021.emnlp-main.752|
|251|Excess Capacity and Backdoor Poisoning|Naren Sarayu Manoj, Avrim Blum|<li><span style="color:#FF5733;">2021-01-01</span></li>|NeurIPS 2021 Spotlight|[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/excess-capacity-and-backdoor-poisoning/code)|https://proceedings.neurips.cc/paper/2021/hash/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Abstract.html|
|252|Handcrafted Backdoors in Deep Neural Networks|Sanghyun Hong, Nicholas Carlini, Alexey Kurakin|<li><span style="color:#FF5733;">2021-01-01</span></li>|NeurIPS 2022 Accept|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/handcrafted-backdoors-in-deep-neural-networks/code)|http://papers.nips.cc/paper_files/paper/2022/hash/3538a22cd3ceb8f009cc62b9e535c29f-Abstract-Conference.html|
|253|CRFL: Certifiably Robust Federated Learning against Backdoor Attacks|Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li|<li><span style="color:#FF5733;">2021-01-01</span></li>|OpenAlex|https://github.com/AI-secure/CRFL.|http://proceedings.mlr.press/v139/xie21a.html|
|254|An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware|M. Caner Tol, Saad Islam, Berk Sunar, Ziming Zhang|<li><span style="color:#FF5733;">2021-01-01</span></li>|ICLR 2022 Submitted|[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/an-optimization-perspective-on-realizing/code)|https://openreview.net/pdf/628fdbcebf74b3b22b28cf024722d2d5b78c9136.pdf|
|255|Adversarial Unlearning of Backdoors via Implicit Hypergradient|Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia|<li><span style="color:#FF5733;">2021-01-01</span></li>|ICLR 2022 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/adversarial-unlearning-of-backdoors-via/code)|https://openreview.net/pdf/6aeb6e81c9d0eadbb4cfbefb6caac0f155d561ea.pdf|
|256|Poisoned classifiers are not only backdoored, they are fundamentally
  broken|Mingjie Sun, Siddhant Agarwal, J. Zico Kolter|<li><span style="color:#FF5733;">2020-10-01</span></li>|ICLR 2022 Submitted|[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/poisoned-classifiers-are-not-only-backdoored/code)|https://openreview.net/pdf/4959459ccc8a6c2d401fe6ca978ce4b82b4f3ff0.pdf|
|257|Weight Poisoning Attacks on Pre-trained Models|Keita Kurita, Paul Michel, Graham Neubig|<li><span style="color:#FF5733;">2020-04-01</span></li>|arXiv|https://github.com/neulab/RIPPLe.|http://arxiv.org/abs/2004.06660v1|
|258|Backdoor Attack against Speaker Verification|Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia|<li><span style="color:#FF5733;">2020-01-01</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/zhaitongqing233/Backdoor-attack-against-speaker-verification.|https://doi.org/10.1109/ICASSP39728.2021.9413468|
|259|Backdoor Learning: A Survey|Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia|<li><span style="color:#FF5733;">2020-01-01</span></li>|IEEE Transactions on Neural Networks and Learning Systems|https://github.com/THUYimingLi/backdoor-learning-resources|https://doi.org/10.1109/TNNLS.2022.3182979|
|260|Graph Backdoor|Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang|<li><span style="color:#FF5733;">2020-01-01</span></li>|USENIX Security Symposium|https://github.com/HarrialX/GraphBackdoor|https://www.usenix.org/conference/usenixsecurity21/presentation/xi|
|261|Input-Aware Dynamic Backdoor Attack|Tuan Anh Nguyen, Anh Tuan Tran|<li><span style="color:#FF5733;">2020-01-01</span></li>|Neural Information Processing Systems|https://github.com/VinAIResearch/input-aware-backdoor-attack-release.|https://openreview.nethttp://proceedings.neurips.cc/paper/2020/file/234e691320c0ad5b45ee3c96d0d7b8f8-Paper.pdf|
|262|Rethinking the Trigger of Backdoor Attack|Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia|<li><span style="color:#FF5733;">2020-01-01</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/rethinking-the-trigger-of-backdoor-attack/code)|https://openreview.net/pdf/f41085225b4c2960c0e50e0201c0c0ab536e020f.pdf|
|263|Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness|Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin|<li><span style="color:#FF5733;">2019-12-20</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/bridging-mode-connectivity-in-loss-landscapes/code)|https://openreview.net/pdf/fb8082dd5515e11c88f59b0f4911266f1891fb61.pdf|
|264|Attack-Resistant Federated Learning with Residual-based Reweighting|Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen|<li><span style="color:#FF5733;">2019-12-01</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/attack-resistant-federated-learning-with/code)|https://openreview.net/pdf/1ea807b624ecc563e3b617f0948502afeee0ec8c.pdf|
